---
title: "season-preview-boneyard"
author: "Syver Johansen"
date: "2024-11-03"
output: html_document
---


#Boneyard

## Regression Models
Let's move on to modeling and try to use a different number of approaches and calculate their summary statistics.  Since our predictions are more interested in how the better skiers do than the worse skiers, we will use a customized R2 value that does just that

We will start out with just normal, un transformed data.

### Linear Regression
```{r lin-reg}
library(ggplot2)
library(dplyr)

# Function to create the model, print summary, calculate custom R², and plot results
create_and_plot_model <- function(df,col, X_label, y_label, title) {
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data
  
  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Create the formula for the model
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the model
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  plot(model)
  # ggplot for Predicted vs Actual
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
}
  


# Prepare y (Pct_of_Max_Points)


create_and_plot_model(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Pelo")

create_and_plot_model(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Distance")

create_and_plot_model(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Sprint")

create_and_plot_model(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Distance + Sprint")
```
From this we can actually get a pretty good custom R2 when using both previous distance and previous sprint.  Let's see if log can do any better

### Log Regression
```{r log-reg}
library(ggplot2)
library(dplyr)

# Function to create the model, print summary, calculate custom R², and plot results
create_and_plot_model <- function(df,col, X_label, y_label, title) {
  df$Pct_of_Max_Points = log10(df$Pct_of_Max_Points+.000001)
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data
  
  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Create the formula for the model
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the model
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  plot(model)
  # ggplot for Predicted vs Actual
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
}
  


# Prepare y (Pct_of_Max_Points)


create_and_plot_model(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Pelo")

create_and_plot_model(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Distance")

create_and_plot_model(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Sprint")

create_and_plot_model(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Distance + Sprint")
```
Nope!  Way worse



Let's try weighted regression

### Weighted Regression
```{r weight-reg}
library(ggplot2)
library(dplyr)

# Function to create the model, print summary, calculate custom R², and plot results
library(ggplot2)
library(dplyr)

# Function to create the model, print summary, calculate custom R², and plot results
create_and_plot_weighted_model <- function(df, col, X_label, y_label, title) {
  set.seed(42)  # Set a seed for reproducibility
  
  # Split data into train and test sets
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))  # 80% training data
  df_train <- df[train_index, ]
  df_test <- df[-train_index, ]
  
  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Custom weights based on y_train and y_test
  df_train$weights <- y_train  # Add the custom weights as a column in df_train
  df_test$weights <- y_test    # Add the custom weights as a column in df_test
  
  # Create the formula for the model
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the weighted regression model
  model <- lm(formula, data = df_train, weights = weights)
  print(summary(model))  # Print model summary
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  # Calculate custom weighted R² for train data
  weighted_rss_train <- sum(df_train$weights * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(df_train$weights * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  weighted_rss_test <- sum(df_test$weights * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(df_test$weights * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores
  print(paste("Custom Weighted R² (Train):", custom_r2_train))
  print(paste("Custom Weighted R² (Test):", custom_r2_test))
  
  # Data for plotting (Actual vs Predicted for test data)
  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  # Plot the diagnostics using the default plot function for the model
  par(mfrow = c(2, 2))  # Set up a 2x2 plot layout for diagnostic plots
  plot(model)
  
  # ggplot for Predicted vs Actual
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = paste("Actual", y_label),
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
}
# Call 1: Single explanatory variable (Prev_Pelo)
create_and_plot_weighted_model(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Pelo")

# Call 2: Single explanatory variable (Prev_Distance)
create_and_plot_weighted_model(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Distance")

# Call 3: Single explanatory variable (Prev_Sprint)
create_and_plot_weighted_model(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Sprint")

# Example usage for multiple explanatory variables:
create_and_plot_weighted_model(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Distance + Sprint")



```

A slight improvement over regular linear, but those residuals are still too high and seem to undersell good skiers.  Maybe a square root transformation will work.

### Square Root Regression
```{r sq-reg}
library(ggplot2)
library(dplyr)

# Function to create the model, print summary, calculate custom R², and plot results with square root transformation
create_and_plot_model_sqrt <- function(df, col, X_label, y_label, title) {
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]  # Testing data

  # Extract dependent variable (apply square root transformation)
  y_train <- sqrt(df_train[[y_label]])
  y_test <- sqrt(df_test[[y_label]])
  
  # Create the formula for the model
  formula_string <- paste("sqrt(", y_label, ") ~", paste(col, collapse = " + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the model
  model <- lm(formula, data = df_train)
  print(summary(model))  # Print model summary
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))
  
  # Plot predicted vs actual values
  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  # ggplot for Predicted vs Actual
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = paste("Actual (Square Root of", y_label, ")"),
         y = "Predicted (Square Root)") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  # Plot the residuals
  plot(model)
}

# Example usage with square root transformation
create_and_plot_model_sqrt(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "Square Root: Pct_of_Max_Points ~ #Prev_Pelo")
create_and_plot_model_sqrt(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "Square Root: #Pct_of_Max_Points ~ Prev_Distance")
create_and_plot_model_sqrt(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "Square Root: #Pct_of_Max_Points ~ Prev_Sprint")
create_and_plot_model_sqrt(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint",  "Pct_of_Max_Points","Square Root: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint")


```
Best one yet, using plain elo to predict.  Not sure why the squre root works so well.


Let's see if Box-Cox does any better 

### Box-Cox Regression
```{r bc-reg}
library(ggplot2)
library(dplyr)
library(MASS)

# Function to create the model, print summary, calculate custom R², and plot results
create_and_plot_model <- function(df, col, X_label, y_label, title) {
  # Ensure Pct_of_Max_Points is positive
  df[[y_label]] <- df[[y_label]] + 0.000001  # Avoid log of zero

  # Fit the initial linear model
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  formula <- as.formula(formula_string)
  lm_model <- lm(formula, data = df)
  print(summary(lm_model))

  # Box-Cox transformation
  bc <- MASS::boxcox(lm_model, lambda = seq(-2, 2, by = 0.1))  # Specify a range for lambda
  best_lambda <- bc$x[which.max(bc$y)]  # Find the optimal lambda
  print(paste("Best lambda:", best_lambda))

  # Apply the Box-Cox transformation
  if (best_lambda != 0) {
    df[[y_label]] <- (df[[y_label]]^best_lambda - 1) / best_lambda
  } else {
    df[[y_label]] <- log(df[[y_label]])
  }

  # Split the data into training and testing sets
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data

  # Fit the model again with transformed data
  model <- lm(formula, data = df_train)
  print(summary(model))  # Print model summary

  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data

  # Calculate custom weighted R² for train data
  custom_weight_train <- df_train[[y_label]]
  weighted_rss_train <- sum(custom_weight_train * (df_train[[y_label]] - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (df_train[[y_label]] - mean(df_train[[y_label]]))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)

  # Calculate custom weighted R² for test data
  custom_weight_test <- df_test[[y_label]]
  weighted_rss_test <- sum(custom_weight_test * (df_test[[y_label]] - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (df_test[[y_label]] - mean(df_test[[y_label]]))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)

  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  # ggplot for Predicted vs Actual
  plot_data <- data.frame(Actual = df_test[[y_label]], Predicted = y_pred_test)
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
}

  


# Prepare y (Pct_of_Max_Points)


# create_and_plot_model(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Pelo")
# lm_model <- lm(Pct_of_Max_Points~Prev_Pelo, data = df)
# bc <- boxcox(lm_model)
# 
# create_and_plot_model(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Distance")
# 
# create_and_plot_model(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Sprint")
# 
# create_and_plot_model(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint", "Pct_of_Max_Points", "Pct_of_Max_Points ~ Distance + Sprint")
```
Right now Box-Cox is not running how we want it to run.  We ran it in Python however, and got this.

Quantile transformation looked promising earlier.  Let;s see if the regression scores at all

### Quantile Regression
```{r quantile-reg}
# Load required libraries
library(quantreg)
# Function to create the quantile regression model, print summary, calculate custom R², and plot results
create_and_plot_model_quantile <- function(df, col, X_label, y_label, title, tau = 0.75) {
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data
  
  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Create the formula for the model
  formula_string <- paste(y_label, "~", paste(col, collapse = " + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the quantile regression model
  model <- rq(formula, data = df_train, tau = tau)
  print(summary(model))  # Print model summary
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  # Calculate residuals
  residuals_train <- y_train - y_pred_train
  residuals_test <- y_test - y_pred_test
  
  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))
  
  # Plot predicted vs actual values
  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  # ggplot for Predicted vs Actual
  p1 <- ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  # Residuals vs Fitted plot
  residuals_data <- data.frame(Fitted = y_pred_test, Residuals = residuals_test)
  p2 <- ggplot(residuals_data, aes(x = Fitted, y = Residuals)) +
    geom_point(color = "darkgreen") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuals vs Fitted",
         x = "Fitted Values",
         y = "Residuals") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  # Histogram of residuals
  p3 <- ggplot(data.frame(Residuals = residuals_test), aes(x = Residuals)) +
    geom_histogram(bins = 30, fill = "lightblue", color = "black") +
    labs(title = "Histogram of Residuals",
         x = "Residuals",
         y = "Frequency") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title

  # Print all plots
  print(p1)
  print(p2)
  print(p3)
}

create_and_plot_model_quantile(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "Quantile Regression: Pct_of_Max_Points ~ Prev_Pelo")
create_and_plot_model_quantile(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "Quantile Regression: Pct_of_Max_Points ~ Prev_Distance")
create_and_plot_model_quantile(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "Quantile Regression: Pct_of_Max_Points ~ Sprint")
create_and_plot_model_quantile(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint", "Pct_of_Max_Points", "Quantile Regression: Pct_of_Max_Points ~ Distance + Sprint")


```
Obviously the residuals are gonna get messed up by this, by 90th percentile quantile regression is the best we've seen so far.

### Polynomial Regression

We will start this by finding what degree has the best term.
```{r poly-disc}
find_optimal_polynomial_degree <- function(df, cols, y_label, max_degree = 20) {
  results <- data.frame(Degree = integer(), Custom_R2_Train = numeric(), Custom_R2_Test = numeric())
  
  for (degree in 1:max_degree) {
    # Train and evaluate the polynomial model for the given degree
    train_index <- sample(1:nrow(df), 0.8 * nrow(df))  # Train/Test split
    df_train <- df[train_index, ]  # Training data
    df_test <- df[-train_index, ]  # Testing data
    
    # Extract dependent variable
    y_train <- df_train[[y_label]]
    y_test <- df_test[[y_label]]
    
    # Create polynomial terms
    poly_terms <- sapply(cols, function(col) paste("poly(", col, ", degree =", degree, ")", sep = ""))
    formula_string <- paste(y_label, "~", paste(poly_terms, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Train the polynomial model
    model <- lm(formula, data = df_train)
    
    # Make predictions
    y_pred_train <- predict(model, newdata = df_train)
    y_pred_test <- predict(model, newdata = df_test)
    
    # Calculate custom weighted R² for train data
    custom_weight_train <- y_train
    weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
    weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
    custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
    
    # Calculate custom weighted R² for test data
    custom_weight_test <- y_test
    weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
    weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
    custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
    
    # Save the results
    results <- rbind(results, data.frame(Degree = degree, Custom_R2_Train = custom_r2_train, Custom_R2_Test = custom_r2_test))
  }
  
  return(results)
}

# Find the optimal degree for your models
df_results <- find_optimal_polynomial_degree(df, c("Prev_Pelo"), "Pct_of_Max_Points", max_degree = 20)
distance_results <- find_optimal_polynomial_degree(distance_df, c("Prev_Distance"), "Pct_of_Max_Points", max_degree = 20)
sprint_results <- find_optimal_polynomial_degree(sprint_df, c("Prev_Sprint"), "Pct_of_Max_Points", max_degree = 20)
both_results <- find_optimal_polynomial_degree(both_df, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", max_degree = 20)

# Check the results and find the best degree based on test R²
df_results
distance_results
sprint_results
both_results

```


Over many trials, it seems that three was the most consistent.  Now let's use that term.

```{r poly-reg}
library(ggplot2)
library(dplyr)

# Function to create the polynomial model, print summary, calculate custom R², and plot results
create_and_plot_model_polynomial <- function(df, cols, y_label, title, degree = 2) {
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data
  
  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Create polynomial terms for each independent variable
  poly_terms <- sapply(cols, function(col) paste("poly(", col, ", degree =", degree, ")", sep = ""))
  formula_string <- paste(y_label, "~", paste(poly_terms, collapse = " + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the polynomial model
  model <- lm(formula, data = df_train)
  print(summary(model))  # Print model summary
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))
  
  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  # ggplot for Predicted vs Actual
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", formula = formula, color = "red", se = FALSE) +
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
}

# Prepare y (Pct_of_Max_Points)
create_and_plot_model_polynomial(df, c("Prev_Pelo"), "Pct_of_Max_Points", "Polynomial Regression: Pct_of_Max_Points ~ Prev_Pelo", degree = 5)

create_and_plot_model_polynomial(distance_df, c("Prev_Distance"), "Pct_of_Max_Points", "Polynomial Regression: Pct_of_Max_Points ~ Prev_Distance", degree = 5)

create_and_plot_model_polynomial(sprint_df, c("Prev_Sprint"), "Pct_of_Max_Points", "Polynomial Regression: Pct_of_Max_Points ~ Prev_Sprint", degree = 5)

create_and_plot_model_polynomial(both_df, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Polynomial Regression: Pct_of_Max_Points ~ Distance + Sprint", degree = 5)


```

Let's try this with a tobit model

### Tobit Regression
```{r tobit-reg}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(AER)  # For Tobit model

# Function to create the Tobit model, print summary, calculate custom R², and plot results
create_and_plot_model_tobit <- function(df, col, X_label, y_label, title) {
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data

  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]

  # Create the formula for the model
  formula_string <- paste(y_label, "~", paste(col, collapse = " + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)

  # Train the Tobit model
  model <- tobit(formula, data = df_train, left = .05)  # Assuming left-censoring at 0
  print(summary(model))  # Print model summary

  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data

  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)

  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)

  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)

  # ggplot for Predicted vs Actual
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the titl
  
}

# Prepare y (Pct_of_Max_Points)

create_and_plot_model_tobit(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "Tobit Model: Pct_of_Max_Points ~ Prev_Pelo")

create_and_plot_model_tobit(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "Tobit Model: Pct_of_Max_Points ~ Prev_Distance")

create_and_plot_model_tobit(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "Tobit Model: Pct_of_Max_Points ~ Sprint")

create_and_plot_model_tobit(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint", "Pct_of_Max_Points", "Tobit Model: Pct_of_Max_Points ~ Distance + Sprint")

```


We see that from the Tobit model is actually pretty solid.  Let's move onto segmented regression.

### Segmented Regression
``` {r spline-reg}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(splines)  # For piecewise cubic regression

# Function to create the model, print summary, calculate custom R², and plot results
create_and_plot_model_spline <- function(df, col, X_label, y_label, title, knots) {
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data

  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]

  # Create spline terms for multiple columns
  spline_terms <- sapply(col, function(c) paste("bs(", c, ", knots = c(", paste(knots, collapse = ", "), "), degree = 3)", sep = ""))
  formula_string <- paste(y_label, "~", paste(spline_terms, collapse = " + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)

  # Train the model
  model <- lm(formula, data = df_train)
  print(summary(model))  # Print model summary

  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data

  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)

  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)

  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)

  # ggplot for Predicted vs Actual
  p1 <- ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  print(p1)
  plot(model)
}

# Prepare y (Pct_of_Max_Points)

# Specify knots for your models; adjust the knots based on your data
knots_prev_pelo <- c(1300, 1500, 1700)

# Call the function with single variable for each case
create_and_plot_model_spline(df, "Prev_Pelo", "Prev_Pelo", "Pct_of_Max_Points", "Piecewise Cubic: Pct_of_Max_Points ~ Prev_Pelo", knots_prev_pelo)
create_and_plot_model_spline(distance_df, "Prev_Distance", "Prev_Distance", "Pct_of_Max_Points", "Piecewise Cubic: Pct_of_Max_Points ~ Prev_Distance", knots_prev_pelo)
create_and_plot_model_spline(sprint_df, "Prev_Sprint", "Prev_Sprint", "Pct_of_Max_Points", "Piecewise Cubic: Pct_of_Max_Points ~ Prev_Sprint", knots_prev_pelo)

# Call the function with both Distance and Sprint
create_and_plot_model_spline(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev Distance + Prev Sprint", "Pct_of_Max_Points", "Piecewise Cubic: Pct_of_Max_Points ~ Distance + Sprint", knots_prev_pelo)


```
Now trying it without splines where each one has it's own regression
```{r seg-reg}
library(gridExtra)
# Create piecewise variables
create_piecewise_vars <- function(df, knots) {
  df_copy <- df  # Create a copy to avoid overwriting
  for (i in seq_along(knots)) {
    if (i == 1) {
      df_copy[[paste0("piece_", i)]] <- pmax(0, df_copy$Prev_Pelo - knots[i])
    } else {
      df_copy[[paste0("piece_", i)]] <- pmax(0, df_copy$Prev_Pelo - knots[i]) - pmax(0, df_copy$Prev_Pelo - knots[i - 1])
    }
  }
  return(df_copy)
}

# Function to create and plot the piecewise model
create_and_plot_piecewise_model <- function(df, y_label, title) {
  # Define knots
  knots <- c(1200, 1300, 1400, 1500, 1600, 1650)
  df <- create_piecewise_vars(df, knots)
  
  # Create the formula for the model
  formula_string <- paste(y_label, "~", paste(paste0("piece_", seq_along(knots)), collapse = " + "))
  formula <- as.formula(formula_string)
  
  # Train the model
  model <- lm(formula, data = df)
  print(summary(model))  # Print model summary
  
  # Make predictions
  df$Predicted <- predict(model)
  
  # Calculate custom weighted R²
  y_train <- df[[y_label]]
  custom_weight <- y_train  # Using actual values as weights
  weighted_rss <- sum(custom_weight * (y_train - df$Predicted)^2)
  weighted_tss <- sum(custom_weight * (y_train - mean(y_train))^2)
  custom_r2 <- 1 - (weighted_rss / weighted_tss)
  
  # Print custom R²
  print(paste("Custom R²:", custom_r2))
  
  # Plot predicted vs actual
  p1 <- ggplot(df, aes(x = !!sym(y_label), y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = "Predicted vs Actual", x = "Actual Values", y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  print(p1)  # Print the first plot
  
  # Plot the piecewise regression line
  p2 <- ggplot(df, aes(x = Prev_Pelo, y = !!sym(y_label))) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_line(aes(y = Predicted), color = "red") +
    labs(title = title, x = "Prev_Pelo", y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  print(p2)  # Print the second plot
  
  # Optionally plot diagnostics
  plot(model)
}

# Create and plot the piecewise models
create_and_plot_piecewise_model(df, "Pct_of_Max_Points", "Piecewise Linear Regression: Pct_of_Max_Points ~ Prev_Pelo")
#create_and_plot_piecewise_model(distance_df, "Pct_of_Max_Points", "Piecewise Linear Regression: Pct_of_Max_Points ~ Prev_Distance")
#create_and_plot_piecewise_model(sprint_df, "Pct_of_Max_Points", "Piecewise Linear Regression: Pct_of_Max_Points ~ Prev_Sprint")
#create_and_plot_piecewise_model(both_df, "Pct_of_Max_Points", "Piecewise Linear Regression: Pct_of_Max_Points ~ Distance + Sprint")


```

The segmented regression needs to be reworked.  Let's try knn to finish

### KNN Regression
``` {r knn-reg}
library(FNN)
library(ggplot2)
library(gridExtra)

create_and_plot_knn_model <- function(df, y_label, explanatory_vars, title, n_neighbors) {
  set.seed(42)

  # Split the data
  knn_idx = sample(seq_len(nrow(df)), size = 0.8 * nrow(df))
  
  trn_df <- df[knn_idx, ]
  tst_df <- df[-knn_idx, ]
  
  # Prepare training and testing data
  X_trn_df <- as.matrix(trn_df[, explanatory_vars, drop = FALSE])
  y_trn_df <- as.numeric(trn_df[[y_label]])
  X_tst_df <- as.matrix(tst_df[, explanatory_vars, drop = FALSE])
  y_tst_df <- as.numeric(tst_df[[y_label]])
  
  # Make predictions on both the training and test sets
  predictions_train <- knn.reg(train = X_trn_df, test = X_trn_df, y = y_trn_df, k = n_neighbors)
  predictions_test <- knn.reg(train = X_trn_df, test = X_tst_df, y = y_trn_df, k = n_neighbors)
    residuals_test <- y_tst_df - predictions_test$pred
  
  # 1. Predicted vs. Actual plot
  plot1 <- ggplot(data.frame(Actual = y_tst_df, Predicted = predictions_test$pred), aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue") +
    geom_abline(slope = 1, intercept = 0, color = "red") +
    labs(title = paste("Predicted vs Actual -", title),
         x = "Actual",
         y = "Predicted") +
    theme_minimal()
  
  # 2. Residuals vs. Fitted plot
  plot2 <- ggplot(data.frame(Fitted = predictions_test$pred, Residuals = residuals_test), aes(x = Fitted, y = Residuals)) +
    geom_point(color = "blue") +
    geom_hline(yintercept = 0, color = "red") +
    labs(title = paste("Residuals vs Fitted -", title),
         x = "Fitted",
         y = "Residuals") +
    theme_minimal()
  
  # 3. Q-Q plot for residuals
  plot3 <- ggplot(data.frame(Residuals = residuals_test), aes(sample = Residuals)) +
    stat_qq(color = "blue") +
    stat_qq_line(color = "red") +
    labs(title = paste("Q-Q Plot of Residuals -", title)) +
    theme_minimal()
  
  # 4. Histogram of residuals
  plot4 <- ggplot(data.frame(Residuals = residuals_test), aes(x = Residuals)) +
    geom_histogram(color = "black", fill = "blue", bins = 30) +
    labs(title = paste("Residuals Histogram -", title),
         x = "Residuals",
         y = "Count") +
    theme_minimal()

  # Arrange the plots in a grid
  grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
  
  # Calculate custom R² for training data
  y_train_mean <- mean(y_trn_df)
  custom_weight_train <- y_trn_df  # Using the actual y values for weights
  weighted_rss_train <- sum(custom_weight_train * (y_trn_df - predictions_train$pred) ^ 2)
  weighted_tss_train <- sum(custom_weight_train * (y_trn_df - y_train_mean) ^ 2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom R² for testing data
  y_test_mean <- mean(y_tst_df)
  custom_weight_test <- y_tst_df  # Using the actual y values for weights
  weighted_rss_test <- sum(custom_weight_test * (y_tst_df - predictions_test$pred) ^ 2)
  weighted_tss_test <- sum(custom_weight_test * (y_tst_df - y_test_mean) ^ 2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)

  # Print model summary
  print(paste("KNN Regression Model Summary for", title, "(n_neighbors:", n_neighbors, ")"))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  # Create a data frame for plotting the test set
  plot_data <- data.frame(
    Actual = y_tst_df,
    Predicted = predictions_test$pred
  )
  
  # Plot the results for the test set
  ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue') +
    geom_abline(slope = 1, intercept = 0, color = 'red', size = 1) +
    labs(title = paste("KNN Regression: Predicted vs Actual -", title),
         x = paste("Actual", y_label),
         y = paste("Predicted", y_label)) +
    theme_minimal()
}

# Example usage:

create_and_plot_knn_model(df, "Pct_of_Max_Points", c("Prev_Pelo"), "KNN(Pct_of_Max_Points) ~ Prev_Pelo", n_neighbors = 5)
create_and_plot_knn_model(distance_df, "Pct_of_Max_Points", c("Prev_Distance"), "KNN(Pct_of_Max_Points) ~ Prev_Distance", n_neighbors = 5)
create_and_plot_knn_model(sprint_df, "Pct_of_Max_Points", c("Prev_Sprint"), "KNN(Pct_of_Max_Points) ~ Prev_Sprint", n_neighbors = 5)

create_and_plot_knn_model(both_df, "Pct_of_Max_Points", c("Prev_Distance", "Prev_Sprint"), "KNN(Pct_of_Max_Points) ~ Prev_Distance + Prev_Sprint", n_neighbors = 5)





```

```{r k-opt}
library(FNN)
library(ggplot2)
library(gridExtra)

# Modify the function to return the custom R² values
create_and_plot_knn_model <- function(df, y_label, explanatory_vars, title, n_neighbors) {
  set.seed(42)

  # Split the data
  knn_idx <- sample(seq_len(nrow(df)), size = 0.8 * nrow(df))
  
  trn_df <- df[knn_idx, ]
  tst_df <- df[-knn_idx, ]
  
  # Prepare training and testing data
  X_trn_df <- as.matrix(trn_df[, explanatory_vars, drop = FALSE])
  y_trn_df <- as.numeric(trn_df[[y_label]])
  X_tst_df <- as.matrix(tst_df[, explanatory_vars, drop = FALSE])
  y_tst_df <- as.numeric(tst_df[[y_label]])
  
  # Make predictions on both the training and test sets
  predictions_train <- knn.reg(train = X_trn_df, test = X_trn_df, y = y_trn_df, k = n_neighbors)
  predictions_test <- knn.reg(train = X_trn_df, test = X_tst_df, y = y_trn_df, k = n_neighbors)
  residuals_test <- y_tst_df - predictions_test$pred
  
  # Calculate custom R² for training data
  y_train_mean <- mean(y_trn_df)
  custom_weight_train <- y_trn_df  # Using the actual y values for weights
  weighted_rss_train <- sum(custom_weight_train * (y_trn_df - predictions_train$pred) ^ 2)
  weighted_tss_train <- sum(custom_weight_train * (y_trn_df - y_train_mean) ^ 2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom R² for testing data
  y_test_mean <- mean(y_tst_df)
  custom_weight_test <- y_tst_df  # Using the actual y values for weights
  weighted_rss_test <- sum(custom_weight_test * (y_tst_df - predictions_test$pred) ^ 2)
  weighted_tss_test <- sum(custom_weight_test * (y_tst_df - y_test_mean) ^ 2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Return R² for training and testing
  return(list(custom_r2_train = custom_r2_train, custom_r2_test = custom_r2_test))
}

# Function to find optimal K
find_optimal_k <- function(df, y_label, explanatory_vars, title, k_values) {
  results <- data.frame(K = k_values, Train_R2 = NA, Test_R2 = NA)

  for (i in seq_along(k_values)) {
    k <- k_values[i]
    model_result <- create_and_plot_knn_model(df, y_label, explanatory_vars, title, k)
    results$Train_R2[i] <- model_result$custom_r2_train
    results$Test_R2[i] <- model_result$custom_r2_test
  }

  # Plotting the R² values for different K
  kplot <- ggplot(results, aes(x = K)) +
    geom_line(aes(y = Train_R2, color = "Train R²")) +
    geom_line(aes(y = Test_R2, color = "Test R²")) +
    labs(title = paste("Optimal K Selection -", title),
         x = "Number of Neighbors (K)",
         y = "Custom R²") +
    scale_color_manual(values = c("Train R²" = "blue", "Test R²" = "red")) +
    theme_minimal()
  
  # Find the optimal K (maximizing Test R²)
  optimal_k <- results$K[which.max(results$Test_R2)]
  print(kplot)
  print(paste("Optimal K:", optimal_k))
  return(optimal_k)
}

# Example usage to find optimal K
k_values <- seq(1, 20)  # Trying K from 1 to 20

optimal_k_prev_pelo <- find_optimal_k(df, "Pct_of_Max_Points", c("Prev_Pelo"), "KNN(Pct_of_Max_Points) ~ Prev_Pelo", k_values)
optimal_k_prev_distance <- find_optimal_k(distance_df, "Pct_of_Max_Points", c("Prev_Distance"), "KNN(Pct_of_Max_Points) ~ Prev_Distance", k_values)
optimal_k_prev_sprint <- find_optimal_k(sprint_df, "Pct_of_Max_Points", c("Prev_Sprint"), "KNN(Pct_of_Max_Points) ~ Prev_Sprint", k_values)

optimal_k_both <- find_optimal_k(both_df, "Pct_of_Max_Points", c("Prev_Distance", "Prev_Sprint"), "KNN(Pct_of_Max_Points) ~ Prev_Distance + Prev_Sprint", k_values)

```
### GAM Regression

Let's try a GAM regression for the last thing

```{r GAM-reg}
library(mgcv)
library(ggplot2)
library(dplyr)

# Function to create the GAM model, print summary, calculate custom R², and plot results
create_and_plot_gam <- function(df, col, X_label, y_label, title) {
  train_index <- sample(1:nrow(df), 0.8 * nrow(df))  # Train/test split
  df_train <- df[train_index, ]  # Training data
  df_test <- df[-train_index, ]   # Testing data
  
  # Extract dependent variable
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Create the formula for the model (use s() to fit smooth terms)
  smooth_terms <- paste("s(", col, ")", collapse=" + ")  # Adding smooth terms for each predictor
  formula_string <- paste(y_label, "~", smooth_terms)
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the GAM model
  gam_model <- gam(formula, data = df_train)
  print(summary(gam_model))  # Print model summary
  
  # Make predictions
  y_pred_train <- predict(gam_model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(gam_model, newdata = df_test)    # Predictions on testing data
  
  # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))
  
  # Create a data frame for plotting predictions vs actual values
  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  # Plot smooth terms (GAM smooths)
  plot(gam_model, se = TRUE, rug = TRUE)
  
  # ggplot for Predicted vs Actual
  p1 <- ggplot(plot_data, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear fit line for reference
    labs(title = title,
         x = y_label,
         y = "Predicted Values") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  print(p1)
  par(mfrow = c(2, 2))

  # Plot residual diagnostic plots for the GAM model
  gam.check(gam_model)
  
  # Reset the plotting area to default (1 plot at a time)
  par(mfrow = c(1, 1))
}



# Prepare data and call the function for different predictors

# GAM with Prev_Pelo
create_and_plot_gam(df, c("Prev_Pelo"), "Prev_Pelo", "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ s(Prev_Pelo)")

# GAM with Prev_Distance
create_and_plot_gam(distance_df, c("Prev_Distance"), "Prev_Distance", "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ s(Prev_Distance)")

# GAM with Prev_Sprint
create_and_plot_gam(sprint_df, c("Prev_Sprint"), "Prev_Sprint", "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ s(Prev_Sprint)")

# GAM with both Prev_Distance and Prev_Sprint
create_and_plot_gam(both_df, c("Prev_Distance", "Prev_Sprint"), "Prev_Distance + Prev_Sprint", "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ s(Prev_Distance) + s(Prev_Sprint)")

```


## Predictions for the Models


### Linear Regression
```{r lin-pred}
library(ggplot2)
library(dplyr)

# Function to create the model, perform bootstrapping, and return averaged predictions
linear_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Create the formula for the model
    formula_string <- paste(y_label, "~", paste(explanatory_vars, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Train the model
    model <- lm(formula, data = df_train_boot)
    
    # Use 2024 data to predict for the 2025 season
    predictions_matrix[, i] <- predict(model, newdata = df_2024)
    
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025 <- rowMeans(predictions_matrix)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season
# Using Prev_Pelo to predict for 2025 with bootstrapping
predictions_2025_prev_pelo <- linear_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Pelo", num_bootstrap)


# Using Prev_Distance and Prev_Sprint to predict for 2025 with bootstrapping
predictions_2025_both <- linear_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", "Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation",  "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation",  "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)

lindf <- both_df_sorted[,c(1, 6)]
colnames(lindf) <- c("Skier", "Linear Regression")
print(lindf)

```

### Log Regression
```{r log-pred}
library(ggplot2)
library(dplyr)

# Function to create the model, perform bootstrapping, and return averaged predictions with log transformation
log_create_bootstrap_model_and_predict<- function(df_train, df_2024, explanatory_vars, y_label, title, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Log-transform the dependent variable
    df_train_boot[[y_label]] <- log(df_train_boot[[y_label]] + .0001)
    
    # Create the formula for the model
    formula_string <- paste(y_label, "~", paste(explanatory_vars, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Train the model
    model <- lm(formula, data = df_train_boot)
    
    # Predict (2024 data is not logged)
    predictions_matrix[, i] <- (predict(model, newdata = df_2024))
  }
  
  # Take the average of all bootstrap predictions
  log_predictions_2025 <- rowMeans(predictions_matrix)
  
  # Exponentiate the averaged predictions to return them to the original scale
  predictions_2025 <- exp(log_predictions_2025)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- exp(apply(predictions_matrix, 1, function(x) quantile(x, 0.025)))  # 2.5th percentile
  ci_upper <- exp(apply(predictions_matrix, 1, function(x) quantile(x, 0.975)))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
distance_df_2024 <- distance_df %>% filter(Season == 2024)
sprint_df_2024 <- sprint_df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season with log transformation
# Using Prev_Pelo to predict for 2025 with bootstrapping
predictions_2025_prev_pelo <- log_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", num_bootstrap)

# Using Prev_Distance and Prev_Sprint to predict for 2025 with bootstrapping
predictions_2025_both <- log_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions

both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation",  "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)



# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation",  "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)

logdf <- both_df_sorted[,c(1, 6)]
colnames(logdf) <- c("Skier", "Log Regression")
print(logdf)


```

### Square Root Regression
``` {r sqroot-pred}
library(ggplot2)
library(dplyr)

# Function to create the model, perform bootstrapping, and return averaged predictions with square root transformation
sqrt_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Square root transform the dependent variable
    df_train_boot[[y_label]] <- sqrt(df_train_boot[[y_label]])
    
    # Create the formula for the model
    formula_string <- paste(y_label, "~", paste(explanatory_vars, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Train the model
    model <- lm(formula, data = df_train_boot)
    
    # Use 2024 data to predict for the 2025 season
    predictions_matrix[, i] <- predict(model, newdata = df_2024)
  }
  
  # Take the average of all bootstrap predictions
  sqrt_predictions_2025 <- rowMeans(predictions_matrix)
  
  # Square the averaged predictions to return them to the original scale
  predictions_2025 <- sqrt_predictions_2025^2
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season
# Using Prev_Pelo to predict for 2025 with bootstrapping and square root transformation
predictions_2025_prev_pelo <- sqrt_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", num_bootstrap)


# Using Prev_Distance and Prev_Sprint to predict for 2025 with bootstrapping and square root transformation
predictions_2025_both <- sqrt_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation",  "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation",  "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)

# Creating a new dataframe for the linear regression predictions
sqdf <- both_df_sorted[,c(1, 6)]
colnames(sqdf) <- c("Skier", "Square Root Regression")
print(sqdf)


```


### Weighted Regression
```{r weight-pred}
library(ggplot2)
library(dplyr)

# Function to create the weighted regression model with bootstrapping and return averaged predictions
weight_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Create the formula for the model
    formula_string <- paste(y_label, "~", paste(explanatory_vars, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Weights based on actual y values (Pct_of_Max_Points)
    weights <- df_train_boot[[y_label]]
    
    # Train the model with weights
    model <- lm(formula, data = df_train_boot, weights = weights)
    
    # Use 2024 data to predict for the 2025 season
    predictions_matrix[, i] <- predict(model, newdata = df_2024)
    
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025 <- rowMeans(predictions_matrix)
  
    # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))

}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season using weighted regression with bootstrapping

# Using Prev_Pelo to predict for 2025
predictions_2025_prev_pelo <- weight_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", "Weighted Regression: Pct_of_Max_Points ~ Prev_Pelo", num_bootstrap)



# Using Prev_Distance and Prev_Sprint to predict for 2025
predictions_2025_both <- weight_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", "Weighted Regression: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation", "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)



# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation", "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)
wdf <- both_df_sorted[,c(1, 6)]
colnames(wdf) <- c("Skier", "Weighted Regression")
print(wdf)


```


### Quantile regression

```{r quant-pred}
# Install and load the quantreg package (if not already installed)
if (!require(quantreg)) install.packages("quantreg")
library(quantreg)
library(ggplot2)
library(dplyr)

# Function to create the quantile regression model with bootstrapping and return predictions
quantile_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, tau_value, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Create the formula for the model
    formula_string <- paste(y_label, "~", paste(explanatory_vars, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Train the quantile regression model with specified tau
    model <- rq(formula, data = df_train_boot, tau = tau_value)
    
    # Use 2024 data to predict for the 2025 season
    predictions_matrix[, i] <- predict(model, newdata = df_2024)
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025 <- rowMeans(predictions_matrix)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season using quantile regression with bootstrapping

# Using Prev_Pelo to predict for 2025
predictions_2025_prev_pelo <- quantile_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", "Quantile Regression: Pct_of_Max_Points ~ Prev_Pelo", 0.9, num_bootstrap)


# Using Prev_Distance and Prev_Sprint to predict for 2025
predictions_2025_both <- quantile_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", "Quantile Regression: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", 0.9, num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation", "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation", "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)
qdf <- both_df_sorted[,c(1, 6)]
colnames(qdf) <- c("Skier", "Quantile Regression")
print(qdf)


```
### Polynomial Regression
```{r poly-pred}
library(ggplot2)
library(dplyr)

# Function to create the polynomial regression model with bootstrapping and return predictions
poly_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Create the formula for the polynomial model (degree 3)
    formula_terms <- unlist(lapply(explanatory_vars, function(var) {
      c(paste("I(", var, ")", sep = ""),
        paste("I(", var, "^2)", sep = ""),
        paste("I(", var, "^3)", sep = ""))
    }))
    
    # Join the terms correctly with " + "
    formula_string <- paste(y_label, "~", paste(formula_terms, collapse = " + "))
    
    # Convert string to formula
    formula <- as.formula(formula_string)
    
    # Train the polynomial regression model
    model <- lm(formula, data = df_train_boot)
    
    # Use 2024 data to predict for the 2025 season
    predictions_matrix[, i] <- predict(model, newdata = df_2024)
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025 <- rowMeans(predictions_matrix)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
distance_df_2024 <- distance_df %>% filter(Season == 2024)
sprint_df_2024 <- sprint_df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season using polynomial regression with bootstrapping

# Using Prev_Pelo to predict for 2025
predictions_2025_prev_pelo <- poly_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", "Polynomial Regression: Pct_of_Max_Points ~ Prev_Pelo", num_bootstrap)


# Using Prev_Distance and Prev_Sprint to predict for 2025
predictions_2025_both <- poly_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", "Polynomial Regression: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation", "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation", "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)
polydf <- both_df_sorted[,c(1, 6)]
colnames(polydf) <- c("Skier", "Polynomial Regression")
print(polydf)


```

### Tobit Regression
```{r tobit-pred}
library(ggplot2)
library(dplyr)
library(AER)  # For Tobit regression

# Function to create the Tobit model with bootstrapping and return predictions
tobit_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, left = 0, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Create the formula for the model
    formula_string <- paste(y_label, "~", paste(explanatory_vars, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Train the Tobit model
    model <- tobit(formula, data = df_train_boot, left = left)
    
    # Use 2024 data to predict for the 2025 season
    predictions_matrix[, i] <- predict(model, newdata = df_2024)
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025 <- rowMeans(predictions_matrix)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season using Tobit regression with bootstrapping

# Using Prev_Pelo to predict for 2025
predictions_2025_prev_pelo <- tobit_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", "Tobit: Pct_of_Max_Points ~ Prev_Pelo", left = 0, num_bootstrap = num_bootstrap)


# Using Prev_Distance and Prev_Sprint to predict for 2025
predictions_2025_both <- tobit_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", "Tobit: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", left = 0, num_bootstrap = num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation", "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation", "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)
tobitdf <- both_df_sorted[,c(1, 6)]
colnames(tobitdf) <- c("Skier", "Tobit Regression")
print(tobitdf)



```

### Segmented Regression (cubic spline)
```{r spline-pred}
library(ggplot2)
library(dplyr)
library(splines)  # For spline regression

# Function to create the model with cubic splines and return bootstrapped predictions
spline_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, knots, title, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Ensure knots are properly printed
  
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Apply bs() to each explanatory variable in the formula
    spline_terms <- sapply(explanatory_vars, function(var) {
      paste("bs(", var, ", knots = c(", paste(knots, collapse = ", "), "), degree = 3)")
    })
    
    # Create the full formula string by concatenating the spline terms
    formula_string <- paste(y_label, "~", paste(spline_terms, collapse = " + "))
    
    formula <- as.formula(formula_string)  # Convert string to formula
    
    # Train the model with cubic splines
    model <- lm(formula, data = df_train_boot)
    
    # Use df_2024 to predict for the new season
    predictions_matrix[, i] <- predict(model, newdata = df_2024)
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025 <- rowMeans(predictions_matrix)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}


# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Specify knots for cubic spline
knots <- c(1200, 1300, 1400, 1500, 1600, 1700)

# Set the number of bootstrap iterations
num_bootstrap <- 100

# Predict Pct_of_Max_Points for 2025 based on the 2024 season using cubic spline regression with bootstrapping
# Using Prev_Pelo to predict for 2025 with cubic splines and bootstrapping
predictions_2025_prev_pelo <- spline_create_bootstrap_model_and_predict(df, df_2024, "Pelo", "Pct_of_Max_Points", knots, "Cubic Spline: Pct_of_Max_Points ~ Prev_Pelo", num_bootstrap)


# Using Prev_Distance and Prev_Sprint to predict for 2025 with cubic splines and bootstrapping
# Note: You'll need to handle both Distance_Pelo and Sprint_Pelo splines together here if both are used in the model.
predictions_2025_both <- spline_create_bootstrap_model_and_predict(both_df, both_df_2024, "Distance_Pelo + Sprint_Pelo", "Pct_of_Max_Points", knots, "Cubic Spline: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation",  "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation",  "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]

(both_df_sorted)
splinedf <- both_df_sorted[,c(1, 6)]
colnames(splinedf) <- c("Skier", "Spline Regression")
print(splinedf)


```

### KNN Regression
```{r KNN-pred}
library(ggplot2)
library(dplyr)
library(FNN)  # For KNN regression

# Function to create the KNN model with bootstrapping and return predictions
knn_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, k = 5, num_bootstrap = 100) {
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)  # To store bootstrapped predictions
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Extract explanatory variables and response variables for training
    X_train <- df_train_boot[, explanatory_vars, drop = FALSE]  # Use base R subsetting
    y_train <- df_train_boot[[y_label]]
    
    # Extract explanatory variables from 2024 data for prediction
    X_2024 <- df_2024[, explanatory_vars, drop = FALSE]  # Use base R subsetting
    
    # Train KNN model and make predictions for 2025 season using 2024 data
    knn_model <- knn.reg(train = X_train, test = X_2024, y = y_train, k = k)
    
    # Store predictions for this bootstrap iteration
    predictions_matrix[, i] <- knn_model$pred
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025 <- rowMeans(predictions_matrix)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)
both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can change this number to control the number of bootstrap samples

# Predict Pct_of_Max_Points for 2025 based on the 2024 season using KNN regression with bootstrapping

# Using Prev_Pelo to predict for 2025 (with k = 7)
predictions_2025_prev_pelo <- knn_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", "KNN: Pct_of_Max_Points ~ Prev_Pelo", k = 7, num_bootstrap = num_bootstrap)

# Using Prev_Distance and Prev_Sprint to predict for 2025 (with k = 7)
predictions_2025_both <- knn_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", "KNN: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", k = 7, num_bootstrap = num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation", "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]
print(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation", "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]
print(both_df_sorted)
knndf <- both_df_sorted[,c(1, 6)]
colnames(knndf) <- c("Skier", "KNN Regression")
print(knndf)


```

### GAM Regression
```{r GAM-pred}
library(ggplot2)
library(dplyr)
library(mgcv)  # For GAM

# Function to create the GAM model with bootstrapping and return predictions
gam_create_bootstrap_model_and_predict <- function(df_train, df_2024, explanatory_vars, y_label, title, num_bootstrap = 100) {
  # Matrix to store predictions from each bootstrap sample
  predictions_matrix <- matrix(NA, nrow = nrow(df_2024), ncol = num_bootstrap)
  
  # Perform bootstrapping
  for (i in 1:num_bootstrap) {
    # Bootstrap resample
    boot_index <- sample(1:nrow(df_train), size = nrow(df_train), replace = TRUE)
    df_train_boot <- df_train[boot_index, ]
    
    # Create the formula for the GAM model, adding smooth terms (s) to the explanatory variables
    smooth_terms <- sapply(explanatory_vars, function(var) paste("s(", var, ")", sep = ""))
    formula_string <- paste(y_label, "~", paste(smooth_terms, collapse = " + "))
    formula <- as.formula(formula_string)
    
    # Train the GAM model on the bootstrap sample
    gam_model <- gam(formula, data = df_train_boot)
    
    # Use the 2024 data to predict for the 2025 season
    predictions_2025 <- predict(gam_model, newdata = df_2024)
    
    # Store predictions for this bootstrap iteration
    predictions_matrix[, i] <- predictions_2025
  }
  
  # Take the average of all bootstrap predictions
  predictions_2025_avg <- rowMeans(predictions_matrix)
  
  # Calculate the 2.5th and 97.5th percentiles for the confidence intervals
  ci_lower <- apply(predictions_matrix, 1, function(x) quantile(x, 0.025))  # 2.5th percentile
  ci_upper <- apply(predictions_matrix, 1, function(x) quantile(x, 0.975))  # 97.5th percentile
  
  # Return the averaged predictions and the confidence intervals
  return(list(predictions = predictions_2025, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Prepare data for 2025 predictions using 2024 data
df_2024 <- df %>% filter(Season == 2024)

both_df_2024 <- both_df %>% filter(Season == 2024)

# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can adjust the number of bootstrap iterations

# Predict Pct_of_Max_Points for 2025 based on the 2024 season using GAM regression with bootstrapping

# Using Prev_Pelo to predict for 2025
predictions_2025_prev_pelo <- gam_create_bootstrap_model_and_predict(df, df_2024, c("Pelo"), "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ Prev_Pelo", num_bootstrap = num_bootstrap)


# Using Prev_Distance and Prev_Sprint to predict for 2025
predictions_2025_both <- gam_create_bootstrap_model_and_predict(both_df, both_df_2024, c("Distance_Pelo", "Sprint_Pelo"), "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)

# Store predictions for further use or analysis
df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_prev_pelo$predictions
both_df_2024$Predicted_Pct_of_Max_Points_2025 <- predictions_2025_both$predictions

# Sort df_2024 and show only the desired columns
df_2024_sorted <- df_2024[order(-df_2024$Predicted_Pct_of_Max_Points_2025), c("Skier", "Nation", "Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]
print(df_2024_sorted)


# Sort both_df_2024 and show only the desired columns
both_df_sorted <- both_df_2024[order(-both_df_2024$Predicted_Pct_of_Max_Points_2025), 
                               c("Skier", "Nation", "Distance_Pelo", "Sprint_Pelo", "Pct_of_Max_Points", "Predicted_Pct_of_Max_Points_2025")]
print(both_df_sorted)
gamdf <- both_df_sorted[,c(1, 6)]
colnames(gamdf) <- c("Skier", "GAM Regression")
print(gamdf)



```

``` {r pct-df}
library(openxlsx)
# Sequentially merge data frames on the "Skier" column
merged_df <- Reduce(function(x, y) merge(x, y, by = "Skier", all = TRUE), 
                    list(lindf, logdf, sqdf, wdf, qdf, polydf, tobitdf, splinedf, knndf, gamdf))

# Display the merged data frame

merged_df <- merged_df[order(-merged_df$`Linear Regression`), ]
print(merged_df)
write.xlsx(merged_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men-models.xlsx")

```



##Model Competition

### 2020
```{r 2020}
#Get a 2020 df to test with
both_df_2020 <- both_df %>% filter(Season == 2020)
both_dfn_2020 <- both_df %>% filter(Season != 2020)
# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can adjust the number of bootstrap iterations

#Start out with Linear
predictions_2020_linear <- linear_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Linear: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Linear <- predictions_2020_linear$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Linear MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Linear-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Linear RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Linear-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Linear MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Linear-both_df_2020$Pct_of_Max_Points))

#Log
predictions_2020_log <- log_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Log: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Log <- predictions_2020_log$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Log MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Log-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Log RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Log-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Log MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Log-both_df_2020$Pct_of_Max_Points))


predictions_2020_sqrt <- sqrt_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Square Root: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Square_Root <- predictions_2020_sqrt$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Square Root MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Square_Root-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Square Root RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Square_Root-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Square Root MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Square_Root-both_df_2020$Pct_of_Max_Points))

predictions_2020_weight <- weight_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Weighted: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Weighted <- predictions_2020_weight$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Weighted MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Weighted-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Weighted RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Weighted-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Weighted MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Weighted-both_df_2020$Pct_of_Max_Points))

predictions_2020_quantile <- quantile_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Quantile: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", tau=.9, num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Quantile <- predictions_2020_quantile$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Quantile MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Quantile-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Quantile RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Quantile-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Quantile MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Quantile-both_df_2020$Pct_of_Max_Points))

predictions_2020_poly <- poly_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Polynomial: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Polynomial <- predictions_2020_poly$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Polynomial MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Polynomial-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Polynomial RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Polynomial-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Polynomial MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Polynomial-both_df_2020$Pct_of_Max_Points))

predictions_2020_tobit <- tobit_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Tobit: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", left=0, num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Tobit <- predictions_2020_tobit$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Tobit MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Tobit-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Tobit RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Tobit-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Tobit MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Tobit-both_df_2020$Pct_of_Max_Points))

knots <- c(1200, 1300, 1400, 1500, 1600, 1700)


predictions_2020_spline <- spline_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, "Prev_Distance + Prev_Sprint",  "Pct_of_Max_Points", knots,"Spline: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_Spline <- predictions_2020_spline$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`Spline MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Spline-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`Spline RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_Spline-both_df_2020$Pct_of_Max_Points))
both_df_2020$`Spline MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_Spline-both_df_2020$Pct_of_Max_Points))

#Start out with KNN
predictions_2020_knn <- knn_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "KNN: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", 5, num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_KNN <- predictions_2020_knn$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`KNN MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_KNN-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`KNN RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_KNN-both_df_2020$Pct_of_Max_Points))
both_df_2020$`KNN MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_KNN-both_df_2020$Pct_of_Max_Points))

#Start out with GAM
predictions_2020_gam <- gam_create_bootstrap_model_and_predict(both_dfn_2020, both_df_2020, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2020$Predicted_Pct_of_Max_Points_GAM <- predictions_2020_gam$predictions
both_df_2020 <- both_df_2020[order(-both_df_2020$Pct_of_Max_Points), ]
both_df_2020$`GAM MSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_GAM-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`GAM RMSE` <- ((both_df_2020$Predicted_Pct_of_Max_Points_GAM-both_df_2020$Pct_of_Max_Points)^2)
both_df_2020$`GAM MAE` <- (abs(both_df_2020$Predicted_Pct_of_Max_Points_GAM-both_df_2020$Pct_of_Max_Points))

both_df_2020 <- both_df_2020[1:30, ]

linearMSE <- mean(both_df_2020$`Linear MSE`)
linearRMSE <- sqrt(mean(both_df_2020$`Linear MSE`))
linearMAE <- mean(both_df_2020$`Linear MAE`)

logMSE <- mean(both_df_2020$`Log MSE`)
logRMSE <- sqrt(mean(both_df_2020$`Log MSE`))
logMAE <- mean(both_df_2020$`Log MAE`)

sqrtMSE <- mean(both_df_2020$`Square Root MSE`)
sqrtRMSE <- sqrt(mean(both_df_2020$`Square Root MSE`))
sqrtMAE <- mean(both_df_2020$`Square Root MAE`)

weightMSE <- mean(both_df_2020$`Weighted MSE`)
weightRMSE <- sqrt(mean(both_df_2020$`Weighted MSE`))
weightMAE <- mean(both_df_2020$`Weighted MAE`)

quantileMSE <- mean(both_df_2020$`Quantile MSE`)
quantileRMSE <- sqrt(mean(both_df_2020$`Quantile MSE`))
quantileMAE <- mean(both_df_2020$`Quantile MAE`)

polyMSE <- mean(both_df_2020$`Polynomial MSE`)
polyRMSE <- sqrt(mean(both_df_2020$`Polynomial MSE`))
polyMAE <- mean(both_df_2020$`Polynomial MAE`)

tobitMSE <- mean(both_df_2020$`Tobit MSE`)
tobitRMSE <- sqrt(mean(both_df_2020$`Tobit MSE`))
tobitMAE <- mean(both_df_2020$`Tobit MAE`)

splineMSE <- mean(both_df_2020$`Spline MSE`)
splineRMSE <- sqrt(mean(both_df_2020$`Spline MSE`))
splineMAE <- mean(both_df_2020$`Spline MAE`)

knnMSE <- mean(both_df_2020$`KNN MSE`)
knnRMSE <- sqrt(mean(both_df_2020$`KNN MSE`))
knnMAE <- mean(both_df_2020$`KNN MAE`)

gamMSE <- mean(both_df_2020$`GAM MSE`)
gamRMSE <- sqrt(mean(both_df_2020$`GAM MSE`))
gamMAE <- mean(both_df_2020$`GAM MAE`)

mse2020 <- c(linearMSE, logMSE, sqrtMSE, weightMSE, quantileMSE, polyMSE, tobitMSE, splineMSE, knnMSE, gamMSE)
rmse2020 <- c(linearRMSE, logRMSE, sqrtRMSE, weightRMSE, quantileRMSE, polyRMSE, tobitRMSE, splineRMSE, knnRMSE, gamRMSE)
mae2020 <- c(linearMAE, logMAE, sqrtMAE, weightMAE, quantileMAE, polyMAE, tobitMAE, splineMAE, knnMAE, gamMAE)

resid2020 <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile","Polynomial", "Tobit", "Spline", "KNN", "GAM") ,`MSE 2020`=mse2020, `RMSE 2020`=rmse2020, `MAE 2020`=mae2020)


```




### 2021
```{r 2021}
#Get a 2021 df to test with
both_df_2021 <- both_df %>% filter(Season == 2021)
both_dfn_2021 <- both_df %>% filter(Season != 2021)
# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can adjust the number of bootstrap iterations

#Start out with Linear
predictions_2021_linear <- linear_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Linear: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Linear <- predictions_2021_linear$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Linear MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Linear-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Linear RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Linear-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Linear MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Linear-both_df_2021$Pct_of_Max_Points))

#Log
predictions_2021_log <- log_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Log: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Log <- predictions_2021_log$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Log MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Log-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Log RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Log-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Log MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Log-both_df_2021$Pct_of_Max_Points))


predictions_2021_sqrt <- sqrt_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Square Root: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Square_Root <- predictions_2021_sqrt$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Square Root MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Square_Root-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Square Root RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Square_Root-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Square Root MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Square_Root-both_df_2021$Pct_of_Max_Points))

predictions_2021_weight <- weight_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Weighted: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Weighted <- predictions_2021_weight$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Weighted MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Weighted-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Weighted RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Weighted-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Weighted MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Weighted-both_df_2021$Pct_of_Max_Points))

predictions_2021_quantile <- quantile_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Quantile: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", tau=.9, num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Quantile <- predictions_2021_quantile$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Quantile MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Quantile-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Quantile RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Quantile-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Quantile MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Quantile-both_df_2021$Pct_of_Max_Points))

predictions_2021_poly <- poly_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Polynomial: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Polynomial <- predictions_2021_poly$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Polynomial MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Polynomial-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Polynomial RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Polynomial-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Polynomial MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Polynomial-both_df_2021$Pct_of_Max_Points))

predictions_2021_tobit <- tobit_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Tobit: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", left=0, num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Tobit <- predictions_2021_tobit$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Tobit MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Tobit-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Tobit RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Tobit-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Tobit MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Tobit-both_df_2021$Pct_of_Max_Points))

knots <- c(1200, 1300, 1400, 1500, 1600, 1700)


predictions_2021_spline <- spline_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, "Prev_Distance + Prev_Sprint",  "Pct_of_Max_Points", knots,"Spline: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_Spline <- predictions_2021_spline$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`Spline MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Spline-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`Spline RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_Spline-both_df_2021$Pct_of_Max_Points))
both_df_2021$`Spline MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_Spline-both_df_2021$Pct_of_Max_Points))

#Start out with KNN
predictions_2021_knn <- knn_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "KNN: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", 5, num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_KNN <- predictions_2021_knn$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`KNN MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_KNN-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`KNN RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_KNN-both_df_2021$Pct_of_Max_Points))
both_df_2021$`KNN MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_KNN-both_df_2021$Pct_of_Max_Points))

#Start out with GAM
predictions_2021_gam <- gam_create_bootstrap_model_and_predict(both_dfn_2021, both_df_2021, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2021$Predicted_Pct_of_Max_Points_GAM <- predictions_2021_gam$predictions
both_df_2021 <- both_df_2021[order(-both_df_2021$Pct_of_Max_Points), ]
both_df_2021$`GAM MSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_GAM-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`GAM RMSE` <- ((both_df_2021$Predicted_Pct_of_Max_Points_GAM-both_df_2021$Pct_of_Max_Points)^2)
both_df_2021$`GAM MAE` <- (abs(both_df_2021$Predicted_Pct_of_Max_Points_GAM-both_df_2021$Pct_of_Max_Points))

both_df_2021 <- both_df_2021[1:30, ]

linearMSE <- mean(both_df_2021$`Linear MSE`)
linearRMSE <- sqrt(mean(both_df_2021$`Linear MSE`))
linearMAE <- mean(both_df_2021$`Linear MAE`)

logMSE <- mean(both_df_2021$`Log MSE`)
logRMSE <- sqrt(mean(both_df_2021$`Log MSE`))
logMAE <- mean(both_df_2021$`Log MAE`)

sqrtMSE <- mean(both_df_2021$`Square Root MSE`)
sqrtRMSE <- sqrt(mean(both_df_2021$`Square Root MSE`))
sqrtMAE <- mean(both_df_2021$`Square Root MAE`)

weightMSE <- mean(both_df_2021$`Weighted MSE`)
weightRMSE <- sqrt(mean(both_df_2021$`Weighted MSE`))
weightMAE <- mean(both_df_2021$`Weighted MAE`)

quantileMSE <- mean(both_df_2021$`Quantile MSE`)
quantileRMSE <- sqrt(mean(both_df_2021$`Quantile MSE`))
quantileMAE <- mean(both_df_2021$`Quantile MAE`)

polyMSE <- mean(both_df_2021$`Polynomial MSE`)
polyRMSE <- sqrt(mean(both_df_2021$`Polynomial MSE`))
polyMAE <- mean(both_df_2021$`Polynomial MAE`)

tobitMSE <- mean(both_df_2021$`Tobit MSE`)
tobitRMSE <- sqrt(mean(both_df_2021$`Tobit MSE`))
tobitMAE <- mean(both_df_2021$`Tobit MAE`)

splineMSE <- mean(both_df_2021$`Spline MSE`)
splineRMSE <- sqrt(mean(both_df_2021$`Spline MSE`))
splineMAE <- mean(both_df_2021$`Spline MAE`)

knnMSE <- mean(both_df_2021$`KNN MSE`)
knnRMSE <- sqrt(mean(both_df_2021$`KNN MSE`))
knnMAE <- mean(both_df_2021$`KNN MAE`)

gamMSE <- mean(both_df_2021$`GAM MSE`)
gamRMSE <- sqrt(mean(both_df_2021$`GAM MSE`))
gamMAE <- mean(both_df_2021$`GAM MAE`)

mse2021 <- c(linearMSE, logMSE, sqrtMSE, weightMSE, quantileMSE, polyMSE, tobitMSE, splineMSE, knnMSE, gamMSE)
rmse2021 <- c(linearRMSE, logRMSE, sqrtRMSE, weightRMSE, quantileRMSE, polyRMSE, tobitRMSE, splineRMSE, knnRMSE, gamRMSE)
mae2021 <- c(linearMAE, logMAE, sqrtMAE, weightMAE, quantileMAE, polyMAE, tobitMAE, splineMAE, knnMAE, gamMAE)

resid2021 <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile","Polynomial", "Tobit", "Spline", "KNN", "GAM") ,`MSE 2021`=mse2021, `RMSE 2021`=rmse2021, `MAE 2021`=mae2021)


```



### 2022
```{r 2022}
#Get a 2022 df to test with
both_df_2022 <- both_df %>% filter(Season == 2022)
both_dfn_2022 <- both_df %>% filter(Season != 2022)
# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can adjust the number of bootstrap iterations

#Start out with Linear
predictions_2022_linear <- linear_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Linear: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Linear <- predictions_2022_linear$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Linear MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Linear-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Linear RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Linear-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Linear MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Linear-both_df_2022$Pct_of_Max_Points))

#Log
predictions_2022_log <- log_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Log: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Log <- predictions_2022_log$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Log MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Log-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Log RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Log-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Log MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Log-both_df_2022$Pct_of_Max_Points))


predictions_2022_sqrt <- sqrt_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Square Root: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Square_Root <- predictions_2022_sqrt$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Square Root MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Square_Root-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Square Root RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Square_Root-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Square Root MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Square_Root-both_df_2022$Pct_of_Max_Points))

predictions_2022_weight <- weight_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Weighted: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Weighted <- predictions_2022_weight$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Weighted MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Weighted-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Weighted RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Weighted-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Weighted MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Weighted-both_df_2022$Pct_of_Max_Points))

predictions_2022_quantile <- quantile_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Quantile: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", tau=.9, num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Quantile <- predictions_2022_quantile$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Quantile MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Quantile-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Quantile RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Quantile-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Quantile MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Quantile-both_df_2022$Pct_of_Max_Points))

predictions_2022_poly <- poly_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Polynomial: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Polynomial <- predictions_2022_poly$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Polynomial MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Polynomial-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Polynomial RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Polynomial-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Polynomial MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Polynomial-both_df_2022$Pct_of_Max_Points))

predictions_2022_tobit <- tobit_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Tobit: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", left=0, num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Tobit <- predictions_2022_tobit$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Tobit MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Tobit-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Tobit RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Tobit-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Tobit MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Tobit-both_df_2022$Pct_of_Max_Points))

knots <- c(1200, 1300, 1400, 1500, 1600, 1700)


predictions_2022_spline <- spline_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, "Prev_Distance + Prev_Sprint",  "Pct_of_Max_Points", knots,"Spline: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_Spline <- predictions_2022_spline$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`Spline MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Spline-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`Spline RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_Spline-both_df_2022$Pct_of_Max_Points))
both_df_2022$`Spline MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_Spline-both_df_2022$Pct_of_Max_Points))

#Start out with KNN
predictions_2022_knn <- knn_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "KNN: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", 5, num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_KNN <- predictions_2022_knn$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`KNN MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_KNN-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`KNN RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_KNN-both_df_2022$Pct_of_Max_Points))
both_df_2022$`KNN MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_KNN-both_df_2022$Pct_of_Max_Points))

#Start out with GAM
predictions_2022_gam <- gam_create_bootstrap_model_and_predict(both_dfn_2022, both_df_2022, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2022$Predicted_Pct_of_Max_Points_GAM <- predictions_2022_gam$predictions
both_df_2022 <- both_df_2022[order(-both_df_2022$Pct_of_Max_Points), ]
both_df_2022$`GAM MSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_GAM-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`GAM RMSE` <- ((both_df_2022$Predicted_Pct_of_Max_Points_GAM-both_df_2022$Pct_of_Max_Points)^2)
both_df_2022$`GAM MAE` <- (abs(both_df_2022$Predicted_Pct_of_Max_Points_GAM-both_df_2022$Pct_of_Max_Points))

both_df_2022 <- both_df_2022[1:30, ]

linearMSE <- mean(both_df_2022$`Linear MSE`)
linearRMSE <- sqrt(mean(both_df_2022$`Linear MSE`))
linearMAE <- mean(both_df_2022$`Linear MAE`)

logMSE <- mean(both_df_2022$`Log MSE`)
logRMSE <- sqrt(mean(both_df_2022$`Log MSE`))
logMAE <- mean(both_df_2022$`Log MAE`)

sqrtMSE <- mean(both_df_2022$`Square Root MSE`)
sqrtRMSE <- sqrt(mean(both_df_2022$`Square Root MSE`))
sqrtMAE <- mean(both_df_2022$`Square Root MAE`)

weightMSE <- mean(both_df_2022$`Weighted MSE`)
weightRMSE <- sqrt(mean(both_df_2022$`Weighted MSE`))
weightMAE <- mean(both_df_2022$`Weighted MAE`)

quantileMSE <- mean(both_df_2022$`Quantile MSE`)
quantileRMSE <- sqrt(mean(both_df_2022$`Quantile MSE`))
quantileMAE <- mean(both_df_2022$`Quantile MAE`)

polyMSE <- mean(both_df_2022$`Polynomial MSE`)
polyRMSE <- sqrt(mean(both_df_2022$`Polynomial MSE`))
polyMAE <- mean(both_df_2022$`Polynomial MAE`)

tobitMSE <- mean(both_df_2022$`Tobit MSE`)
tobitRMSE <- sqrt(mean(both_df_2022$`Tobit MSE`))
tobitMAE <- mean(both_df_2022$`Tobit MAE`)

splineMSE <- mean(both_df_2022$`Spline MSE`)
splineRMSE <- sqrt(mean(both_df_2022$`Spline MSE`))
splineMAE <- mean(both_df_2022$`Spline MAE`)

knnMSE <- mean(both_df_2022$`KNN MSE`)
knnRMSE <- sqrt(mean(both_df_2022$`KNN MSE`))
knnMAE <- mean(both_df_2022$`KNN MAE`)

gamMSE <- mean(both_df_2022$`GAM MSE`)
gamRMSE <- sqrt(mean(both_df_2022$`GAM MSE`))
gamMAE <- mean(both_df_2022$`GAM MAE`)

mse2022 <- c(linearMSE, logMSE, sqrtMSE, weightMSE, quantileMSE, polyMSE, tobitMSE, splineMSE, knnMSE, gamMSE)
rmse2022 <- c(linearRMSE, logRMSE, sqrtRMSE, weightRMSE, quantileRMSE, polyRMSE, tobitRMSE, splineRMSE, knnRMSE, gamRMSE)
mae2022 <- c(linearMAE, logMAE, sqrtMAE, weightMAE, quantileMAE, polyMAE, tobitMAE, splineMAE, knnMAE, gamMAE)

resid2022 <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile","Polynomial", "Tobit", "Spline", "KNN", "GAM") ,`MSE 2022`=mse2022, `RMSE 2022`=rmse2022, `MAE 2022`=mae2022)


```



### 2023
```{r 2023}
#Get a 2023 df to test with
both_df_2023 <- both_df %>% filter(Season == 2023)
both_dfn_2023 <- both_df %>% filter(Season != 2023)
# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can adjust the number of bootstrap iterations

#Start out with Linear
predictions_2023_linear <- linear_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Linear: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Linear <- predictions_2023_linear$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Linear MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Linear-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Linear RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Linear-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Linear MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Linear-both_df_2023$Pct_of_Max_Points))

#Log
predictions_2023_log <- log_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Log: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Log <- predictions_2023_log$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Log MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Log-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Log RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Log-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Log MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Log-both_df_2023$Pct_of_Max_Points))


predictions_2023_sqrt <- sqrt_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Square Root: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Square_Root <- predictions_2023_sqrt$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Square Root MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Square_Root-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Square Root RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Square_Root-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Square Root MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Square_Root-both_df_2023$Pct_of_Max_Points))

predictions_2023_weight <- weight_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Weighted: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Weighted <- predictions_2023_weight$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Weighted MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Weighted-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Weighted RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Weighted-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Weighted MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Weighted-both_df_2023$Pct_of_Max_Points))

predictions_2023_quantile <- quantile_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Quantile: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", tau=.9, num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Quantile <- predictions_2023_quantile$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Quantile MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Quantile-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Quantile RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Quantile-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Quantile MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Quantile-both_df_2023$Pct_of_Max_Points))

predictions_2023_poly <- poly_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Polynomial: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Polynomial <- predictions_2023_poly$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Polynomial MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Polynomial-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Polynomial RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Polynomial-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Polynomial MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Polynomial-both_df_2023$Pct_of_Max_Points))

predictions_2023_tobit <- tobit_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Tobit: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", left=0, num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Tobit <- predictions_2023_tobit$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Tobit MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Tobit-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Tobit RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Tobit-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Tobit MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Tobit-both_df_2023$Pct_of_Max_Points))

knots <- c(1200, 1300, 1400, 1500, 1600, 1700)


predictions_2023_spline <- spline_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, "Prev_Distance + Prev_Sprint",  "Pct_of_Max_Points", knots,"Spline: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_Spline <- predictions_2023_spline$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`Spline MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Spline-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`Spline RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_Spline-both_df_2023$Pct_of_Max_Points))
both_df_2023$`Spline MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_Spline-both_df_2023$Pct_of_Max_Points))

#Start out with KNN
predictions_2023_knn <- knn_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "KNN: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", 5, num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_KNN <- predictions_2023_knn$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`KNN MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_KNN-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`KNN RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_KNN-both_df_2023$Pct_of_Max_Points))
both_df_2023$`KNN MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_KNN-both_df_2023$Pct_of_Max_Points))

#Start out with GAM
predictions_2023_gam <- gam_create_bootstrap_model_and_predict(both_dfn_2023, both_df_2023, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2023$Predicted_Pct_of_Max_Points_GAM <- predictions_2023_gam$predictions
both_df_2023 <- both_df_2023[order(-both_df_2023$Pct_of_Max_Points), ]
both_df_2023$`GAM MSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_GAM-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`GAM RMSE` <- ((both_df_2023$Predicted_Pct_of_Max_Points_GAM-both_df_2023$Pct_of_Max_Points)^2)
both_df_2023$`GAM MAE` <- (abs(both_df_2023$Predicted_Pct_of_Max_Points_GAM-both_df_2023$Pct_of_Max_Points))

both_df_2023 <- both_df_2023[1:30, ]

linearMSE <- mean(both_df_2023$`Linear MSE`)
linearRMSE <- sqrt(mean(both_df_2023$`Linear MSE`))
linearMAE <- mean(both_df_2023$`Linear MAE`)

logMSE <- mean(both_df_2023$`Log MSE`)
logRMSE <- sqrt(mean(both_df_2023$`Log MSE`))
logMAE <- mean(both_df_2023$`Log MAE`)

sqrtMSE <- mean(both_df_2023$`Square Root MSE`)
sqrtRMSE <- sqrt(mean(both_df_2023$`Square Root MSE`))
sqrtMAE <- mean(both_df_2023$`Square Root MAE`)

weightMSE <- mean(both_df_2023$`Weighted MSE`)
weightRMSE <- sqrt(mean(both_df_2023$`Weighted MSE`))
weightMAE <- mean(both_df_2023$`Weighted MAE`)

quantileMSE <- mean(both_df_2023$`Quantile MSE`)
quantileRMSE <- sqrt(mean(both_df_2023$`Quantile MSE`))
quantileMAE <- mean(both_df_2023$`Quantile MAE`)

polyMSE <- mean(both_df_2023$`Polynomial MSE`)
polyRMSE <- sqrt(mean(both_df_2023$`Polynomial MSE`))
polyMAE <- mean(both_df_2023$`Polynomial MAE`)

tobitMSE <- mean(both_df_2023$`Tobit MSE`)
tobitRMSE <- sqrt(mean(both_df_2023$`Tobit MSE`))
tobitMAE <- mean(both_df_2023$`Tobit MAE`)

splineMSE <- mean(both_df_2023$`Spline MSE`)
splineRMSE <- sqrt(mean(both_df_2023$`Spline MSE`))
splineMAE <- mean(both_df_2023$`Spline MAE`)

knnMSE <- mean(both_df_2023$`KNN MSE`)
knnRMSE <- sqrt(mean(both_df_2023$`KNN MSE`))
knnMAE <- mean(both_df_2023$`KNN MAE`)

gamMSE <- mean(both_df_2023$`GAM MSE`)
gamRMSE <- sqrt(mean(both_df_2023$`GAM MSE`))
gamMAE <- mean(both_df_2023$`GAM MAE`)

mse2023 <- c(linearMSE, logMSE, sqrtMSE, weightMSE, quantileMSE, polyMSE, tobitMSE, splineMSE, knnMSE, gamMSE)
rmse2023 <- c(linearRMSE, logRMSE, sqrtRMSE, weightRMSE, quantileRMSE, polyRMSE, tobitRMSE, splineRMSE, knnRMSE, gamRMSE)
mae2023 <- c(linearMAE, logMAE, sqrtMAE, weightMAE, quantileMAE, polyMAE, tobitMAE, splineMAE, knnMAE, gamMAE)

resid2023 <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile","Polynomial", "Tobit", "Spline", "KNN", "GAM") ,`MSE 2023`=mse2023, `RMSE 2023`=rmse2023, `MAE 2023`=mae2023)
```

### 2024
```{r 2024}
#Get a 2024 df to test with
both_df_2024 <- both_df %>% filter(Season == 2024)
both_dfn_2024 <- both_df %>% filter(Season != 2024)
# Set the number of bootstrap iterations
num_bootstrap <- 100  # You can adjust the number of bootstrap iterations

#Start out with Linear
predictions_2024_linear <- linear_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Linear: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Linear <- predictions_2024_linear$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Linear MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Linear-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Linear RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Linear-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Linear MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Linear-both_df_2024$Pct_of_Max_Points))

#Log
predictions_2024_log <- log_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Log: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Log <- predictions_2024_log$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Log MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Log-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Log RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Log-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Log MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Log-both_df_2024$Pct_of_Max_Points))


predictions_2024_sqrt <- sqrt_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Square Root: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Square_Root <- predictions_2024_sqrt$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Square Root MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Square_Root-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Square Root RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Square_Root-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Square Root MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Square_Root-both_df_2024$Pct_of_Max_Points))

predictions_2024_weight <- weight_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Weighted: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Weighted <- predictions_2024_weight$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Weighted MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Weighted-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Weighted RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Weighted-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Weighted MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Weighted-both_df_2024$Pct_of_Max_Points))

predictions_2024_quantile <- quantile_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Quantile: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", tau=.9, num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Quantile <- predictions_2024_quantile$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Quantile MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Quantile-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Quantile RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Quantile-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Quantile MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Quantile-both_df_2024$Pct_of_Max_Points))

predictions_2024_poly <- poly_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Polynomial: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Polynomial <- predictions_2024_poly$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Polynomial MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Polynomial-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Polynomial RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Polynomial-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Polynomial MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Polynomial-both_df_2024$Pct_of_Max_Points))

predictions_2024_tobit <- tobit_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "Tobit: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", left=0, num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Tobit <- predictions_2024_tobit$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Tobit MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Tobit-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Tobit RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Tobit-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Tobit MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Tobit-both_df_2024$Pct_of_Max_Points))

knots <- c(1200, 1300, 1400, 1500, 1600, 1700)


predictions_2024_spline <- spline_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, "Prev_Distance + Prev_Sprint",  "Pct_of_Max_Points", knots,"Spline: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_Spline <- predictions_2024_spline$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`Spline MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Spline-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`Spline RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_Spline-both_df_2024$Pct_of_Max_Points))
both_df_2024$`Spline MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_Spline-both_df_2024$Pct_of_Max_Points))

#Start out with KNN
predictions_2024_knn <- knn_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "KNN: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", 5, num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_KNN <- predictions_2024_knn$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`KNN MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_KNN-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`KNN RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_KNN-both_df_2024$Pct_of_Max_Points))
both_df_2024$`KNN MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_KNN-both_df_2024$Pct_of_Max_Points))

#Start out with GAM
predictions_2024_gam <- gam_create_bootstrap_model_and_predict(both_dfn_2024, both_df_2024, c("Prev_Distance", "Prev_Sprint"), "Pct_of_Max_Points", "GAM: Pct_of_Max_Points ~ Prev_Distance + Prev_Sprint", num_bootstrap = num_bootstrap)
both_df_2024$Predicted_Pct_of_Max_Points_GAM <- predictions_2024_gam$predictions
both_df_2024 <- both_df_2024[order(-both_df_2024$Pct_of_Max_Points), ]
both_df_2024$`GAM MSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_GAM-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`GAM RMSE` <- ((both_df_2024$Predicted_Pct_of_Max_Points_GAM-both_df_2024$Pct_of_Max_Points)^2)
both_df_2024$`GAM MAE` <- (abs(both_df_2024$Predicted_Pct_of_Max_Points_GAM-both_df_2024$Pct_of_Max_Points))

both_df_2024 <- both_df_2024[1:30, ]

linearMSE <- mean(both_df_2024$`Linear MSE`)
linearRMSE <- sqrt(mean(both_df_2024$`Linear MSE`))
linearMAE <- mean(both_df_2024$`Linear MAE`)

logMSE <- mean(both_df_2024$`Log MSE`)
logRMSE <- sqrt(mean(both_df_2024$`Log MSE`))
logMAE <- mean(both_df_2024$`Log MAE`)

sqrtMSE <- mean(both_df_2024$`Square Root MSE`)
sqrtRMSE <- sqrt(mean(both_df_2024$`Square Root MSE`))
sqrtMAE <- mean(both_df_2024$`Square Root MAE`)

weightMSE <- mean(both_df_2024$`Weighted MSE`)
weightRMSE <- sqrt(mean(both_df_2024$`Weighted MSE`))
weightMAE <- mean(both_df_2024$`Weighted MAE`)

quantileMSE <- mean(both_df_2024$`Quantile MSE`)
quantileRMSE <- sqrt(mean(both_df_2024$`Quantile MSE`))
quantileMAE <- mean(both_df_2024$`Quantile MAE`)

polyMSE <- mean(both_df_2024$`Polynomial MSE`)
polyRMSE <- sqrt(mean(both_df_2024$`Polynomial MSE`))
polyMAE <- mean(both_df_2024$`Polynomial MAE`)

tobitMSE <- mean(both_df_2024$`Tobit MSE`)
tobitRMSE <- sqrt(mean(both_df_2024$`Tobit MSE`))
tobitMAE <- mean(both_df_2024$`Tobit MAE`)

splineMSE <- mean(both_df_2024$`Spline MSE`)
splineRMSE <- sqrt(mean(both_df_2024$`Spline MSE`))
splineMAE <- mean(both_df_2024$`Spline MAE`)

knnMSE <- mean(both_df_2024$`KNN MSE`)
knnRMSE <- sqrt(mean(both_df_2024$`KNN MSE`))
knnMAE <- mean(both_df_2024$`KNN MAE`)

gamMSE <- mean(both_df_2024$`GAM MSE`)
gamRMSE <- sqrt(mean(both_df_2024$`GAM MSE`))
gamMAE <- mean(both_df_2024$`GAM MAE`)

mse2024 <- c(linearMSE, logMSE, sqrtMSE, weightMSE, quantileMSE, polyMSE, tobitMSE, splineMSE, knnMSE, gamMSE)
rmse2024 <- c(linearRMSE, logRMSE, sqrtRMSE, weightRMSE, quantileRMSE, polyRMSE, tobitRMSE, splineRMSE, knnRMSE, gamRMSE)
mae2024 <- c(linearMAE, logMAE, sqrtMAE, weightMAE, quantileMAE, polyMAE, tobitMAE, splineMAE, knnMAE, gamMAE)

resid2024 <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile","Polynomial", "Tobit", "Spline", "KNN", "GAM") ,`MSE 2024`=mse2024, `RMSE 2024`=rmse2024, `MAE 2024`=mae2024)


```

``` {r pct-df}
library(openxlsx)
# Sequentially merge data frames on the "Skier" column
merged_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list(resid2020, resid2021, resid2022, resid2023, resid2024))

merged_df$Avg_MSE<- rowMeans(merged_df[, seq(2, ncol(merged_df), by = 3)], na.rm = TRUE)
merged_df$Avg_RMSE<- rowMeans(merged_df[, seq(3, ncol(merged_df), by = 3)], na.rm = TRUE)
merged_df$Avg_MAE<- rowMeans(merged_df[, seq(4, ncol(merged_df), by = 3)], na.rm = TRUE)

merged_df <- merged_df[order(merged_df$Avg_MSE), ]



write.xlsx(merged_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men-best-models.xlsx")

```

```{r final-pred}
# Example DTW calculation with visualization
library(dtw)
library(ggplot2)

# Create example trajectories
# Imagine these are Elo ratings for two skiers who peaked at different ages
skier1_trajectory <- c(1500, 1600, 1800, 2000, 2100, 2000, 1900)  # Peaks earlier
skier2_trajectory <- c(1500, 1550, 1600, 1800, 2000, 2100, 2000)  # Peaks later

# Calculate DTW alignment once
dtw_alignment <- dtw(skier1_trajectory, skier2_trajectory, keep = TRUE)

# Function to visualize DTW matching
plot_dtw_matching <- function(seq1, seq2, alignment) {
  # Create matching data
  matches <- data.frame(
    x1 = seq_along(seq1)[alignment$index1],
    x2 = seq_along(seq2)[alignment$index2],
    y1 = seq1[alignment$index1],
    y2 = seq2[alignment$index2]
  )
  
  # Create plot
  ggplot() +
    # Plot original sequences
    geom_line(aes(x = 1:length(seq1), y = seq1, color = "Skier 1"), size = 1) +
    geom_line(aes(x = 1:length(seq2), y = seq2, color = "Skier 2"), size = 1) +
    # Add matching lines
    geom_segment(data = matches, 
                aes(x = x1, y = y1, xend = x2, yend = y2),
                color = "gray", alpha = 0.5) +
    labs(title = "DTW Alignment of Two Skier Trajectories",
         subtitle = "Gray lines show how points are matched between trajectories",
         x = "Time Point", 
         y = "Elo Rating",
         color = "Skier") +
    theme_minimal()
}

# Calculate regular Euclidean distance
euclidean_dist <- sqrt(sum((skier1_trajectory - skier2_trajectory)^2))

# Calculate DTW distance
dtw_dist <- dtw_alignment$distance

# Print comparison
cat("Euclidean Distance:", euclidean_dist, "\n")
cat("DTW Distance:", dtw_dist, "\n")

# Print the warping path
cat("\nWarping Path (index1 -> index2):\n")
for(i in 1:length(dtw_alignment$index1)) {
  cat(sprintf("Point %d: Skier1[%d] matches with Skier2[%d]\n", 
              i, dtw_alignment$index1[i], dtw_alignment$index2[i]))
}

# Plot the DTW matching
plot_dtw_matching(skier1_trajectory, skier2_trajectory, dtw_alignment)
```


```{r similarity-scores}
calculate_skier_similarity <- function(chrono_df, metrics = c("Pelo", "Pct_of_Max_Points")) {
  # First, create career trajectories for each skier
  career_trajectories <- chrono_df %>%
    dplyr::arrange(ID, Age) %>%
    dplyr::group_by(ID) %>%
    dplyr::mutate(
      Age_Clean = as.numeric(Age),
      Age_Clean = ifelse(is.na(Age_Clean), 
                        median(Age_Clean, na.rm = TRUE),
                        Age_Clean)
    ) %>%
    # Get key info for each skier
    dplyr::summarise(
      Name = first(Skier),
      Birth = first(Birthday),
      Nation = first(Nation),
      Career_Start = min(Season),
      Career_End = max(Season),
      Career_Length = n_distinct(Season),
      Peak_Points = max(Pct_of_Max_Points, na.rm = TRUE),
      Peak_Elo = max(Pelo, na.rm = TRUE),
      # Store full trajectories
      Age_Trajectory = list(Age_Clean),
      Elo_Trajectory = list(Pelo),
      Points_Trajectory = list(Pct_of_Max_Points),
      Distance_Trajectory = list(Prev_Distance),
      Sprint_Trajectory = list(Prev_Sprint),
      Experience_Trajectory = list(Exp),
      .groups = 'drop'
    ) %>%
    # Create unique identifier
    dplyr::mutate(
      Unique_ID = paste0(ID, "_", Name)
    )
  
  # Print duplicate name check
  name_counts <- table(career_trajectories$Name)
  duplicates <- name_counts[name_counts > 1]
  if(length(duplicates) > 0) {
    print("Found duplicate names:")
    print(duplicates)
  }
  
# Function to calculate more interpretable similarity scores
# Function to calculate more interpretable similarity scores
calculate_trajectory_similarity <- function(traj1, traj2) {
  # Remove NAs from both trajectories
  valid_indices1 <- !is.na(traj1)
  valid_indices2 <- !is.na(traj2)
  traj1 <- traj1[valid_indices1]
  traj2 <- traj2[valid_indices2]
  
  # Check if we have enough valid data points
  if(length(traj1) < 2 || length(traj2) < 2) {
    return(0)  # Return 0 similarity if insufficient data
  }
  
  # Interpolate to same length for correlation
  max_length <- max(length(traj1), length(traj2))
  
  # Safe interpolation with error handling
  tryCatch({
    traj1_interp <- approx(seq_along(traj1), traj1, n = max_length)$y
    traj2_interp <- approx(seq_along(traj2), traj2, n = max_length)$y
    
    # Check for constant trajectories
    sd1 <- sd(traj1_interp)
    sd2 <- sd(traj2_interp)
    
    # Calculate correlation on interpolated data
    # Suppress warnings for correlation calculation
    cor_score <- if(sd1 == 0 || sd2 == 0) {
      # If trajectories are constant, check if they're the same constant
      if(mean(traj1_interp) == mean(traj2_interp)) {
        1  # Perfect correlation for identical constant trajectories
      } else {
        0  # No correlation for different constant trajectories
      }
    } else {
      suppressWarnings(cor(traj1_interp, traj2_interp, use = "complete.obs"))
    }
    
    if(is.na(cor_score)) cor_score <- 0
    
    # Calculate shape similarity using DTW with error handling
    shape_score <- tryCatch({
      dtw_dist <- dtw(traj1, traj2, keep = TRUE)$distance
      max_dist <- max(abs(diff(range(c(traj1, traj2))))) * max_length
      score <- 1 - (dtw_dist / max_dist)
      if(is.na(score)) 0 else score
    }, error = function(e) {
      0
    })
    
    # Calculate level similarity using interpolated data
    level_diff <- mean(abs(traj1_interp - traj2_interp), na.rm = TRUE)
    max_val <- max(abs(c(traj1_interp, traj2_interp)), na.rm = TRUE)
    level_score <- 1 - (level_diff / max_val)
    if(is.na(level_score)) level_score <- 0
    
    # Combine scores (weighted average)
    similarity <- (cor_score + shape_score + level_score) / 3
    
    # Scale to 0-100 for interpretability
    similarity <- (similarity + 1) * 50  # Transform from [-1,1] to [0,100]
    
    # Handle any remaining NAs
    if(is.na(similarity)) similarity <- 0
    
    return(similarity)
  }, error = function(e) {
    return(0)  # Return 0 similarity if interpolation fails
  })
}
  
 # Calculate similarity matrix with component scores
  n_skiers <- nrow(career_trajectories)
  similarity_matrix <- matrix(0, n_skiers, n_skiers)
  
  # Also store component similarities for explanation
  elo_similarities <- matrix(0, n_skiers, n_skiers)
  points_similarities <- matrix(0, n_skiers, n_skiers)
  
  for(i in 1:(n_skiers-1)) {
    for(j in (i+1):n_skiers) {
      # Calculate component similarities
      elo_sim <- calculate_trajectory_similarity(
        unlist(career_trajectories$Elo_Trajectory[i]),
        unlist(career_trajectories$Elo_Trajectory[j])
      )
      
      points_sim <- calculate_trajectory_similarity(
        unlist(career_trajectories$Points_Trajectory[i]),
        unlist(career_trajectories$Points_Trajectory[j])
      )
      
      # Store component similarities
      elo_similarities[i,j] <- elo_similarities[j,i] <- elo_sim
      points_similarities[i,j] <- points_similarities[j,i] <- points_sim
      
      # Overall similarity is average of components
      similarity_matrix[i,j] <- similarity_matrix[j,i] <- 
        (elo_sim + points_sim) / 2
    }
  }
  
  # Convert to similarity dataframe using unique IDs
  similarity_df <- as.data.frame(similarity_matrix)
  colnames(similarity_df) <- career_trajectories$Unique_ID
  rownames(similarity_df) <- career_trajectories$Unique_ID
  
  # Function to get most similar skiers
 get_most_similar <- function(skier_name, n = 5) {
  # Find all instances of the skier name
  skier_ids <- career_trajectories %>%
    dplyr::filter(Name == skier_name) %>%
    dplyr::pull(Unique_ID)
  
  if(length(skier_ids) == 0) {
    return(NULL)
  }
  
  # If multiple instances, use the most recent one
  skier_id <- skier_ids[1]
  
  # Get similarities for this skier
  similarities <- similarity_df[skier_id,]
  
  # Convert to a named vector for proper sorting
  sim_vector <- as.numeric(similarities)
  names(sim_vector) <- colnames(similarities)
  
  # Sort similarities
  sorted_sims <- sort(sim_vector, decreasing = TRUE)
  
  # Get top N similar skiers
  similar_ids <- names(head(sorted_sims, n+1))
  
    similar_skiers <- career_trajectories %>%
      dplyr::filter(Unique_ID %in% similar_ids) %>%
      dplyr::select(Name, Nation, Career_Start, Career_End, 
                   Peak_Points, Peak_Elo, Unique_ID) %>%
      dplyr::mutate(
        Overall_Similarity = sorted_sims[match(Unique_ID, names(sorted_sims))],
        Elo_Similarity = elo_similarities[match(Unique_ID, rownames(elo_similarities)),
                                        match(skier_id, colnames(elo_similarities))],
        Points_Similarity = points_similarities[match(Unique_ID, rownames(points_similarities)),
                                             match(skier_id, colnames(points_similarities))],
        Similarity_Explanation = sprintf(
          "%.1f overall (Elo: %.1f, Points: %.1f)",
          Overall_Similarity, Elo_Similarity, Points_Similarity
        )
      ) %>%
      dplyr::arrange(desc(Overall_Similarity))
    
    # Add explanation of what scores mean
    similarity_interpretation <- data.frame(
      Score_Range = c("90-100", "80-89", "70-79", "60-69", "Below 60"),
      Interpretation = c(
        "Nearly identical career trajectories",
        "Very similar career development patterns",
        "Similar overall career patterns with some differences",
        "Some similar career aspects but notable differences",
        "Substantially different career patterns"
      )
    )
    
    return(list(
      similarities = sorted_sims[1:(n+1)],
      details = similar_skiers,
      interpretation = similarity_interpretation
    ))
  }
  


  
  # Create career trajectory visualization
  plot_career_comparison <- function(skier_name, similar_skier_id) {
    main_skier <- career_trajectories %>%
      dplyr::filter(Name == skier_name) %>%
      dplyr::slice(1)  # Take first instance if multiple
    
    comp_skier <- career_trajectories %>%
      dplyr::filter(Unique_ID == similar_skier_id)
    
    # Plot Elo trajectories
    p1 <- ggplot() +
      geom_line(data = data.frame(
        Age = unlist(main_skier$Age_Trajectory),
        Elo = unlist(main_skier$Elo_Trajectory)
      ), aes(x = Age, y = Elo, color = "Main")) +
      geom_line(data = data.frame(
        Age = unlist(comp_skier$Age_Trajectory),
        Elo = unlist(comp_skier$Elo_Trajectory)
      ), aes(x = Age, y = Elo, color = "Similar")) +
      labs(title = paste("Elo Rating Trajectories -", 
                        main_skier$Name, "vs", comp_skier$Name),
           x = "Age", y = "Elo Rating") +
      theme_minimal()
    
    # Plot Points trajectories
    p2 <- ggplot() +
      geom_line(data = data.frame(
        Age = unlist(main_skier$Age_Trajectory),
        Points = unlist(main_skier$Points_Trajectory)
      ), aes(x = Age, y = Points, color = "Main")) +
      geom_line(data = data.frame(
        Age = unlist(comp_skier$Age_Trajectory),
        Points = unlist(comp_skier$Points_Trajectory)
      ), aes(x = Age, y = Points, color = "Similar")) +
      labs(title = "Points Trajectories",
           x = "Age", y = "Points (% of Max)") +
      theme_minimal()
    
    return(list(elo_plot = p1, points_plot = p2))
  }
  
  return(list(
    career_trajectories = career_trajectories,
    similarity_matrix = similarity_matrix,
    get_most_similar = get_most_similar,
    plot_comparison = plot_career_comparison
  ))
}

# Run the analysis
similarity_analysis <- calculate_skier_similarity(df82)

normalize_name <- function(name) {
  name %>%
    stringr::str_replace_all("ø", "o") %>%
    stringr::str_replace_all("æ", "ae") %>%
    stringr::str_replace_all("å", "a")
}

# Example usage
skier_name <- "Erik Valnes"  # Replace with actual skier name
similar_skiers <- similarity_analysis$get_most_similar(skier_name)

# Create visualizations for most similar skier
if(!is.null(similar_skiers)) {
  most_similar_id <- similar_skiers$details$Unique_ID[2]  # First one is the skier himself
  comparison_plots <- similarity_analysis$plot_comparison(skier_name, most_similar_id)
  
  # Display plots
  print(comparison_plots$elo_plot)
  print(comparison_plots$points_plot)
  
  # Save results
  write.xlsx(list(
    Similar_Skiers = similar_skiers$details %>% dplyr::select(-Unique_ID),
    Career_Details = similarity_analysis$career_trajectories %>%
      dplyr::filter(Unique_ID %in% similar_skiers$details$Unique_ID) %>%
      dplyr::select(-Unique_ID)
  ), 
  file = paste0("/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/",
                gsub(" ", "_", skier_name), "_similar_skiers.xlsx"),
  sheetName = c("Similar_Skiers", "Career_Details"))
}
```


---
title: "Season Preview in R"
author: "Syver Johansen"
date: "2024-10-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Season Preview

What I want the 2026 preview to have.

1) Points prediction for the season
2) What it would take for Klaebo to lose the season
3) What does the standings look like if the Russians come back (using their results from 2022 with age Elo enhancement that we did we Johaug)
4) Break out potential for skiers.

Basically the same as last year, but with Russians and leave out Johaug from everything.

This is an R Markdown document for the season preview stats.  It is a mockup of what will actually go in the final document.  The final one will have both Python and R as they have different strenghts in what they do.

First we will load the necessary libraries

```{r load-packages, message=FALSE, warning=FALSE}
library(dplyr)
library(purrr)
library(tidyr)
library(arrow)

library(AER)
library(ggplot2)
library(effects)
library(segmented)
library(splines)
library(FNN)
library(mgcv)
library(randomForest)
```

## Data setup
Now we load in the data

```{r load-data}
M_chrono <- read_feather('/Users/syverjohansen/ski/elo/python/ski/polars/excel365/men_chrono.feather')

# Step Two: Create a column called WC Points that maps place to world cup points from a list
wc_points <- c(100,95,90,85,80,75,72,69,66,63,60,58,56,54,52,50,48,46,44,42,40,38,36,34,32,30,28,26,24,22,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1)
stage_points <- c(50, 47, 44, 41, 38, 35, 32, 30, 28, 26, 24, 22, 20, 18, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1)
tds_points <- c(300, 285, 270, 255, 240, 216, 207, 198, 189, 180, 174, 168, 162, 156, 150, 144, 138, 132, 126, 120, 114, 108, 102, 96, 90, 84, 78, 72, 66, 60, 57, 54, 51, 48, 45, 42, 39, 36, 33, 30, 27, 24, 21, 18, 15, 12, 9, 6, 3)

# Function to safely fetch points based on Place
get_points <- function(place, points_list) {
  if (place >= 1 && place <= length(points_list)) {
    return(points_list[place])
  }
  return(0)
}

# Apply points logic based on Event and Distance
df <- M_chrono %>%
  mutate(Points = case_when(
    Event == "Tour de Ski" & Distance == "Stage" ~ map_int(Place, ~ get_points(.x, tds_points)),
    Event == "Tour de Ski" ~ map_int(Place, ~ get_points(.x, stage_points)),
    TRUE ~ map_int(Place, ~ get_points(.x, wc_points))
  ))

# Sort the df by Date, Race, and Place
df <- df %>%
  arrange(Date, Race, Place)

# Step Three: Filter for the last five years
df <- df %>%
  filter(Event %in% c("Offseason", "World Cup", "Nordic Opening", "Tour de Ski",  "World Cup Final", "Ski Tour Canada")) %>%
  group_by(ID, Season) %>%
  mutate(Cumulative_Points = cumsum(Points)) %>%
  ungroup()

df <- df %>%
  group_by(ID, Season) %>%
  mutate(Races_in_Season = n()) %>%
  ungroup() %>%
  filter(Races_in_Season>=10)

# Function to replace NAs with the first quartile within each group
replace_na_with_quartile <- function(x) {
  quartile_1 <- quantile(x, 0.25, na.rm = TRUE)  # Calculate the first quartile, ignoring NAs
  ifelse(is.na(x), quartile_1, x)  # Replace NAs with the first quartile
}

# Apply this logic to each of the specified columns and then perform the percentage calculations
df <- df %>%
  group_by(Season, Race) %>%
  mutate(
    Distance_Pelo = replace_na_with_quartile(Distance_Pelo),
    Distance_C_Pelo = replace_na_with_quartile(Distance_C_Pelo),
    Distance_F_Pelo = replace_na_with_quartile(Distance_F_Pelo),
    Pelo = replace_na_with_quartile(Pelo),
    Sprint_Pelo = replace_na_with_quartile(Sprint_Pelo),
    Sprint_C_Pelo = replace_na_with_quartile(Sprint_C_Pelo),
    Sprint_F_Pelo = replace_na_with_quartile(Sprint_F_Pelo),
    Freestyle_Pelo = replace_na_with_quartile(Freestyle_Pelo),
    Classic_Pelo = replace_na_with_quartile(Classic_Pelo)
  ) %>%
  ungroup()
df <- df %>%
  filter(! Distance %in% c("Ts", "Rel"))


# Step One: Filter for first place
first_place <- df %>%
  filter(Place == 1)

# Step Two: Group by Season and calculate total points for first place
max_points_per_season <- first_place %>%
  group_by(Season) %>%
  summarise(Max_Points = sum(Points), .groups = 'drop')

# Step Three: Join the max points back to the original data frame
df <- df %>%
  left_join(max_points_per_season, by = "Season")

# Step Four: Calculate percentage of maximum points
df <- df %>%
  mutate(Pct_of_Max_Points = Cumulative_Points / Max_Points)


# Step Four: Set up the explanatory variables
# Filter for the offseason
elo_df <- df %>%
  filter(Event == "Offseason") %>%
  arrange(ID, Season) %>%
  group_by(ID) %>%
  mutate(Prev_Pelo = lag(Pelo)) %>%
  ungroup()



elo_df <- df %>%
  filter(Event == "Offseason") %>%
  arrange(ID, Season) %>%
  group_by(ID) %>%
  mutate(Prev_Pelo = lag(Pelo), Prev_Sprint=lag(Sprint_Pelo), Prev_Sprint_C=lag(Sprint_C_Pelo), Prev_Sprint_F=lag(Sprint_F_Pelo), Prev_Distance=lag(Distance_Pelo), Prev_Distance_F=lag(Distance_F_Pelo), Prev_Distance_C=lag(Distance_C_Pelo), Prev_F=lag(Freestyle_Pelo), Prev_C=lag(Classic_Pelo), Prev_Pct_of_Max_Points=lag(Pct_of_Max_Points)) %>%
  ungroup()





elo_df <- elo_df %>%
  filter(Season>2018)

df <- elo_df

df <- df %>%
  group_by(Season) %>%
  mutate(
    Prev_Distance = replace_na_with_quartile(Prev_Distance),
    Prev_Distance_C = replace_na_with_quartile(Prev_Distance_C),
    Prev_Distance_F = replace_na_with_quartile(Prev_Distance_F),
    Prev_Pelo = replace_na_with_quartile(Prev_Pelo),
    Prev_Sprint = replace_na_with_quartile(Prev_Sprint),
    Prev_Sprint_C = replace_na_with_quartile(Prev_Sprint_C),
    Prev_Sprint_F = replace_na_with_quartile(Prev_Sprint_F),
    Prev_F = replace_na_with_quartile(Prev_F),
    Prev_C = replace_na_with_quartile(Prev_C),
    Prev_Pct_of_Max_Points = replace(Prev_Pct_of_Max_Points, is.na(Prev_Pct_of_Max_Points), 0)
  ) %>%
  ungroup()

```


##EDA
Let's take a look at the data we're working with starting with some histograms
``` {r eda}
library(e1071)
hist(df$Prev_Pelo, main="Distribution of Previous Elo", xlab="Previous Elo")
#hist(distance_df$Prev_Distance, main="Distribution of Previous Distance Elo", xlab="Previous Distance Elo")
#hist(sprint_df$Prev_Sprint, main="Distribution of Previous Sprint Elo", xlab="Previous Sprint Elo")
hist(df$Pct_of_Max_Points, main="Distribution of the Percent of Maximum Points", xlab="Percent of Maximum Points")

skewness(df$Prev_Pelo)
#skewness(distance_df$Prev_Distance)
#skewness(sprint_df$Prev_Sprint)
skewness(df$Pct_of_Max_Points)

kurtosis(df$Prev_Pelo)
#kurtosis(distance_df$Prev_Distance)
#kurtosis(sprint_df$Prev_Sprint)
kurtosis(df$Pct_of_Max_Points)

```
We see that they all lean right skewed, but the Pct of Max Points leans it the farthest.  Since it is above the threshold for skewness of 2, we will have to transform it.

On the kurtosis front, it is more of the same.  In the appropriate range for the Elo scores, but for pct_of_max_points it is not.

Let's try some transformations starting with a log 10

```{r log-dist}
df$Log10_Pct_of_Max_Points = log10(df$Pct_of_Max_Points+.00001)


hist(df$Log10_Pct_of_Max_Points)

skewness(df$Log10_Pct_of_Max_Points)
kurtosis(df$Log10_Pct_of_Max_Points)
```
That seemed to improve it drastically.  Let's see if any others do well.

``` {r bc-dist}
my_df <- df
my_df$Pct_of_Max_Points <- my_df$Pct_of_Max_Points + 0.000001  # Avoid log of zero
summary(my_df$Pct_of_Max_Points)

lm_model <- lm(Pct_of_Max_Points ~ Prev_Pelo, data = my_df)

# Box-Cox transformation
bc <- boxcox(lm_model)
best_lambda <- bc$x[which.max(bc$y)]

# Apply the transformation
my_df$y_transformed <-(my_df$Pct_of_Max_Points^best_lambda - 1) / best_lambda

# Plot the histogram of transformed data
hist(my_df$y_transformed, main = "Histogram of Transformed Pct_of_Max_Points", 
     xlab = "Transformed Pct_of_Max_Points", col = "skyblue", breaks = 20)
skewness(my_df$y_transformed)
kurtosis(my_df$y_transformed)
```
Also improved drastically, with much better skew and similar kurtosis.  The issue still remains the massive spire at 0.

Let's try one final transformation, getting a quantile transformation


```{r quantile-dist}
 library(caret)
 library(moments)
# pre_process_model <- preProcess(df[, "Pct_of_Max_Points", drop = FALSE], method = "BoxCox")
# 
# # Transform the column
# df$Quantile_Cumulative_Points <- predict(pre_process_model, df[, "Pct_of_Max_Points", drop = FALSE])
# 
# # Plot histogram of Quantile Cumulative Points
# hist(df$Quantile_Cumulative_Points[[1]], breaks = 30, col = "green", 
#      main = "Distribution of Percent of Maximum Points",
#      xlab = "Percent of Maximum Points", ylab = "Frequency")
# 
# # Calculate skewness of Quantile Cumulative Points
# skewness(df$Quantile_Cumulative_Points[[1]])
```
Skewness is too high





###Plotting Residuals
```{r residuals}
library(gridExtra)
library(quantreg)
df_race = df
custom_r2 <- function(y_train, y_pred_train, y_test, y_pred_test){
    # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  

  # Print custom R² scores

  return(list(r2_train = custom_r2_train, r2_test = custom_r2_test))
}

act_custom_rmse <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Actual), ]
  prediction_df$RMSE <- (prediction_df$Prediction-prediction_df$Actual)^2
  prediction_df <- prediction_df %>%
    filter(Actual>=.22)
  RMSE <- sqrt(mean(prediction_df$RMSE))
  
  return(RMSE)
}

act_custom_mae <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Actual), ]
  prediction_df$MAE <- abs(prediction_df$Prediction-prediction_df$Actual)
  prediction_df <- prediction_df %>%
    filter(Actual>=.22)
  MAE <- mean(prediction_df$MAE)
  
  return(MAE)
}

pred_custom_rmse <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Prediction), ]
  prediction_df$RMSE <- (prediction_df$Prediction-prediction_df$Actual)^2
  prediction_df <- prediction_df %>%
    filter(Prediction>=.22)
  RMSE <- sqrt(mean(prediction_df$RMSE))
  
  return(RMSE)
}

pred_custom_mae <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Prediction), ]
  prediction_df$MAE <- abs(prediction_df$Prediction-prediction_df$Actual)
  prediction_df <- prediction_df %>%
    filter(Prediction>=.22)
  MAE <- mean(prediction_df$MAE)
  
  return(MAE)
}


model_comp <- function(df, season, col, X_label, y_label, title){
  df_train <- df %>%
    filter(Season!=season)
  df_test <- df %>%
    filter(Season==season)

  
  custom_r2_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost"),
                               R2=NA)  # Initialize with a generic name
  colnames(custom_r2_table)[2] <- paste("R2", season, sep="_")
 # Initialize the column with NA values

  act_rmse_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost"),
                               RMSE=NA)  # Initialize with a generic name
  colnames(act_rmse_table)[2] <- paste("Actual_RMSE", season, sep="_")
  
  act_mae_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost"),
                               MAE=NA)  # Initialize with a generic name
  colnames(act_mae_table)[2] <- paste("Actual_MAE", season, sep="_")
  
  pred_rmse_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost"),
                               RMSE=NA)  # Initialize with a generic name
  colnames(pred_rmse_table)[2] <- paste("Pred_RMSE", season, sep="_")
  
  pred_mae_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost"),
                               MAE=NA)  # Initialize with a generic name
  colnames(pred_mae_table)[2] <- paste("Pred_MAE", season, sep="_")
  
  #Linear Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[1,2] <- custom_r2_test
  act_rmse_table[1,2] <- Act_RMSE
  act_mae_table[1,2] <- Act_MAE
  pred_rmse_table[1,2] <- Pred_RMSE
  pred_mae_table[1,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
  
  #Log Regression
  y_train <- log10(df_train[[y_label]]+.00001)
  y_test <- log10(df_test[[y_label]]+.00001)
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- exp(predict(model, newdata = df_train))  # Predictions on training data
  y_pred_test <- exp(predict(model, newdata = df_test))    # Predictions on testing data
  y_train <- exp(y_train)
  y_test <- exp(y_test)
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(exp(y_test)))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[2,2] <- custom_r2_test
  act_rmse_table[2,2] <- Act_RMSE
  act_mae_table[2,2] <- Act_MAE
  pred_rmse_table[2,2] <- Pred_RMSE
  pred_mae_table[2,2] <- Pred_MAE
  
  print(paste("Log Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #Square Root Regression
  y_train <- sqrt(df_train[[y_label]])
  y_test <- sqrt(df_test[[y_label]])
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)^2  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)^2    # Predictions on testing data
  y_train <- y_train^2
  y_test <- y_test^2
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  print(prediction_df)
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[3,2] <- custom_r2_test
  act_rmse_table[3,2] <- Act_RMSE
  act_mae_table[3,2] <- Act_MAE
  pred_rmse_table[3,2] <- Pred_RMSE
  pred_mae_table[3,2] <- Pred_MAE
  
  print(paste("Square Root Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #Weighted Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  weight = y_train
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train, weights=y_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data

  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  print(prediction_df)
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[4,2] <- custom_r2_test
  act_rmse_table[4,2] <- Act_RMSE
  act_mae_table[4,2] <- Act_MAE
  pred_rmse_table[4,2] <- Pred_RMSE
  pred_mae_table[4,2] <- Pred_MAE
  
  print(paste("Weighted Regression Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #Quantile Regression
  tau_value=.75
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  weight = y_train
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- rq(formula, data = df_train, tau = tau_value)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data

  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  print(prediction_df)
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[5,2] <- custom_r2_test
  act_rmse_table[5,2] <- Act_RMSE
  act_mae_table[5,2] <- Act_MAE
  pred_rmse_table[5,2] <- Pred_RMSE
  pred_mae_table[5,2] <- Pred_MAE
  
  print(paste("Quantile Regression Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  print(length(y_test))
  print(length(y_pred_test))
y_test <- as.numeric(y_test)  # Ensure numeric
y_pred_test <- as.numeric(y_pred_test)
sum(is.na(y_test))  # Should return 0 if no NAs
sum(is.na(y_pred_test))
  
  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  print(plot_data)
  
# Set up the 2x2 plot layout
par(mfrow = c(2, 2))

# 1. Residuals vs Fitted
plot(fitted(model), residuals(model),
     main = "Residuals vs Fitted",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")

# 2. Q-Q Plot of Residuals
qqnorm(residuals(model), main = "Normal Q-Q")
qqline(residuals(model), col = "red")

# 3. Scale-Location Plot (sqrt of standardized residuals vs modelted values)
sqrt_std_resid <- sqrt(abs(residuals(model)))  # Standardized residuals
plot(fitted(model), sqrt_std_resid,
     main = "Scale-Location",
     xlab = "Fitted values",
     ylab = "Sqrt |Residuals|")
abline(h = 0, col = "red")

# 4. Residuals vs Leverage
plot(seq_along(residuals(model)), residuals(model),
     main = "Residuals vs Index",
     xlab = "Index",
     ylab = "Residuals")
abline(h = 0, col = "red")


  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  
   #Polynomial Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  poly_terms <- paste0("poly(", col, ", 3)", collapse=" + ")
  formula_string <- paste(y_label, "~", poly_terms)
  
  # Print the formula for verification
  print(formula_string)
  
  # Create the formula object
  formula <- as.formula(formula_string)
  
  # Fit the polynomial regression model (degree 3)
  model <- lm(formula, data = df_train)
  
  # Print model summary
  print(summary(model))
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test) 
    
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[6,2] <- custom_r2_test
  act_rmse_table[6,2] <- Act_RMSE
  act_mae_table[6,2] <- Act_MAE
  pred_rmse_table[6,2] <- Pred_RMSE
  pred_mae_table[6,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)

  
    #Tobit Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- tobit(formula, data=df_train, left=.3)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[7,2] <- custom_r2_test
  act_rmse_table[7,2] <- Act_RMSE
  act_mae_table[7,2] <- Act_MAE
  pred_rmse_table[7,2] <- Pred_RMSE
  pred_mae_table[7,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  fitted_values <- fitted(model)
residuals <- residuals(model)

# 1. Residuals vs Fitted plot
p1 <- ggplot(data = data.frame(fitted = fitted_values, residuals = residuals), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, col = "red") +
  labs(title = "Residuals vs Fitted", x = "Fitted values", y = "Residuals") +
  theme_minimal()

# 2. Q-Q plot of residuals
p2 <- ggplot(data = data.frame(sample = residuals), aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Q-Q plot", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# 3. Scale-Location plot (square root of residuals)
p3 <- ggplot(data = data.frame(fitted = fitted_values, residuals = sqrt(abs(residuals))), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, col = "red") +
  labs(title = "Scale-Location", x = "Fitted values", y = "Sqrt(|Residuals|)") +
  theme_minimal()

# 4. Residuals vs Leverage (no hatvalues for tobit, so using fitted values as a proxy)
p4 <- ggplot(data = data.frame(fitted = fitted_values, residuals = residuals), aes(x = fitted, y = residuals)) +
  geom_point() +
  labs(title = "Residuals vs Leverage (approx.)", x = "Fitted values", y = "Residuals") +
  theme_minimal()

# Arrange the plots in a 2x2 grid
grid.arrange(p1, p2, p3, p4, nrow = 2)


  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
  #Cubic Spline Regression
  knots <- c(.7, .75, .8, .85, .9, .95)

y_train <- df_train[[y_label]]
y_test <- df_test[[y_label]]

# Create spline terms for multiple columns
spline_terms <- sapply(col, function(c) paste("bs(", c, ", knots = c(", paste(knots, collapse = ", "), "), degree = 3)", sep = ""))
formula_string <- paste(y_label, "~", paste(spline_terms, collapse = " + "))

# Print the formula for verification
print(formula_string)

# Convert the formula string to an actual formula object
formula <- as.formula(formula_string)

# Train the model with the B-spline transformations
model <- lm(formula, data = df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[8,2] <- custom_r2_test
  act_rmse_table[8,2] <- Act_RMSE
  act_mae_table[8,2] <- Act_MAE
  pred_rmse_table[8,2] <- Pred_RMSE
  pred_mae_table[8,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #KNN Regression

  X_trn_df <- as.matrix(df_train[, col, drop = FALSE])
  y_trn_df <- as.numeric(df_train[[y_label]])
  X_tst_df <- as.matrix(df_test[, col, drop = FALSE])
  y_tst_df <- as.numeric(df_test[[y_label]])
  
  n_neighbors=30
  
  # Make predictions on both the training and test sets
  y_pred_train <- knn.reg(train = X_trn_df, test = X_trn_df, y = y_trn_df, k = n_neighbors)

  y_pred_test <- knn.reg(train = X_trn_df, test = X_tst_df, y = y_trn_df, k = n_neighbors)

  residuals_test <- y_tst_df - y_pred_train$pred
  

  
  prediction_df <- data.frame(Prediction=c(y_pred_test$pred), Actual=c(y_tst_df))

  
  custom_r2_train <- custom_r2(y_trn_df, y_pred_train$pred, y_tst_df, y_pred_test$pred)$r2_train
  custom_r2_test <- custom_r2(y_trn_df, y_pred_train$pred, y_tst_df, y_pred_test$pred)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  
  custom_r2_table[9,2] <- custom_r2_test
  act_rmse_table[9,2] <- Act_RMSE
  act_mae_table[9,2] <- Act_MAE
  pred_rmse_table[9,2] <- Pred_RMSE
  pred_mae_table[9,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  # Calculate residuals
  residuals_train <- y_trn_df - y_pred_train$pred
  residuals_test <- y_tst_df - y_pred_test$pred
  
  # Create a data frame for plotting
  plot_data_train <- data.frame(Fitted = y_pred_train$pred, Residuals = residuals_train)
  plot_data_test <- data.frame(Fitted = y_pred_test$pred, Residuals = residuals_test)
  
  # Set up the plotting area for 2x2 layout
  par(mfrow = c(2, 2))
  
  # 1. Residuals vs Fitted
  plot(plot_data_train$Fitted, plot_data_train$Residuals,
       xlab = "Fitted Values", ylab = "Residuals",
       main = "Residuals vs Fitted",
       pch = 19, col = "blue")
  abline(h = 0, lty = 2, col = "red")
  
  # 2. Q-Q plot of residuals
  qqnorm(residuals_train, main = "Q-Q Plot of Residuals")
  qqline(residuals_train, col = "red")
  
  # 3. Scale-Location Plot
  sqrt_residuals <- sqrt(abs(plot_data_train$Residuals))
  plot(plot_data_train$Fitted, sqrt_residuals,
       xlab = "Fitted Values", ylab = "Sqrt |Residuals|",
       main = "Scale-Location",
       pch = 19, col = "blue")
  abline(h = 0, lty = 2, col = "red")
  
  # 4. Residuals vs Leverage (not typically applicable for KNN)
  # For this plot, we can just show residuals vs fitted values again
  plot(plot_data_train$Fitted, residuals_train,
       xlab = "Fitted Values", ylab = "Residuals",
       main = "Residuals vs Fitted (Leverage Plot)",
       pch = 19, col = "blue")
  abline(h = 0, lty = 2, col = "red")

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
    #GAM Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Create the formula for the model (use s() to fit smooth terms)
  smooth_terms <- paste("s(", col, ")", collapse=" + ")  # Adding smooth terms for each predictor
  formula_string <- paste(y_label, "~", smooth_terms)
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the GAM model
  model <- gam(formula, data = df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[10,2] <- custom_r2_test
  act_rmse_table[10,2] <- Act_RMSE
  act_mae_table[10,2] <- Act_MAE
  pred_rmse_table[10,2] <- Pred_RMSE
  pred_mae_table[10,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  gam.check(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
  
#XGBoost
library(xgboost)

# Convert data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(df_train[col]), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(df_test[col]), label = y_test)

# Set XGBoost parameters
params <- list(
  objective = "reg:squarederror",
  booster = "gbtree",
  eta = 0.1,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train the model
model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = 0
)

# Make predictions
y_pred_train <- predict(model, dtrain)
y_pred_test <- predict(model, dtest)

# Calculate metrics
prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
Act_RMSE <- act_custom_rmse(prediction_df)
Act_MAE <- act_custom_mae(prediction_df)
Pred_RMSE <- pred_custom_rmse(prediction_df)
Pred_MAE <- pred_custom_mae(prediction_df)

# Update tables
custom_r2_table[11,2] <- custom_r2_test
act_rmse_table[11,2] <- Act_RMSE
act_mae_table[11,2] <- Act_MAE
pred_rmse_table[11,2] <- Pred_RMSE
pred_mae_table[11,2] <- Pred_MAE

# Print summary
print(paste("XGBoost Model Summary for: ", title))
print(paste("Custom R² (Train):", custom_r2_train))
print(paste("Custom R² (Test):", custom_r2_test))

# Create and display Predicted vs Actual plot
plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
p <- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = title,
       x = "Predicted Values",
       y = y_label) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

print(p)

# Optional: Cross-validation
cv <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 5,
  early_stopping_rounds = 10,
  verbose = 0
)

print(paste("Best iteration:", which.min(cv$evaluation_log$test_rmse_mean)))
    
  print(custom_r2_table)
  print(act_rmse_table)
  print(pred_rmse_table)
  print(act_mae_table)
  print(pred_mae_table)
  return(list(r2=custom_r2_table, actual_rmse=act_rmse_table, predicted_rmse=pred_rmse_table, actual_mae=act_mae_table, predicted_mae=pred_mae_table))
}
  

  model_comp_2019 <- model_comp(df_race, 2019, c("Prev_Pelo"), "Previous Elo", "Pct_of_Max_Points", "Comparing Previous Elo and Pct_of_Max_Points in 2019")
  model_comp_2020 <- model_comp(df_race, 2020, c("Prev_Pelo"), "Previous Elo", "Pct_of_Max_Points", "Comparing Previous Elo and Pct_of_Max_Points in 2020")
  model_comp_2021 <- model_comp(df_race, 2021, c("Prev_Pelo"), "Previous Elo", "Pct_of_Max_Points", "Comparing Previous Elo and Pct_of_Max_Points in 2021")
  model_comp_2022 <- model_comp(df_race, 2022, c("Prev_Pelo"), "Previous Elo", "Pct_of_Max_Points", "Comparing Previous Elo and Pct_of_Max_Points in 2022")
  model_comp_2023 <- model_comp(df_race, 2023, c("Prev_Pelo"), "Previous Elo", "Pct_of_Max_Points", "Comparing Previous Elo and Pct_of_Max_Points in 2023")
  model_comp_2024 <- model_comp(df_race, 2024, c("Prev_Pelo"), "Previous Elo", "Pct_of_Max_Points", "Comparing Previous Elo and Pct_of_Max_Points in 2024")
  
  library(openxlsx)
# Sequentially merge data frames on the "Skier" column
r2_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list(model_comp_2019$r2, model_comp_2020$r2, model_comp_2021$r2, 
                         model_comp_2022$r2, model_comp_2023$r2, model_comp_2024$r2))

actual_rmse_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list(model_comp_2019$actual_rmse, model_comp_2020$actual_rmse, model_comp_2021$actual_rmse, model_comp_2022$actual_rmse, model_comp_2023$actual_rmse, model_comp_2024$actual_rmse))

predicted_rmse_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list( model_comp_2019$predicted_rmse, model_comp_2020$predicted_rmse, model_comp_2021$predicted_rmse, model_comp_2022$predicted_rmse, model_comp_2023$predicted_rmse, model_comp_2024$predicted_rmse))

actual_mae_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list( model_comp_2019$actual_mae, model_comp_2020$actual_mae, model_comp_2021$actual_mae, model_comp_2022$actual_mae, model_comp_2023$actual_mae, model_comp_2024$actual_mae))

predicted_mae_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list( model_comp_2019$predicted_mae, model_comp_2020$predicted_mae, model_comp_2021$predicted_mae, model_comp_2022$predicted_mae, model_comp_2023$predicted_mae, model_comp_2024$predicted_mae))

r2_df$Avg <- rowMeans(r2_df[, seq(2, ncol(r2_df), by = 1)], na.rm = TRUE)
r2_df <- r2_df[order(-r2_df$Avg), ] 
write.xlsx(r2_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_simple-model-tests-r2.xlsx")

actual_rmse_df$Avg <- rowMeans(actual_rmse_df[, seq(2, ncol(actual_rmse_df), by = 1)], na.rm = TRUE)
actual_rmse_df <- actual_rmse_df[order(actual_rmse_df$Avg), ] 
write.xlsx(actual_rmse_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/race-prediction/men_simple-model-tests-actual_rmse.xlsx")

predicted_rmse_df$Avg <- rowMeans(predicted_rmse_df[, seq(2, ncol(predicted_rmse_df), by = 1)], na.rm = TRUE)
predicted_rmse_df <- predicted_rmse_df[order(predicted_rmse_df$Avg), ] 
write.xlsx(predicted_rmse_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_simple-model-tests-predicted_rmse.xlsx")

actual_mae_df$Avg <- rowMeans(actual_mae_df[, seq(2, ncol(actual_mae_df), by = 1)], na.rm = TRUE)
actual_mae_df <- actual_mae_df[order(actual_mae_df$Avg), ] 
write.xlsx(actual_mae_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_simple-model-tests-actual_mae.xlsx")

predicted_mae_df$Avg <- rowMeans(predicted_mae_df[, seq(2, ncol(predicted_mae_df), by = 1)], na.rm = TRUE)
predicted_mae_df <- predicted_mae_df[order(predicted_mae_df$Avg), ] 
write.xlsx(predicted_mae_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_simple-model-tests-predicted_mae.xlsx")
```

## Feature Selection
```{r exhaustive-search}
# Step 1: Install and load the leaps package
library(leaps)

# Step 2: Prepare your data
response_variable <- "Pct_of_Max_Points"
explanatory_vars <- c("Prev_Pelo", "Prev_Distance", "Prev_Distance_C", "Prev_Distance_F", "Prev_Sprint", "Prev_Sprint_C", "Prev_Sprint_F", "Prev_F", "Prev_C", "Prev_Pct_of_Max_Points")

# Step 3: Create a formula for the model
formula <- as.formula(paste(response_variable, "~", paste(explanatory_vars, collapse = " + ")))
train_data <- df_race %>% filter(Season < 2024)

# Filter testing data for season 2024
test_data <- df_race %>% filter(Season == 2024)
# Step 4: Perform exhaustive search
exhaustive_selection <- regsubsets(formula, data = train_data, nbest = 1, method = "exhaustive")

# Step 5: Summary of the exhaustive search
summary_exhaustive <- summary(exhaustive_selection)

# Step 6: Evaluate the results
# For example, you can look for the model with the highest adjusted R-squared
best_model_index <- which.max(summary_exhaustive$adjr2)
best_selected_vars <- names(coef(exhaustive_selection, best_model_index))

# Display the best selected variables
best_selected_vars



# Example to find the best model based on BIC
best_bic_model_index <- which.min(summary_exhaustive$bic)
best_bic_selected_vars <- names(coef(exhaustive_selection, best_bic_model_index))

# Display the best BIC selected variables
best_bic_selected_vars


```


## Model Selection

### Exhaustive Model
```{r exhaustive-model}
library(quantreg)
library(gridExtra)
custom_r2 <- function(y_train, y_pred_train, y_test, y_pred_test){
    # Calculate custom weighted R² for train data
  custom_weight_train <- y_train  # Using y_train as weights
  weighted_rss_train <- sum(custom_weight_train * (y_train - y_pred_train)^2)
  weighted_tss_train <- sum(custom_weight_train * (y_train - mean(y_train))^2)
  custom_r2_train <- 1 - (weighted_rss_train / weighted_tss_train)
  
  # Calculate custom weighted R² for test data
  custom_weight_test <- y_test  # Using y_test as weights
  weighted_rss_test <- sum(custom_weight_test * (y_test - y_pred_test)^2)
  weighted_tss_test <- sum(custom_weight_test * (y_test - mean(y_test))^2)
  custom_r2_test <- 1 - (weighted_rss_test / weighted_tss_test)
  
  # Print custom R² scores

  return(list(r2_train = custom_r2_train, r2_test = custom_r2_test))
}

act_custom_rmse <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Actual), ]
  prediction_df$RMSE <- (prediction_df$Prediction-prediction_df$Actual)^2
  prediction_df <- prediction_df %>%
    filter(Actual>=.22)
  RMSE <- sqrt(mean(prediction_df$RMSE))
  
  return(RMSE)
}

act_custom_mae <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Actual), ]
  prediction_df$MAE <- abs(prediction_df$Prediction-prediction_df$Actual)
  prediction_df <- prediction_df %>%
    filter(Actual>=.22)
  MAE <- mean(prediction_df$MAE)
  
  return(MAE)
}

pred_custom_rmse <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Prediction), ]
  prediction_df$RMSE <- (prediction_df$Prediction-prediction_df$Actual)^2
  prediction_df <- prediction_df %>%
    filter(Prediction>=.22)
  RMSE <- sqrt(mean(prediction_df$RMSE))
  
  return(RMSE)
}

pred_custom_mae <- function(prediction_df){
  #Want to know how close I am to actual results
  prediction_df <- prediction_df[order(-prediction_df$Prediction), ]
  prediction_df$MAE <- abs(prediction_df$Prediction-prediction_df$Actual)
  prediction_df <- prediction_df %>%
    filter(Prediction>=.22)
  MAE <- mean(prediction_df$MAE)
  
  return(MAE)
}


model_comp <- function(df, season, col, X_label, y_label, title){
  df_train <- df %>%
    filter(Season!=season)
  df_test <- df %>%
    filter(Season==season)
  custom_r2_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost", "Random Forest"),
                               R2=NA)  # Initialize with a generic name
  colnames(custom_r2_table)[2] <- paste("R2", season, sep="_")
 # Initialize the column with NA values

  act_rmse_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost", "Random Forest"),
                               RMSE=NA)  # Initialize with a generic name
  colnames(act_rmse_table)[2] <- paste("Actual_RMSE", season, sep="_")
  
  act_mae_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost", "Random Forest"),
                               MAE=NA)  # Initialize with a generic name
  colnames(act_mae_table)[2] <- paste("Actual_MAE", season, sep="_")
  
  pred_rmse_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost", "Random Forest"),
                               RMSE=NA)  # Initialize with a generic name
  colnames(pred_rmse_table)[2] <- paste("Pred_RMSE", season, sep="_")
  
  pred_mae_table <- data.frame(Model=c("Linear", "Log", "Square Root", "Weighted", "Quantile", 
                                       "Polynomial", "Tobit", "Spline", "KNN", "GAM", "XGBoost", "Random Forest"),
                               MAE=NA)  # Initialize with a generic name
  colnames(pred_mae_table)[2] <- paste("Pred_MAE", season, sep="_")
  
  #Linear Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[1,2] <- custom_r2_test
  act_rmse_table[1,2] <- Act_RMSE
  act_mae_table[1,2] <- Act_MAE
  pred_rmse_table[1,2] <- Pred_RMSE
  pred_mae_table[1,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
  
  #Log Regression
  y_train <- log10(df_train[[y_label]]+.00001)
  y_test <- log10(df_test[[y_label]]+.00001)
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- exp(predict(model, newdata = df_train))  # Predictions on training data
  y_pred_test <- exp(predict(model, newdata = df_test))    # Predictions on testing data
  y_train <- exp(y_train)
  y_test <- exp(y_test)
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(exp(y_test)))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[2,2] <- custom_r2_test
  act_rmse_table[2,2] <- Act_RMSE
  act_mae_table[2,2] <- Act_MAE
  pred_rmse_table[2,2] <- Pred_RMSE
  pred_mae_table[2,2] <- Pred_MAE
  
  print(paste("Log Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #Square Root Regression
  y_train <- sqrt(df_train[[y_label]])
  y_test <- sqrt(df_test[[y_label]])
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)^2  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)^2    # Predictions on testing data
  y_train <- y_train^2
  y_test <- y_test^2
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  print(prediction_df)
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[3,2] <- custom_r2_test
  act_rmse_table[3,2] <- Act_RMSE
  act_mae_table[3,2] <- Act_MAE
  pred_rmse_table[3,2] <- Pred_RMSE
  pred_mae_table[3,2] <- Pred_MAE
  
  print(paste("Square Root Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #Weighted Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  weight = y_train
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- lm(formula, data=df_train, weights=y_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data

  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  print(prediction_df)
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[4,2] <- custom_r2_test
  act_rmse_table[4,2] <- Act_RMSE
  act_mae_table[4,2] <- Act_MAE
  pred_rmse_table[4,2] <- Pred_RMSE
  pred_mae_table[4,2] <- Pred_MAE
  
  print(paste("Weighted Regression Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #Quantile Regression
  tau_value=.75
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  weight = y_train
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- rq(formula, data = df_train, tau = tau_value)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data

  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  print(prediction_df)
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[5,2] <- custom_r2_test
  act_rmse_table[5,2] <- Act_RMSE
  act_mae_table[5,2] <- Act_MAE
  pred_rmse_table[5,2] <- Pred_RMSE
  pred_mae_table[5,2] <- Pred_MAE
  
  print(paste("Quantile Regression Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  print(length(y_test))
  print(length(y_pred_test))
y_test <- as.numeric(y_test)  # Ensure numeric
y_pred_test <- as.numeric(y_pred_test)
sum(is.na(y_test))  # Should return 0 if no NAs
sum(is.na(y_pred_test))
  
  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  print(plot_data)
  
# Set up the 2x2 plot layout
par(mfrow = c(2, 2))

# 1. Residuals vs Fitted
plot(fitted(model), residuals(model),
     main = "Residuals vs Fitted",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")

# 2. Q-Q Plot of Residuals
qqnorm(residuals(model), main = "Normal Q-Q")
qqline(residuals(model), col = "red")

# 3. Scale-Location Plot (sqrt of standardized residuals vs modelted values)
sqrt_std_resid <- sqrt(abs(residuals(model)))  # Standardized residuals
plot(fitted(model), sqrt_std_resid,
     main = "Scale-Location",
     xlab = "Fitted values",
     ylab = "Sqrt |Residuals|")
abline(h = 0, col = "red")

# 4. Residuals vs Leverage
plot(seq_along(residuals(model)), residuals(model),
     main = "Residuals vs Index",
     xlab = "Index",
     ylab = "Residuals")
abline(h = 0, col = "red")


  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  
   #Polynomial Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  poly_terms <- paste0("poly(", col, ", 3)", collapse=" + ")
  formula_string <- paste(y_label, "~", poly_terms)
  
  # Print the formula for verification
  print(formula_string)
  
  # Create the formula object
  formula <- as.formula(formula_string)
  
  # Fit the polynomial regression model (degree 3)
  model <- lm(formula, data = df_train)
  
  # Print model summary
  print(summary(model))
  
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test) 
    
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[6,2] <- custom_r2_test
  act_rmse_table[6,2] <- Act_RMSE
  act_mae_table[6,2] <- Act_MAE
  pred_rmse_table[6,2] <- Pred_RMSE
  pred_mae_table[6,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)

  
    #Tobit Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  formula_string <- paste(y_label, "~", paste(col, collapse=" + "))
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  model <- tobit(formula, data=df_train, left=.3)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[7,2] <- custom_r2_test
  act_rmse_table[7,2] <- Act_RMSE
  act_mae_table[7,2] <- Act_MAE
  pred_rmse_table[7,2] <- Pred_RMSE
  pred_mae_table[7,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  fitted_values <- fitted(model)
residuals <- residuals(model)

# 1. Residuals vs Fitted plot
p1 <- ggplot(data = data.frame(fitted = fitted_values, residuals = residuals), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, col = "red") +
  labs(title = "Residuals vs Fitted", x = "Fitted values", y = "Residuals") +
  theme_minimal()

# 2. Q-Q plot of residuals
p2 <- ggplot(data = data.frame(sample = residuals), aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Q-Q plot", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# 3. Scale-Location plot (square root of residuals)
p3 <- ggplot(data = data.frame(fitted = fitted_values, residuals = sqrt(abs(residuals))), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, col = "red") +
  labs(title = "Scale-Location", x = "Fitted values", y = "Sqrt(|Residuals|)") +
  theme_minimal()

# 4. Residuals vs Leverage (no hatvalues for tobit, so using fitted values as a proxy)
p4 <- ggplot(data = data.frame(fitted = fitted_values, residuals = residuals), aes(x = fitted, y = residuals)) +
  geom_point() +
  labs(title = "Residuals vs Leverage (approx.)", x = "Fitted values", y = "Residuals") +
  theme_minimal()

# Arrange the plots in a 2x2 grid
grid.arrange(p1, p2, p3, p4, nrow = 2)


  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
  #Cubic Spline Regression
  knots <- c(.7, .75, .8, .85, .9, .95)

y_train <- df_train[[y_label]]
y_test <- df_test[[y_label]]

# Create spline terms for multiple columns
spline_terms <- sapply(col, function(c) paste("bs(", c, ", knots = c(", paste(knots, collapse = ", "), "), degree = 3)", sep = ""))
formula_string <- paste(y_label, "~", paste(spline_terms, collapse = " + "))

# Print the formula for verification
print(formula_string)

# Convert the formula string to an actual formula object
formula <- as.formula(formula_string)

# Train the model with the B-spline transformations
model <- lm(formula, data = df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[8,2] <- custom_r2_test
  act_rmse_table[8,2] <- Act_RMSE
  act_mae_table[8,2] <- Act_MAE
  pred_rmse_table[8,2] <- Pred_RMSE
  pred_mae_table[8,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  plot(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)  
  
  #KNN Regression

  X_trn_df <- as.matrix(df_train[, col, drop = FALSE])
  y_trn_df <- as.numeric(df_train[[y_label]])
  X_tst_df <- as.matrix(df_test[, col, drop = FALSE])
  y_tst_df <- as.numeric(df_test[[y_label]])
  
  n_neighbors=30
  
  # Make predictions on both the training and test sets
  y_pred_train <- knn.reg(train = X_trn_df, test = X_trn_df, y = y_trn_df, k = n_neighbors)

  y_pred_test <- knn.reg(train = X_trn_df, test = X_tst_df, y = y_trn_df, k = n_neighbors)

  residuals_test <- y_tst_df - y_pred_train$pred
  

  
  prediction_df <- data.frame(Prediction=c(y_pred_test$pred), Actual=c(y_tst_df))

  
  custom_r2_train <- custom_r2(y_trn_df, y_pred_train$pred, y_tst_df, y_pred_test$pred)$r2_train
  custom_r2_test <- custom_r2(y_trn_df, y_pred_train$pred, y_tst_df, y_pred_test$pred)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  
  custom_r2_table[9,2] <- custom_r2_test
  act_rmse_table[9,2] <- Act_RMSE
  act_mae_table[9,2] <- Act_MAE
  pred_rmse_table[9,2] <- Pred_RMSE
  pred_mae_table[9,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  # Calculate residuals
  residuals_train <- y_trn_df - y_pred_train$pred
  residuals_test <- y_tst_df - y_pred_test$pred
  
  # Create a data frame for plotting
  plot_data_train <- data.frame(Fitted = y_pred_train$pred, Residuals = residuals_train)
  plot_data_test <- data.frame(Fitted = y_pred_test$pred, Residuals = residuals_test)
  
  # Set up the plotting area for 2x2 layout
  par(mfrow = c(2, 2))
  
  # 1. Residuals vs Fitted
  plot(plot_data_train$Fitted, plot_data_train$Residuals,
       xlab = "Fitted Values", ylab = "Residuals",
       main = "Residuals vs Fitted",
       pch = 19, col = "blue")
  abline(h = 0, lty = 2, col = "red")
  
  # 2. Q-Q plot of residuals
  qqnorm(residuals_train, main = "Q-Q Plot of Residuals")
  qqline(residuals_train, col = "red")
  
  # 3. Scale-Location Plot
  sqrt_residuals <- sqrt(abs(plot_data_train$Residuals))
  plot(plot_data_train$Fitted, sqrt_residuals,
       xlab = "Fitted Values", ylab = "Sqrt |Residuals|",
       main = "Scale-Location",
       pch = 19, col = "blue")
  abline(h = 0, lty = 2, col = "red")
  
  # 4. Residuals vs Leverage (not typically applicable for KNN)
  # For this plot, we can just show residuals vs fitted values again
  plot(plot_data_train$Fitted, residuals_train,
       xlab = "Fitted Values", ylab = "Residuals",
       main = "Residuals vs Fitted (Leverage Plot)",
       pch = 19, col = "blue")
  abline(h = 0, lty = 2, col = "red")

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
    #GAM Regression
  y_train <- df_train[[y_label]]
  y_test <- df_test[[y_label]]
  
  # Create the formula for the model (use s() to fit smooth terms)
  smooth_terms <- paste("s(", col, ")", collapse=" + ")  # Adding smooth terms for each predictor
  formula_string <- paste(y_label, "~", smooth_terms)
  print(formula_string)  # Print the formula for verification
  formula <- as.formula(formula_string)
  
  # Train the GAM model
  model <- gam(formula, data = df_train)
  print(summary(model))  # Print model summary
  # Make predictions
  y_pred_train <- predict(model, newdata = df_train)  # Predictions on training data
  y_pred_test <- predict(model, newdata = df_test)    # Predictions on testing data
  
  prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
  custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
  custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
  Act_RMSE <- act_custom_rmse(prediction_df)
  Act_MAE <- act_custom_mae(prediction_df)
  Pred_RMSE <- pred_custom_rmse(prediction_df)
  Pred_MAE <- pred_custom_mae(prediction_df)
  
  
  custom_r2_table[10,2] <- custom_r2_test
  act_rmse_table[10,2] <- Act_RMSE
  act_mae_table[10,2] <- Act_MAE
  pred_rmse_table[10,2] <- Pred_RMSE
  pred_mae_table[10,2] <- Pred_MAE
  
  print(paste("Linear Model Summary for: ", title))
  print(paste("Custom R² (Train):", custom_r2_train))
  print(paste("Custom R² (Test):", custom_r2_test))

  plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
  
  par(mfrow = c(2, 2))
  gam.check(model)

  
  # ggplot for Predicted vs Actual
  p1<- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = title,
         x = "Predicted Values",
         y = y_label) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
  
  print(p1)
  
  
#XGBoost
library(xgboost)

# Convert data to DMatrix format
col <- c("Prev_Pelo", "Prev_Distance", "Prev_Distance_C", "Prev_Distance_F", "Prev_Sprint", "Prev_Sprint_C", "Prev_Sprint_F", "Prev_F", "Prev_C", "Prev_Pct_of_Max_Points")
dtrain <- xgb.DMatrix(data = as.matrix(df_train[col]), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(df_test[col]), label = y_test)

# Set XGBoost parameters
# params <- list(
#   objective = "reg:squarederror",
#   eval_metric = "rmse",
#   booster = "gbtree",
#   eta = 0.1,
#   max_depth = 6,
#   min_child_weight = 1,
#   subsample = 0.8,
#   colsample_bytree = 0.8
# )


params <- list(
  booster = "gblinear",
  objective = "reg:squarederror",
  eval_metric = "rmse",
  lambda = 0.1,        # L2 regularization
  alpha = 0,          # L1 regularization
  lambda_bias = 0,    # L2 regularization on bias
  eta = 0.3          # Learning rate
)

# Train the model
model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = 0
)

# Make predictions
y_pred_train <- predict(model, dtrain)
y_pred_test <- predict(model, dtest)

# Calculate metrics
prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
Act_RMSE <- act_custom_rmse(prediction_df)
Act_MAE <- act_custom_mae(prediction_df)
Pred_RMSE <- pred_custom_rmse(prediction_df)
Pred_MAE <- pred_custom_mae(prediction_df)

# Update tables
custom_r2_table[11,2] <- custom_r2_test
act_rmse_table[11,2] <- Act_RMSE
act_mae_table[11,2] <- Act_MAE
pred_rmse_table[11,2] <- Pred_RMSE
pred_mae_table[11,2] <- Pred_MAE

# Print summary
print(paste("XGBoost Model Summary for: ", title))
print(paste("Custom R² (Train):", custom_r2_train))
print(paste("Custom R² (Test):", custom_r2_test))

# Create and display Predicted vs Actual plot
plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)
p <- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = title,
       x = "Predicted Values",
       y = y_label) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

print(p)

# Optional: Cross-validation
cv <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 5,
  early_stopping_rounds = 10,
  verbose = 0
)

print(paste("Best iteration:", which.min(cv$evaluation_log$test_rmse_mean)))

# Random Forest
# Convert data format (keeping this line for consistency with your framework)
col <- c("Prev_Pelo", "Prev_Distance", "Prev_Distance_C", "Prev_Distance_F", "Prev_Sprint", "Prev_Sprint_C", "Prev_Sprint_F", "Prev_F", "Prev_C", "Prev_Pct_of_Max_Points")

# Train the model
model <- randomForest(
  formula = as.formula(paste("Pct_of_Max_Points ~", paste(col, collapse = " + "))),
  data = df_train,
  ntree = 500,
  importance = TRUE
)

# Make predictions
y_pred_train <- predict(model, newdata = df_train)
y_pred_test <- predict(model, newdata = df_test)

# Calculate metrics
prediction_df <- data.frame(Prediction=c(y_pred_test), Actual=c(y_test))
custom_r2_train <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_train
custom_r2_test <- custom_r2(y_train, y_pred_train, y_test, y_pred_test)$r2_test
Act_RMSE <- act_custom_rmse(prediction_df)
Act_MAE <- act_custom_mae(prediction_df)
Pred_RMSE <- pred_custom_rmse(prediction_df)
Pred_MAE <- pred_custom_mae(prediction_df)

# Update tables
custom_r2_table[12,2] <- custom_r2_test
act_rmse_table[12,2] <- Act_RMSE
act_mae_table[12,2] <- Act_MAE
pred_rmse_table[12,2] <- Pred_RMSE
pred_mae_table[12,2] <- Pred_MAE

# Print summary
print(paste("Random Forest Model Summary for:", title))
print(paste("Custom R² (Train):", custom_r2_train))
print(paste("Custom R² (Test):", custom_r2_test))

# Error rate vs number of trees plot
error_df <- data.frame(
  Trees = 1:model$ntree,
  Error = model$mse
)

p1 <- ggplot(error_df, aes(x = Trees, y = sqrt(Error))) +
  geom_line() +
  labs(
    x = "Number of trees",
    y = "RMSE (out-of-bag)",
    title = "Random Forest Learning Curve"
  ) +
  theme_minimal()

print(p1)

# Variable importance plot
importance_df <- data.frame(
  Feature = rownames(randomForest::importance(model)),
  Importance = randomForest::importance(model)[,"%IncMSE"]
) %>%
  dplyr::arrange(desc(Importance))

p2 <- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    x = "Features",
    y = "Importance (% Increase in MSE)",
    title = "Feature Importance in Random Forest Model"
  ) +
  theme_minimal()

print(p2)

# Actual vs Predicted plot
plot_data <- data.frame(Actual = y_test, Predicted = y_pred_test)

p3 <- ggplot(plot_data, aes(x = Predicted, y = Actual)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = title,
       x = "Predicted Values",
       y = y_label) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

print(p3)

# Print top feature importances
print("Top Feature Importances:")
print(importance_df)

# Return model and importance information


# Print top 5 most important variables
#print("Top 3 Most Important Variables:")
#print(head(importance_df[order(-importance_df$IncNodePurity), ], 5))

    
  print(custom_r2_table)
  print(act_rmse_table)
  print(pred_rmse_table)
  print(act_mae_table)
  print(pred_mae_table)
  return(list(r2=custom_r2_table, actual_rmse=act_rmse_table, predicted_rmse=pred_rmse_table, actual_mae=act_mae_table, predicted_mae=pred_mae_table))
  

}
  explanatory_vars <- c("Prev_Pct_of_Max_Points", "Prev_Distance", "Prev_Sprint")
  


  model_comp_2019 <- model_comp(df_race, 2019, explanatory_vars, "Distance Classic Pelo", "Pct_of_Max_Points", "Comparing Distance Classic Elo and Points in 2019")
  model_comp_2020 <- model_comp(df_race, 2020, explanatory_vars, "Distance Classic Pelo", "Pct_of_Max_Points", "Comparing Distance Classic Elo and Points in 2020")
  model_comp_2021 <- model_comp(df_race, 2021, explanatory_vars, "Distance Classic Pelo", "Pct_of_Max_Points", "Comparing Distance Classic Elo and Points in 2021")
  model_comp_2022 <- model_comp(df_race, 2022, explanatory_vars, "Distance Classic Pelo", "Pct_of_Max_Points", "Comparing Distance Classic Elo and Points in 2022")
  model_comp_2023 <- model_comp(df_race, 2023, explanatory_vars, "Distance Classic Pelo", "Pct_of_Max_Points", "Comparing Distance Classic Elo and Points in 2023")
  model_comp_2024 <- model_comp(df_race, 2024, explanatory_vars, "Distance Classic Pelo", "Pct_of_Max_Points", "Comparing Distance Classic Elo and Points in 2024")
  
  library(openxlsx)
# Sequentially merge data frames on the "Skier" column
r2_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list(model_comp_2019$r2, model_comp_2020$r2, model_comp_2021$r2, 
                         model_comp_2022$r2, model_comp_2023$r2, model_comp_2024$r2))

actual_rmse_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list(model_comp_2019$actual_rmse, model_comp_2020$actual_rmse, model_comp_2021$actual_rmse, model_comp_2022$actual_rmse, model_comp_2023$actual_rmse, model_comp_2024$actual_rmse))

predicted_rmse_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list( model_comp_2019$predicted_rmse, model_comp_2020$predicted_rmse, model_comp_2021$predicted_rmse, model_comp_2022$predicted_rmse, model_comp_2023$predicted_rmse, model_comp_2024$predicted_rmse))

actual_mae_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list(model_comp_2019$actual_mae, model_comp_2020$actual_mae, model_comp_2021$actual_mae, model_comp_2022$actual_mae, model_comp_2023$actual_mae, model_comp_2024$actual_mae))

predicted_mae_df <- Reduce(function(x, y) merge(x, y, by = "Model", all = TRUE), 
                    list(model_comp_2019$predicted_mae, model_comp_2020$predicted_mae, model_comp_2021$predicted_mae, model_comp_2022$predicted_mae, model_comp_2023$predicted_mae, model_comp_2024$predicted_mae))



r2_df$Avg <- rowMeans(r2_df[, seq(2, ncol(r2_df), by = 1)], na.rm = TRUE)
r2_df <- r2_df[order(-r2_df$Avg), ] 
write.xlsx(r2_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_model-exhaustive-tests-r2.xlsx")

actual_rmse_df$Avg <- rowMeans(actual_rmse_df[, seq(2, ncol(actual_rmse_df), by = 1)], na.rm = TRUE)
actual_rmse_df <- actual_rmse_df[order(actual_rmse_df$Avg), ] 
write.xlsx(actual_rmse_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_model-exhaustive-tests-actual_rmse.xlsx")

predicted_rmse_df$Avg <- rowMeans(predicted_rmse_df[, seq(2, ncol(predicted_rmse_df), by = 1)], na.rm = TRUE)
predicted_rmse_df <- predicted_rmse_df[order(predicted_rmse_df$Avg), ] 
write.xlsx(predicted_rmse_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_model-exhaustive-tests-predicted_rmse.xlsx")

actual_mae_df$Avg <- rowMeans(actual_mae_df[, seq(2, ncol(actual_mae_df), by = 1)], na.rm = TRUE)
actual_mae_df <- actual_mae_df[order(actual_mae_df$Avg), ] 
write.xlsx(actual_mae_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_model-resursive-tests-actual_mae.xlsx")

predicted_mae_df$Avg <- rowMeans(predicted_mae_df[, seq(2, ncol(predicted_mae_df), by = 1)], na.rm = TRUE)
predicted_mae_df <- predicted_mae_df[order(predicted_mae_df$Avg), ] 
write.xlsx(predicted_mae_df, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_model-exhaustive-tests-predicted_mae.xlsx")

```

Both Random Forest and GAM were really good, but will continue with the GAM train here since it seems to be better with the top level ever so slightly.

## Results examination
```{r results-examination}
explanatory_vars <- c("Prev_Pct_of_Max_Points", "Prev_Distance", "Prev_Sprint")
winning_model <- gam(Pct_of_Max_Points ~ s(Prev_Pct_of_Max_Points) + s(Prev_Distance) + s(Prev_Sprint), data=df_race)

model_2025 <- gam(Pct_of_Max_Points ~ s(Prev_Pct_of_Max_Points) + s(Distance_Pelo) + s(Sprint_Pelo), data=df_race)
df_2025 <- df_race %>% filter(Season == 2024)
predictions <- predict(model_2025, newdata=df_2025, se.fit=TRUE)
# Calculate confidence intervals (using 95% CI)
df_2025$Predicted_Points <- predictions$fit
df_2025$CI_Lower <- predictions$fit - (1.96 * predictions$se.fit)
df_2025$CI_Upper <- predictions$fit + (1.96 * predictions$se.fit)

# Constrain to [0,1] if needed
df_2025$CI_Lower <- pmax(df_2025$CI_Lower, 0)
df_2025$CI_Upper <- pmin(df_2025$CI_Upper, 1)

# Display results

output_25 = df_2025[order(-df_2025$Predicted_Points), 
        c("Skier", "Nation","Pct_of_Max_Points", "Predicted_Points", "CI_Lower", "CI_Upper")]

output_25 <- output_25[,1:4]

colnames(output_25) <- c("Skier", "Nation","2023-24 Points", "2024-25 Predicted Points")

write.xlsx(output_25, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_predictions.xlsx")

```



##Probabilities



###Monte Carlo
```{r monte-carlo}
library(MASS)
library(dplyr)

# Function to run Monte Carlo simulation
simulate_season <- function(df_predictions, n_simulations = 10000) {
  # Extract the variance-covariance matrix from the GAM model
  vcov_matrix <- vcov(winning_model)
  
  # Number of skiers
  n_skiers <- nrow(df_predictions)
  
  # Store winners
  winners <- numeric(n_simulations)
  
  # For each simulation
  for(i in 1:n_simulations) {
    # Generate random predictions based on the model uncertainty
    simulated_points <- rnorm(n_skiers, 
                            mean = df_predictions$Predicted_Points,
                            sd = (df_predictions$CI_Upper - df_predictions$CI_Lower) / (2 * 1.96))
    
    # Ensure points are between 0 and 1
    simulated_points <- pmax(0, pmin(1, simulated_points))
    
    # Record winner (index of maximum points)
    winners[i] <- which.max(simulated_points)
  }
  
  # Calculate probabilities
  win_probabilities <- table(winners) / n_simulations
  
  # Create results dataframe
  results <- data.frame(
    Skier = df_predictions$Skier[as.numeric(names(win_probabilities))],
    Win_Probability = as.numeric(win_probabilities)
  )
  
  # Sort by probability
  results <- results[order(-results$Win_Probability), ]
  
  return(results)
}

# Run simulation
results <- simulate_season(df_2025)

# Format results with proper handling of decimals
results <- results %>%
  mutate(
    Decimal_Odds = 1 / Win_Probability,
    American_Odds = ifelse(Win_Probability >= 0.5,
                          -Win_Probability/(1-Win_Probability) * 100,
                          (1-Win_Probability)/Win_Probability * 100)
  ) %>%
  mutate(
    Win_Probability = sprintf("%.1f%%", Win_Probability * 100),
    Decimal_Odds = round(Decimal_Odds, 2),
    American_Odds = ifelse(American_Odds > 0, 
                          sprintf("+%.0f", round(American_Odds, 0)),
                          sprintf("%.0f", round(American_Odds, 0)))
  )

# Add Nation from df_2025
results <- results %>%
  dplyr::left_join(df_2025 %>% dplyr::select(Skier, Nation), by = "Skier")

# Reorder columns to include Nation
results <- results %>%
  dplyr::select(Skier, Nation, everything())

print(results)

# Save results
write.xlsx(results, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_odds.xlsx")
```


###Bootstrap
```{r bootstrap}
library(boot)

# Function to run bootstrap simulation
bootstrap_season <- function(df_train, df_predictions, n_bootstrap = 1000) {
  
  # Bootstrap function
  boot_fn <- function(data, indices) {
    # Fit model on bootstrap sample
    boot_model <- gam(Pct_of_Max_Points ~ s(Prev_Pelo) + s(Prev_Distance) + 
                     s(Prev_Sprint_C) + s(Prev_Sprint_F), 
                     data = data[indices,])
    
    # Predict on new data
    predictions <- predict(boot_model, newdata = df_predictions)
    
    # Return winner index
    return(which.max(predictions))
  }
  
  # Run bootstrap
  boot_results <- boot(data = df_train, 
                      statistic = boot_fn, 
                      R = n_bootstrap)
  
  # Calculate win probabilities
  win_counts <- table(boot_results$t)
  win_probabilities <- win_counts / n_bootstrap
  
  # Create results dataframe
  results <- data.frame(
    Skier = df_predictions$Skier[as.numeric(names(win_probabilities))],
    Win_Probability = as.numeric(win_probabilities)
  )
  
  # Sort by probability
  results <- results[order(-results$Win_Probability), ]
  
  # Add odds calculations
  results <- results %>%
    mutate(
      Decimal_Odds = 1 / Win_Probability,
      American_Odds = ifelse(Win_Probability >= 0.5,
                            -Win_Probability/(1-Win_Probability) * 100,
                            (1-Win_Probability)/Win_Probability * 100),
      Win_Probability = sprintf("%.1f%%", Win_Probability * 100),
      Decimal_Odds = round(Decimal_Odds, 2),
      American_Odds = ifelse(American_Odds > 0, 
                            sprintf("+%d", round(American_Odds)),
                            sprintf("%d", round(American_Odds)))
    )
  
  return(results)
}

# Run bootstrap simulation
bootstrap_results <- bootstrap_season(
  df_train = df_race %>% filter(Season != 2024),
  df_predictions = df_2025
)

print(bootstrap_results)
```

Ok Clearly there are some issues here.  Klaebo does not have a 100% shot of winning the World Cup.  Maybe there is a better way to do this.  Let's try creating a variable, place which ranks the skiers for each World Cup and try different binomial or classifications to accomplish the task.

### Feature Selection
```{r odd_feat}
library(leaps)
library(caret)
library(glmnet)
library(Boruta)

# First create all outcome variables
df_place <- df_race %>%
  group_by(Season) %>%
  mutate(Place = rank(-Pct_of_Max_Points, ties.method = "min")) %>%
  ungroup() %>%
  mutate(
    Top1 = factor(ifelse(Place <= 1, "Yes", "No"), levels = c("No", "Yes")),
    TopThree = factor(ifelse(Place <= 3, "Yes", "No"), levels = c("No", "Yes")),
    Top5 = factor(ifelse(Place <= 5, "Yes", "No"), levels = c("No", "Yes")),
    Top10 = factor(ifelse(Place <= 10, "Yes", "No"), levels = c("No", "Yes")),
    Top30 = factor(ifelse(Place <= 30, "Yes", "No"), levels = c("No", "Yes"))
  )

# Define all features
features <- c("Prev_Pelo", "Prev_Distance", "Prev_Distance_C", "Prev_Distance_F",
              "Prev_Sprint", "Prev_Sprint_C", "Prev_Sprint_F", "Prev_F", "Prev_C", "Prev_Pct_of_Max_Points")

# Function to evaluate a specific feature set
evaluate_features <- function(feature_set, target_var, data) {
  # Split into training and validation
  train_data <- data %>% 
    dplyr::select(all_of(feature_set), !!sym(target_var))
  
  # Set up cross-validation
  ctrl <- trainControl(
    method = "cv",
    number = 5,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
  
  # Train model
  model <- train(
    as.formula(paste(target_var, "~.")),
    data = train_data,
    method = "glm",
    family = "binomial",
    trControl = ctrl,
    metric = "ROC"
  )
  
  return(model$results$ROC)
}

# 1. Exhaustive Search using regsubsets
exhaustive_search <- function(target_var) {
  formula_str <- as.formula(paste(target_var, "~", paste(features, collapse = " + ")))
  
  # Perform exhaustive search
  search <- regsubsets(formula_str,
                      data = df_place,
                      nvmax = length(features),
                      method = "exhaustive")
  
  # Get summary
  summary_search <- summary(search)
  
  # Create matrix of results
  results <- data.frame(
    n_vars = 1:length(features),
    adjr2 = summary_search$adjr2,
    cp = summary_search$cp,
    bic = summary_search$bic
  )
  
  # Get variable combinations
  var_combinations <- summary_search$which[,-1]  # Remove intercept column
  colnames(var_combinations) <- features
  
  # Combine with results
  results <- cbind(results, var_combinations)
  
  return(results)
}

# 2. LASSO Feature Selection
lasso_selection <- function(target_var) {
  # Convert factor to numeric for LASSO
  y <- as.numeric(df_place[[target_var]]) - 1
  
  # Prepare matrix of predictors
  x <- as.matrix(df_place[, features])
  
  # Fit LASSO
  cv_lasso <- cv.glmnet(x, y, family="binomial", alpha=1)
  
  # Get coefficients at optimal lambda
  coef_lasso <- coef(cv_lasso, s="lambda.min")
  
  # Create importance data frame
  importance <- data.frame(
    Feature = rownames(coef_lasso),
    Coefficient = as.vector(coef_lasso)
  ) %>%
    filter(Feature != "(Intercept)") %>%
    arrange(desc(abs(Coefficient)))
  
  return(importance)
}

# 3. Boruta Feature Selection
boruta_selection <- function(target_var) {
  # Prepare formula
  formula_str <- as.formula(paste(target_var, "~", paste(features, collapse = " + ")))
  
  # Run Boruta
  boruta_output <- Boruta(formula_str, data = df_place, doTrace = 2)
  
  # Get results
  boruta_results <- attStats(boruta_output)
  
  return(boruta_results)
}

# 4. Correlation Analysis
correlation_analysis <- function() {
  # Create correlation matrix
  cor_matrix <- cor(df_place[, features])
  
  # Convert to long format for easier filtering
  cor_long <- reshape2::melt(cor_matrix) %>%
    filter(abs(value) > 0.7 & Var1 != Var2)  # Filter high correlations
  
  return(list(
    matrix = cor_matrix,
    high_correlations = cor_long
  ))
}

# Run all selection methods for each target
targets <- c("Top1", "TopThree", "Top5", "Top10", "Top30")
selection_results <- list()

# First, check for multicollinearity
print("Checking feature correlations...")
cor_results <- correlation_analysis()
print("Highly correlated features (|r| > 0.7):")
print(cor_results$high_correlations)

for(target in targets) {
  cat("\nProcessing target:", target, "\n")
  selection_results[[target]] <- list(
    exhaustive = exhaustive_search(target),
    lasso = lasso_selection(target),
    boruta = boruta_selection(target)
  )
}

# Function to evaluate top feature combinations
evaluate_top_combinations <- function(target) {
  # Get top features from each method
  lasso_top <- selection_results[[target]]$lasso$Feature[1:5]  # Top 5 from LASSO
  boruta_top <- rownames(selection_results[[target]]$boruta)[selection_results[[target]]$boruta$decision == "Confirmed"]
  
  # Combine unique features
  top_features <- unique(c(lasso_top, boruta_top))
  
  # Generate combinations of these top features
  feature_combinations <- list()
  for(i in 2:min(5, length(top_features))) {  # Test combinations of 2-5 features
    combs <- combn(top_features, i, simplify = FALSE)
    feature_combinations <- c(feature_combinations, combs)
  }
  
  # Evaluate each combination
  results <- data.frame(
    Features = sapply(feature_combinations, paste, collapse = ", "),
    ROC = sapply(feature_combinations, function(x) evaluate_features(x, target, df_place))
  )
  
  # Sort by ROC
  results <- results[order(-results$ROC), ]
  return(results)
}

# Evaluate combinations for each target
evaluation_results <- lapply(targets, evaluate_top_combinations)
names(evaluation_results) <- targets

# Create results summary
results_summary <- list()
for(target in targets) {
  # Get top features from each method
  lasso_features <- selection_results[[target]]$lasso$Feature[1:5]
  boruta_features <- rownames(selection_results[[target]]$boruta)[selection_results[[target]]$boruta$decision == "Confirmed"]
  best_combination <- evaluation_results[[target]]$Features[1]
  
  results_summary[[target]] <- list(
    LASSO_top5 = lasso_features,
    Boruta_confirmed = boruta_features,
    Best_combination = best_combination,
    Best_ROC = evaluation_results[[target]]$ROC[1]
  )
}

# Print results
for(target in targets) {
  cat("\n=== Results for", target, "===\n")
  cat("\nBest feature combination:", results_summary[[target]]$Best_combination)
  cat("\nROC Score:", results_summary[[target]]$Best_ROC)
  cat("\n\nLASSO top 5 features:", paste(results_summary[[target]]$LASSO_top5, collapse = ", "))
  cat("\nBoruta confirmed features:", paste(results_summary[[target]]$Boruta_confirmed, collapse = ", "))
  cat("\n------------------------\n")
}

# Save detailed results to Excel
results_for_excel <- list(
  Correlations = cor_results$high_correlations)

for(target in targets) {
  results_for_excel[[paste0(target, "_evaluated_combinations")]] <- evaluation_results[[target]]
  results_for_excel[[paste0(target, "_lasso")]] <- selection_results[[target]]$lasso
  results_for_excel[[paste0(target, "_boruta")]] <- as.data.frame(selection_results[[target]]$boruta)
  results_for_excel[[paste0(target, "_best_combination")]] <- data.frame(
    Features = results_summary[[target]]$Best_combination,
    ROC = results_summary[[target]]$Best_ROC
  )
}

write.xlsx(results_for_excel, 
           file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/feature-selection-results-complete.xlsx",
           rowNames = TRUE)

# Create visualization of feature importance across methods
library(ggplot2)

# Create importance plot for each target
for(target in targets) {
  # Combine importance scores
  importance_scores <- data.frame(
    Feature = features,
    LASSO = abs(selection_results[[target]]$lasso$Coefficient[match(features, selection_results[[target]]$lasso$Feature)]),
    Boruta = selection_results[[target]]$boruta$meanImp[match(features, rownames(selection_results[[target]]$boruta))]
  )
  
  # Create plot
  p <- ggplot(reshape2::melt(importance_scores, id.vars = "Feature"), 
              aes(x = Feature, y = value, fill = variable)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~variable, scales = "free_y") +
    theme_minimal() +
    labs(title = paste("Feature Importance for", target),
         x = "Features",
         y = "Importance Score") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
}
```

``` {r non-ml-feat}
library(leaps)
library(caret)

# Define all features
features <- c("Prev_Pelo", "Prev_Distance", "Prev_Distance_C", "Prev_Distance_F", 
              "Prev_Sprint", "Prev_Sprint_C", "Prev_Sprint_F", "Prev_F", "Prev_C", "Prev_Pct_of_Max_Points")


# Function to evaluate ordinal logistic model (for TopThree)
evaluate_polr <- function(feature_set, data) {
  tryCatch({
    formula_str <- paste("TopThree ~", paste(feature_set, collapse = " + "))
    print(formula_str)  # Debug print
    formula <- as.formula(formula_str)
    model <- polr(formula, data = data)
    return(AIC(model))
  }, error = function(e) {
    print(paste("Error:", e$message))  # Debug print
    return(Inf)
  })
}

# Function to evaluate binary logistic model (for Top5, Top10, Top30)
evaluate_glm <- function(feature_set, data, target) {
  tryCatch({
    formula_str <- as.formula(paste(target, "~", paste(feature_set, collapse = " + ")))
    model <- glm(formula_str, family = binomial, data = data)
    # Use AIC for model comparison
    return(AIC(model))
  }, error = function(e) {
    return(Inf)
  })
}

# And modify exhaustive_feature_search to use evaluate_glm for all targets
exhaustive_feature_search <- function(target) {
  best_aic <- Inf
  best_features <- NULL
  
  for(i in 2:5) {
    combinations <- combn(features, i, simplify = FALSE)
    for(feature_set in combinations) {
      aic <- evaluate_glm(feature_set, df_place, target)
      if(aic < best_aic) {
        best_aic <- aic
        best_features <- feature_set
      }
    }
  }
  return(list(features = best_features, aic = best_aic))
}

# Perform search for each target
targets <- c("TopThree", "Top5", "Top10", "Top30")
best_features <- list()

cat("\nPerforming exhaustive feature search...\n")

for(target in targets) {
  cat("\nSearching for best features for", target, "...\n")
  result <- exhaustive_feature_search(target)
  best_features[[target]] <- result
  cat("Best features:", paste(result$features, collapse = ", "), "\n")
  cat("AIC:", result$aic, "\n")
}

# Function to evaluate model performance using cross-validation
# Modify evaluate_model_cv function
evaluate_model_cv <- function(features, target, data) {
  formula_str <- as.formula(paste(target, "~", paste(features, collapse = " + ")))
  
  # Set up cross-validation 
  folds <- createFolds(data[[target]], k = 5)
  aucs <- sapply(folds, function(test_idx) {
    train <- data[-test_idx, ]
    test <- data[test_idx, ]
    
    model <- glm(formula_str, family = binomial, data = train)
    pred <- predict(model, newdata = test, type = "response")
    return(pROC::auc(as.numeric(test[[target]]), pred))
  })
  
  return(mean(aucs))
}

# Evaluate performance of best feature sets
performance_results <- list()

for(target in targets) {
  features <- best_features[[target]]$features
  perf <- evaluate_model_cv(features, target, df_place)
  performance_results[[target]] <- perf
}

# Create summary dataframe
summary_df <- data.frame(
  Target = targets,
  Features = sapply(best_features, function(x) paste(x$features, collapse = ", ")),
  AIC = sapply(best_features, function(x) x$aic),
  CV_Performance = unlist(performance_results)
)

# Print results
cat("\nBest Feature Sets and Performance:\n")
print(summary_df)

# Save results
write.xlsx(list(
  Summary = summary_df,
  FeatureSets = do.call(rbind, lapply(best_features, function(x) 
    data.frame(Features = paste(x$features, collapse = ", "), AIC = x$aic)))
), 
file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/feature-selection-logistic.xlsx")

# Function to compare model performance with different feature sets
compare_models <- function(target, features_old, features_new) {
  # Old model
  model_old <- glm(as.formula(paste(target, "~", paste(features_old, collapse = " + "))), 
                   family = binomial, data = df_place)
  
  # New model
  model_new <- glm(as.formula(paste(target, "~", paste(features_new, collapse = " + "))), 
                   family = binomial, data = df_place)
  
  return(list(
    old_aic = AIC(model_old),
    new_aic = AIC(model_new),
    aic_improvement = AIC(model_old) - AIC(model_new)
  ))
}

# Compare current features with newly found best features
current_features <- c("Prev_Pelo", "Prev_Distance", "Prev_Sprint_C", "Prev_Sprint_F")
comparisons <- list()

for(target in targets) {
  new_features <- best_features[[target]]$features
  comparison <- compare_models(target, current_features, new_features)
  comparisons[[target]] <- comparison
}

# Print comparison results
cat("\nModel Comparisons (Current vs New Features):\n")
for(target in targets) {
  cat("\n", target, ":\n")
  cat("Current features AIC:", comparisons[[target]]$old_aic, "\n")
  cat("New features AIC:", comparisons[[target]]$new_aic, "\n")
  cat("AIC improvement:", comparisons[[target]]$aic_improvement, "\n")
}

# Save comparison results
comparison_df <- do.call(rbind, lapply(names(comparisons), function(target) {
  data.frame(
    Target = target,
    Current_AIC = comparisons[[target]]$old_aic,
    New_AIC = comparisons[[target]]$new_aic,
    Improvement = comparisons[[target]]$aic_improvement
  )
}))

write.xlsx(comparison_df, 
           file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_model-comparisons.xlsx")
```

###Setup

#### Non-ML -- Old selection
```{r odds-setup}
# First let's add the Place column based on rankings within each season
df_place <- df_race %>%
  group_by(Season) %>%
  mutate(Place = rank(-Pct_of_Max_Points, ties.method = "min")) %>%
  ungroup()

# Create categorical outcomes for different cutoffs
df_place <- df_place %>%
  mutate(
    TopThree = factor(ifelse(Place <= 3, Place, 4)),
    Top5 = factor(ifelse(Place <= 5, 1, 0)),
    Top10 = factor(ifelse(Place <= 10, 1, 0)),
    Top30 = factor(ifelse(Place <= 30, 1, 0))
  )

# Prepare 2025 prediction data with current (2024) values
pred_data <- df_2025 %>%
  dplyr::select(Skier, Nation, 
                Pelo, Distance_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
                Sprint_Pelo, Classic_Pelo, Freestyle_Pelo,
                Pct_of_Max_Points) %>%
  rename(
    Prev_Pelo = Pelo,
    Prev_Distance = Distance_Pelo,
    Prev_Sprint_C = Sprint_C_Pelo,
    Prev_Sprint_F = Sprint_F_Pelo,
    Prev_Sprint = Sprint_Pelo,
    Prev_C = Classic_Pelo,
    Prev_F = Freestyle_Pelo,
    Prev_Pct_of_Max_Points = Pct_of_Max_Points
  )



# Fit models for each cutoff (now including 2024 season)
place_model <- polr(TopThree ~ Prev_Pct_of_Max_Points+ Prev_Distance + Prev_Sprint,
                   data = df_place)  # Removed the Season filter

top5_model <- glm(Top5 ~ Prev_Pct_of_Max_Points+ Prev_Distance + Prev_Sprint,
                 family = binomial,
                 data = df_place)  # Removed the Season filter

top10_model <- glm(Top10 ~ Prev_Pct_of_Max_Points+ Prev_Distance + Prev_Sprint,
                  family = binomial,
                  data = df_place)  # Removed the Season filter

top30_model <- glm(Top30 ~ Prev_Pct_of_Max_Points+ Prev_Distance + Prev_Sprint,
                  family = binomial,
                  data = df_place)  # Removed the Season filter

# Get predicted probabilities for 2025 skiers using 2024 end-of-season values
pred_probs <- predict(place_model, pred_data, type = "probs")
top5_probs <- predict(top5_model, pred_data, type = "response")
top10_probs <- predict(top10_model, pred_data, type = "response")
top30_probs <- predict(top30_model, pred_data, type = "response")

# Create results dataframe with all probabilities
results <- data.frame(
  Skier = df_2025$Skier,
  Nation = df_2025$Nation,
  Win_Prob = pred_probs[,1],
  Second_Prob = pred_probs[,2],
  Third_Prob = pred_probs[,3],
  Top3_Prob = pred_probs[,1] + pred_probs[,2] + pred_probs[,3],
  Top5_Prob = top5_probs,
  Top10_Prob = top10_probs,
  Top30_Prob = top30_probs,
  Outside_Prob = pred_probs[,4]
)

# Calculate decimal and American odds for all probabilities
results <- results %>%
  mutate(
    Win_Decimal_Odds = 1 / Win_Prob,
    Win_American_Odds = ifelse(Win_Prob >= 0.5,
                              -Win_Prob/(1-Win_Prob) * 100,
                              (1-Win_Prob)/Win_Prob * 100),
    Top3_Decimal_Odds = 1 / Top3_Prob,
    Top3_American_Odds = ifelse(Top3_Prob >= 0.5,
                               -Top3_Prob/(1-Top3_Prob) * 100,
                               (1-Top3_Prob)/Top3_Prob * 100),
    Top5_Decimal_Odds = 1 / Top5_Prob,
    Top5_American_Odds = ifelse(Top5_Prob >= 0.5,
                               -Top5_Prob/(1-Top5_Prob) * 100,
                               (1-Top5_Prob)/Top5_Prob * 100),
    Top10_Decimal_Odds = 1 / Top10_Prob,
    Top10_American_Odds = ifelse(Top10_Prob >= 0.5,
                                -Top10_Prob/(1-Top10_Prob) * 100,
                                (1-Top10_Prob)/Top10_Prob * 100),
    Top30_Decimal_Odds = 1 / Top30_Prob,
    Top30_American_Odds = ifelse(Top30_Prob >= 0.5,
                                -Top30_Prob/(1-Top30_Prob) * 100,
                                (1-Top30_Prob)/Top30_Prob * 100)
  ) %>%
  mutate(
    Win_Prob = sprintf("%.1f%%", Win_Prob * 100),
    Second_Prob = sprintf("%.1f%%", Second_Prob * 100),
    Third_Prob = sprintf("%.1f%%", Third_Prob * 100),
    Top3_Prob = sprintf("%.1f%%", Top3_Prob * 100),
    Top5_Prob = sprintf("%.1f%%", Top5_Prob * 100),
    Top10_Prob = sprintf("%.1f%%", Top10_Prob * 100),
    Top30_Prob = sprintf("%.1f%%", Top30_Prob * 100),
    Outside_Prob = sprintf("%.1f%%", Outside_Prob * 100),
    Win_Decimal_Odds = round(Win_Decimal_Odds, 2),
    Top3_Decimal_Odds = round(Top3_Decimal_Odds, 2),
    Top5_Decimal_Odds = round(Top5_Decimal_Odds, 2),
    Top10_Decimal_Odds = round(Top10_Decimal_Odds, 2),
    Top30_Decimal_Odds = round(Top30_Decimal_Odds, 2),
    Win_American_Odds = ifelse(Win_American_Odds > 0, 
                              sprintf("+%.0f", round(Win_American_Odds, 0)),
                              sprintf("%.0f", round(Win_American_Odds, 0))),
    Top3_American_Odds = ifelse(Top3_American_Odds > 0, 
                               sprintf("+%.0f", round(Top3_American_Odds, 0)),
                               sprintf("%.0f", round(Top3_American_Odds, 0))),
    Top5_American_Odds = ifelse(Top5_American_Odds > 0, 
                               sprintf("+%.0f", round(Top5_American_Odds, 0)),
                               sprintf("%.0f", round(Top5_American_Odds, 0))),
    Top10_American_Odds = ifelse(Top10_American_Odds > 0, 
                                sprintf("+%.0f", round(Top10_American_Odds, 0)),
                                sprintf("%.0f", round(Top10_American_Odds, 0))),
    Top30_American_Odds = ifelse(Top30_American_Odds > 0, 
                                sprintf("+%.0f", round(Top30_American_Odds, 0)),
                                sprintf("%.0f", round(Top30_American_Odds, 0)))
  ) %>%
  arrange(desc(as.numeric(sub("%", "", Win_Prob))))

# Print top 10 most likely winners
print("\nTop 10 Predicted World Cup Contenders:")
print(head(results, 10))

# Print model summaries to see the impact of including 2024
print("\nModel Summaries:")
print(summary(place_model))
print(summary(top5_model))
print(summary(top10_model))
print(summary(top30_model))

results$Outside_Prob <- NULL
names(results) <- gsub("_", " ", names(results))
results
# Save results
write.xlsx(results, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_placement-odds.xlsx")
```




#### Odds testing
```{r old-school-odds-test}
library(tidyverse)
library(MASS)
library(Metrics)

calculate_brier_score <- function(actual, predicted) {
  # Convert factors to numeric
  actual <- as.numeric(as.character(actual))
  predicted <- as.numeric(predicted)
  mean((actual - predicted)^2)
}

calculate_log_loss <- function(actual, predicted) {
  # Convert factors to numeric
  actual <- as.numeric(as.character(actual))
  predicted <- as.numeric(predicted)
  # Clip probabilities
  predicted <- pmax(pmin(predicted, 0.999), 0.001)
  -mean(actual * log(predicted) + (1 - actual) * log(1 - predicted))
}

evaluate_season_predictions <- function(df, season) {
  # Ensure factor columns are converted to numeric
  train_data <- df_place %>% 
    filter(Season != season) %>%
    mutate(across(c(TopThree, Top5, Top10, Top30), ~as.numeric(as.character(.))))
    
  test_data <- df_place %>% 
    filter(Season == season) %>%
    mutate(across(c(TopThree, Top5, Top10, Top30), ~as.numeric(as.character(.))))

  # Fit models using training data
  place_model <- polr(factor(TopThree) ~ Prev_Pct_of_Max_Points + Prev_Distance + Prev_Sprint,
                     data = train_data)
  top5_model <- glm(Top5 ~ Prev_Pct_of_Max_Points + Prev_Distance + Prev_Sprint,
                    family = binomial, data = train_data)
  top10_model <- glm(Top10 ~ Prev_Pct_of_Max_Points + Prev_Distance + Prev_Sprint,
                     family = binomial, data = train_data)
  top30_model <- glm(Top30 ~ Prev_Pct_of_Max_Points + Prev_Distance + Prev_Sprint,
                     family = binomial, data = train_data)

  # Make predictions
  test_data$pred_top3 <- predict(place_model, newdata = test_data, type = "probs")[,1]
  test_data$pred_top5 <- predict(top5_model, newdata = test_data, type = "response")
  test_data$pred_top10 <- predict(top10_model, newdata = test_data, type = "response")
  test_data$pred_top30 <- predict(top30_model, newdata = test_data, type = "response")

  # Calculate metrics
  metrics <- list(
    season = season,
    n_predictions = nrow(test_data),
    
    brier_top3 = calculate_brier_score(test_data$TopThree, test_data$pred_top3),
    brier_top5 = calculate_brier_score(test_data$Top5, test_data$pred_top5),
    brier_top10 = calculate_brier_score(test_data$Top10, test_data$pred_top10),
    brier_top30 = calculate_brier_score(test_data$Top30, test_data$pred_top30),
    
    log_loss_top3 = calculate_log_loss(test_data$TopThree, test_data$pred_top3),
    log_loss_top5 = calculate_log_loss(test_data$Top5, test_data$pred_top5),
    log_loss_top10 = calculate_log_loss(test_data$Top10, test_data$pred_top10),
    log_loss_top30 = calculate_log_loss(test_data$Top30, test_data$pred_top30)
  )

  create_reliability_data <- function(actual, predicted, n_bins = 10) {
    # Convert factors to numeric
    actual <- as.numeric(as.character(actual))
    predicted <- as.numeric(predicted)
    
    bins <- cut(predicted, breaks = seq(0, 1, length.out = n_bins + 1), 
                labels = FALSE, include.lowest = TRUE)
    
    reliability_df <- data.frame(
      bin = bins,
      actual = actual,
      predicted = predicted
    ) %>%
      group_by(bin) %>%
      summarize(
        observed_freq = mean(actual),
        predicted_prob = mean(predicted),
        n_samples = n(),
        .groups = 'drop'  # Added to avoid grouping warning
      )
    
    return(reliability_df)
  }

  metrics$reliability <- list(
    top3 = create_reliability_data(test_data$TopThree, test_data$pred_top3),
    top5 = create_reliability_data(test_data$Top5, test_data$pred_top5),
    top10 = create_reliability_data(test_data$Top10, test_data$pred_top10),
    top30 = create_reliability_data(test_data$Top30, test_data$pred_top30)
  )

  return(metrics)
}

# Rest of the code remains the same...

# Evaluate all seasons
evaluate_all_seasons <- function(df) {
  seasons <- 2019:2024
  results <- map(seasons, ~evaluate_season_predictions(df, .x))
  names(results) <- paste0("Season_", seasons)
  return(results)
}

# Plot reliability diagram for a specific prediction type
plot_reliability_diagram <- function(reliability_data, title) {
  ggplot(reliability_data) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
    geom_point(aes(x = predicted_prob, y = observed_freq, size = n_samples)) +
    geom_line(aes(x = predicted_prob, y = observed_freq)) +
    scale_size_continuous(range = c(2, 10)) +
    labs(
      title = title,
      x = "Predicted Probability",
      y = "Observed Frequency",
      size = "Number of\nPredictions"
    ) +
    theme_minimal() +
    coord_equal() +
    ylim(0, 1) +
    xlim(0, 1)
}


evaluation_results <- evaluate_all_seasons(df_place)

# Print summary metrics for each season
summary_df <- bind_rows(
  lapply(names(evaluation_results), function(season) {
    data.frame(
      Season = season,
      N = evaluation_results[[season]]$n_predictions,
      Brier_Top3 = evaluation_results[[season]]$brier_top3,
      Brier_Top5 = evaluation_results[[season]]$brier_top5,
      Brier_Top10 = evaluation_results[[season]]$brier_top10,
      Brier_Top30 = evaluation_results[[season]]$brier_top30
    )
  })
)
print(summary_df)
# Plot reliability diagram for Top 3 predictions for a specific season
plot_reliability_diagram(
  evaluation_results$Season_2021$reliability$top3,
  "Top 3 Prediction Reliability - 2023 Season"
)
```


#### Non-ML New Selection
```{r new-sel-non-ml}
# First let's add the Place column based on rankings within each season
df_place <- df_race %>%
  group_by(Season) %>%
  mutate(Place = rank(-Pct_of_Max_Points, ties.method = "min")) %>%
  ungroup()

# Create categorical outcomes for different cutoffs
df_place <- df_place %>%
  mutate(
    TopThree = factor(ifelse(Place <= 3, Place, 4)),
    Top5 = factor(ifelse(Place <= 5, 1, 0)),
    Top10 = factor(ifelse(Place <= 10, 1, 0)),
    Top30 = factor(ifelse(Place <= 30, 1, 0))
  )

# Use best features for each model (replace these with your actual best features)
topthree_features <- best_features[["TopThree"]]$features
top5_features <- best_features[["Top5"]]$features
top10_features <- best_features[["Top10"]]$features
top30_features <- best_features[["Top30"]]$features

# Create formulas for each model
topthree_formula <- as.formula(paste("TopThree ~", paste(topthree_features, collapse = " + ")))
top5_formula <- as.formula(paste("Top5 ~", paste(top5_features, collapse = " + ")))
top10_formula <- as.formula(paste("Top10 ~", paste(top10_features, collapse = " + ")))
top30_formula <- as.formula(paste("Top30 ~", paste(top30_features, collapse = " + ")))

# Prepare 2025 prediction data with current (2024) values
pred_data <- df_2025 %>%
  dplyr::select(Skier, Nation, 
                Pelo, Distance_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
                Sprint_Pelo, Classic_Pelo, Freestyle_Pelo, Pct_of_Max_Points) %>%
  rename(
    Prev_Pelo = Pelo,
    Prev_Distance = Distance_Pelo,
    Prev_Sprint_C = Sprint_C_Pelo,
    Prev_Sprint_F = Sprint_F_Pelo,
    Prev_Sprint = Sprint_Pelo,
    Prev_C = Classic_Pelo,
    Prev_F = Freestyle_Pelo,
    Prev_Pct_of_Max_Points = Pct_of_Max_Points
  )

# Add Distance_C and Distance_F if they're not already there
if(!"Prev_Distance_C" %in% names(pred_data)) {
  pred_data <- pred_data %>%
    mutate(Prev_Distance_C = Prev_Distance)
}

if(!"Prev_Distance_F" %in% names(pred_data)) {
  pred_data <- pred_data %>%
    mutate(Prev_Distance_F = Prev_Distance)
}

# Fit models with optimal feature sets
place_model <- polr(topthree_formula, data = df_place)
top5_model <- glm(top5_formula, family = binomial, data = df_place)
top10_model <- glm(top10_formula, family = binomial, data = df_place)
top30_model <- glm(top30_formula, family = binomial, data = df_place)

# Get predicted probabilities for 2025 skiers
pred_probs <- predict(place_model, pred_data, type = "probs")
top5_probs <- predict(top5_model, pred_data, type = "response")
top10_probs <- predict(top10_model, pred_data, type = "response")
top30_probs <- predict(top30_model, pred_data, type = "response")

# Create results dataframe with all probabilities
results <- data.frame(
  Skier = pred_data$Skier,
  Nation = pred_data$Nation,
  Win_Prob = pred_probs[,1],
  Second_Prob = pred_probs[,2],
  Third_Prob = pred_probs[,3],
  Top3_Prob = pred_probs[,1] + pred_probs[,2] + pred_probs[,3],
  Top5_Prob = top5_probs,
  Top10_Prob = top10_probs,
  Top30_Prob = top30_probs,
  Outside_Prob = pred_probs[,4]
)

# Calculate decimal and American odds for all probabilities
results <- results %>%
  mutate(
    Win_Decimal_Odds = 1 / Win_Prob,
    Win_American_Odds = ifelse(Win_Prob >= 0.5,
                              -Win_Prob/(1-Win_Prob) * 100,
                              (1-Win_Prob)/Win_Prob * 100),
    Top3_Decimal_Odds = 1 / Top3_Prob,
    Top3_American_Odds = ifelse(Top3_Prob >= 0.5,
                               -Top3_Prob/(1-Top3_Prob) * 100,
                               (1-Top3_Prob)/Top3_Prob * 100),
    Top5_Decimal_Odds = 1 / Top5_Prob,
    Top5_American_Odds = ifelse(Top5_Prob >= 0.5,
                               -Top5_Prob/(1-Top5_Prob) * 100,
                               (1-Top5_Prob)/Top5_Prob * 100),
    Top10_Decimal_Odds = 1 / Top10_Prob,
    Top10_American_Odds = ifelse(Top10_Prob >= 0.5,
                                -Top10_Prob/(1-Top10_Prob) * 100,
                                (1-Top10_Prob)/Top10_Prob * 100),
    Top30_Decimal_Odds = 1 / Top30_Prob,
    Top30_American_Odds = ifelse(Top30_Prob >= 0.5,
                                -Top30_Prob/(1-Top30_Prob) * 100,
                                (1-Top30_Prob)/Top30_Prob * 100)
  ) %>%
  mutate(
    Win_Prob = sprintf("%.1f%%", Win_Prob * 100),
    Second_Prob = sprintf("%.1f%%", Second_Prob * 100),
    Third_Prob = sprintf("%.1f%%", Third_Prob * 100),
    Top3_Prob = sprintf("%.1f%%", Top3_Prob * 100),
    Top5_Prob = sprintf("%.1f%%", Top5_Prob * 100),
    Top10_Prob = sprintf("%.1f%%", Top10_Prob * 100),
    Top30_Prob = sprintf("%.1f%%", Top30_Prob * 100),
    Outside_Prob = sprintf("%.1f%%", Outside_Prob * 100),
    Win_Decimal_Odds = round(Win_Decimal_Odds, 2),
    Top3_Decimal_Odds = round(Top3_Decimal_Odds, 2),
    Top5_Decimal_Odds = round(Top5_Decimal_Odds, 2),
    Top10_Decimal_Odds = round(Top10_Decimal_Odds, 2),
    Top30_Decimal_Odds = round(Top30_Decimal_Odds, 2),
    Win_American_Odds = ifelse(Win_American_Odds > 0, 
                              sprintf("+%.0f", round(Win_American_Odds, 0)),
                              sprintf("%.0f", round(Win_American_Odds, 0))),
    Top3_American_Odds = ifelse(Top3_American_Odds > 0, 
                               sprintf("+%.0f", round(Top3_American_Odds, 0)),
                               sprintf("%.0f", round(Top3_American_Odds, 0))),
    Top5_American_Odds = ifelse(Top5_American_Odds > 0, 
                               sprintf("+%.0f", round(Top5_American_Odds, 0)),
                               sprintf("%.0f", round(Top5_American_Odds, 0))),
    Top10_American_Odds = ifelse(Top10_American_Odds > 0, 
                                sprintf("+%.0f", round(Top10_American_Odds, 0)),
                                sprintf("%.0f", round(Top10_American_Odds, 0))),
    Top30_American_Odds = ifelse(Top30_American_Odds > 0, 
                                sprintf("+%.0f", round(Top30_American_Odds, 0)),
                                sprintf("%.0f", round(Top30_American_Odds, 0)))
  ) %>%
  arrange(desc(as.numeric(sub("%", "", Win_Prob))))

# Add feature information to results
features_used <- data.frame(
  Model = c("TopThree", "Top5", "Top10", "Top30"),
  Features = c(
    paste(topthree_features, collapse = ", "),
    paste(top5_features, collapse = ", "),
    paste(top10_features, collapse = ", "),
    paste(top30_features, collapse = ", ")
  )
)

# Print top 10 most likely winners
print("\nTop 10 Predicted World Cup Contenders:")
print(head(results, 10))

# Print model summaries and feature sets
print("\nFeature Sets Used:")
print(features_used)

print("\nModel Summaries:")
print(summary(place_model))
print(summary(top5_model))
print(summary(top10_model))
print(summary(top30_model))

# Save results with feature information
write.xlsx(list(
  Predictions = results,
  Features_Used = features_used,
  ModelSummaries = data.frame(
    Model = c("TopThree", "Top5", "Top10", "Top30"),
    AIC = c(AIC(place_model), AIC(top5_model), AIC(top10_model), AIC(top30_model))
  )
), 
file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_placement-odds-optimized.xlsx",
sheetName = c("Predictions", "Features", "Model_Info"))
```

#### ML

```{r ml-odds-setup}
library(caret)
library(ranger) 
library(xgboost)
library(kernlab) 
library(nnet)
library(doParallel)

# Set up parallel processing
registerDoParallel(cores = detectCores() - 1)

# First let's add the Place column based on rankings within each season
df_place <- df_race %>%
  group_by(Season) %>%
  mutate(Place = rank(-Pct_of_Max_Points, ties.method = "min")) %>%
  ungroup()

# Create binary outcomes for different cutoffs with proper factor levels
df_place <- df_place %>%
  mutate(
    Top1 = factor(ifelse(Place <= 1, "Yes", "No"), levels = c("No", "Yes")),
    TopThree = factor(ifelse(Place <= 3, "Yes", "No"), levels = c("No", "Yes")),
    Top5 = factor(ifelse(Place <= 5, "Yes", "No"), levels = c("No", "Yes")),
    Top10 = factor(ifelse(Place <= 10, "Yes", "No"), levels = c("No", "Yes")),
    Top30 = factor(ifelse(Place <= 30, "Yes", "No"), levels = c("No", "Yes"))
  )

# Prepare 2025 prediction data with current (2024) values
pred_data <- df_2025 %>%
  dplyr::select(Skier, Nation, 
                Pelo, Distance_Pelo, Sprint_C_Pelo, Sprint_F_Pelo)

# Rename columns in prediction data to match training data
names(pred_data)[names(pred_data) == "Pelo"] <- "Prev_Pelo"
names(pred_data)[names(pred_data) == "Distance_Pelo"] <- "Prev_Distance"
names(pred_data)[names(pred_data) == "Sprint_C_Pelo"] <- "Prev_Sprint_C"
names(pred_data)[names(pred_data) == "Sprint_F_Pelo"] <- "Prev_Sprint_F"

# Function to train and evaluate models
train_models <- function(data, target, test_data) {
  # Prepare training data
  train_data <- data %>% 
    dplyr::select(Prev_Pelo, Prev_Distance, Prev_Sprint_C, Prev_Sprint_F, !!sym(target))
  
  # Set up cross-validation
  ctrl <- trainControl(
    method = "cv",
    number = 5,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
  
  # Train models
  set.seed(123)
  
  # Logistic Regression (as baseline)
  logit_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "glm",
    family = "binomial",
    trControl = ctrl,
    metric = "ROC"
  )
  
  # Random Forest with importance
  rf_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "ranger",
    trControl = ctrl,
    metric = "ROC",
    importance = 'impurity',
    tuneGrid = expand.grid(
      mtry = c(2, 3, 4),
      splitrule = "gini",
      min.node.size = c(5, 10, 20)
    )
  )
  
  # XGBoost
  xgb_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "xgbTree",
    trControl = ctrl,
    metric = "ROC",
    tuneLength = 5
  )
  
  # Support Vector Machine
  svm_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "svmRadial",
    trControl = ctrl,
    metric = "ROC",
    tuneLength = 5
  )
  
  # Neural Network
  nn_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "nnet",
    trControl = ctrl,
    metric = "ROC",
    tuneLength = 5,
    trace = FALSE,
    maxit = 1000
  )
  
  # Make predictions on test data
  pred_logit <- predict(logit_model, test_data, type = "prob")[,"Yes"]
  pred_rf <- predict(rf_model, test_data, type = "prob")[,"Yes"]
  pred_xgb <- predict(xgb_model, test_data, type = "prob")[,"Yes"]
  pred_svm <- predict(svm_model, test_data, type = "prob")[,"Yes"]
  pred_nn <- predict(nn_model, test_data, type = "prob")[,"Yes"]
  
  # Ensemble prediction (average of all models)
  #pred_ensemble <- (pred_logit + pred_rf + pred_xgb + pred_svm + pred_nn) / 5
  pred_ensemble <- pred_logit
  
  # Return all predictions
  list(
    logit = pred_logit,
    rf = pred_rf,
    xgb = pred_xgb,
    svm = pred_svm,
    nn = pred_nn,
    ensemble = pred_ensemble,
    models = list(
      logit = logit_model,
      rf = rf_model,
      xgb = xgb_model,
      svm = svm_model,
      nn = nn_model
    )
  )
}

# Train models for each threshold
top1_models <- train_models(df_place, "Top1", pred_data)
top3_models <- train_models(df_place, "TopThree", pred_data)
top5_models <- train_models(df_place, "Top5", pred_data)
top10_models <- train_models(df_place, "Top10", pred_data)
top30_models <- train_models(df_place, "Top30", pred_data)

# Create results dataframe using ensemble predictions
results <- data.frame(
  Skier = pred_data$Skier,
  Nation = pred_data$Nation,
  Top1_Prob = top1_models$ensemble,
  Top3_Prob = top3_models$ensemble,
  Top5_Prob = top5_models$ensemble,
  Top10_Prob = top10_models$ensemble,
  Top30_Prob = top30_models$ensemble
)

# Calculate odds
results <- results %>%
  mutate(
    Top1_Decimal_Odds = 1 / Top1_Prob,
    Top1_American_Odds = ifelse(Top1_Prob >= 0.5,
                               -Top1_Prob/(1-Top1_Prob) * 100,
                               (1-Top1_Prob)/Top1_Prob * 100),
    Top3_Decimal_Odds = 1 / Top3_Prob,
    Top3_American_Odds = ifelse(Top3_Prob >= 0.5,
                               -Top3_Prob/(1-Top3_Prob) * 100,
                               (1-Top3_Prob)/Top3_Prob * 100),
    Top5_Decimal_Odds = 1 / Top5_Prob,
    Top5_American_Odds = ifelse(Top5_Prob >= 0.5,
                               -Top5_Prob/(1-Top5_Prob) * 100,
                               (1-Top5_Prob)/Top5_Prob * 100),
    Top10_Decimal_Odds = 1 / Top10_Prob,
    Top10_American_Odds = ifelse(Top10_Prob >= 0.5,
                                -Top10_Prob/(1-Top10_Prob) * 100,
                                (1-Top10_Prob)/Top10_Prob * 100),
    Top30_Decimal_Odds = 1 / Top30_Prob,
    Top30_American_Odds = ifelse(Top30_Prob >= 0.5,
                                -Top30_Prob/(1-Top30_Prob) * 100,
                                (1-Top30_Prob)/Top30_Prob * 100)
  ) %>%
  mutate(
    Top1_Prob = sprintf("%.1f%%", Top1_Prob * 100),
    Top3_Prob = sprintf("%.1f%%", Top3_Prob * 100),
    Top5_Prob = sprintf("%.1f%%", Top5_Prob * 100),
    Top10_Prob = sprintf("%.1f%%", Top10_Prob * 100),
    Top30_Prob = sprintf("%.1f%%", Top30_Prob * 100),
    Top1_Decimal_Odds = round(Top1_Decimal_Odds, 2),
    Top3_Decimal_Odds = round(Top3_Decimal_Odds, 2),
    Top5_Decimal_Odds = round(Top5_Decimal_Odds, 2),
    Top10_Decimal_Odds = round(Top10_Decimal_Odds, 2),
    Top30_Decimal_Odds = round(Top30_Decimal_Odds, 2),
    Top1_American_Odds = ifelse(Top1_American_Odds > 0, 
                               sprintf("+%.0f", round(Top1_American_Odds, 0)),
                               sprintf("%.0f", round(Top1_American_Odds, 0))),
    Top3_American_Odds = ifelse(Top3_American_Odds > 0, 
                               sprintf("+%.0f", round(Top3_American_Odds, 0)),
                               sprintf("%.0f", round(Top3_American_Odds, 0))),
    Top5_American_Odds = ifelse(Top5_American_Odds > 0, 
                               sprintf("+%.0f", round(Top5_American_Odds, 0)),
                               sprintf("%.0f", round(Top5_American_Odds, 0))),
    Top10_American_Odds = ifelse(Top10_American_Odds > 0, 
                                sprintf("+%.0f", round(Top10_American_Odds, 0)),
                                sprintf("%.0f", round(Top10_American_Odds, 0))),
    Top30_American_Odds = ifelse(Top30_American_Odds > 0, 
                                sprintf("+%.0f", round(Top30_American_Odds, 0)),
                                sprintf("%.0f", round(Top30_American_Odds, 0)))
  ) %>%
  arrange(desc(as.numeric(sub("%", "", Top1_Prob))))

# Add individual model predictions for comparison for Top1
results_extended <- results %>%
  mutate(
    Top1_Logit = sprintf("%.1f%%", top1_models$logit * 100),
    Top1_RF = sprintf("%.1f%%", top1_models$rf * 100),
    Top1_XGB = sprintf("%.1f%%", top1_models$xgb * 100),
    Top1_SVM = sprintf("%.1f%%", top1_models$svm * 100),
    Top1_NN = sprintf("%.1f%%", top1_models$nn * 100)
  )

# Print model performance comparison for Top1
print("Model Performance Comparison (Top 1):")
print(top1_models$models$logit)
print(top1_models$models$rf)
print(top1_models$models$xgb)
print(top1_models$models$svm)
print(top1_models$models$nn)

# Save results
write.xlsx(results_extended, file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_placement-odds-ml.xlsx")

# Get variable importance for RF directly from ranger model
var_imp <- ranger::importance(top1_models$models$rf$finalModel)
var_imp_df <- data.frame(
  Variable = names(var_imp),
  Importance = as.numeric(var_imp)
)
var_imp_df <- var_imp_df[order(-var_imp_df$Importance), ]

print("Random Forest Variable Importance:")
print(var_imp_df)

# Variable importance for XGBoost
xgb_imp <- xgb.importance(model = top1_models$models$xgb$finalModel)
print("XGBoost Variable Importance:")
print(xgb_imp)

```


```{r odds-comparison}
# Function to extract features from a comma-separated string
parse_features <- function(feature_string) {
  trimws(unlist(strsplit(feature_string, ",")))
}

# Create optimal feature sets automatically from results_summary
optimal_features_sets <- lapply(targets, function(target) {
  parse_features(results_summary[[target]]$Best_combination)
})
names(optimal_features_sets) <- targets

# Now we can modify the ML predictions code to use these automatically selected features:

# Modified train_models function (same as before, but using optimal_features_sets)
train_models <- function(data, target, test_data, optimal_features) {
  # Prepare training data with only optimal features
  train_data <- data %>% 
    dplyr::select(all_of(c(target, optimal_features)))
  
  test_data <- test_data %>%
    dplyr::select(all_of(optimal_features))
  # Set up cross-validation
  ctrl <- trainControl(
    method = "cv",
    number = 5,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
  
  # Train models
  set.seed(123)
  
  # Logistic Regression (as baseline)
  logit_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "glm",
    family = "binomial",
    trControl = ctrl,
    metric = "ROC"
  )
  
  # Random Forest
  rf_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "ranger",
    trControl = ctrl,
    metric = "ROC",
    importance = 'impurity',
    tuneGrid = expand.grid(
      mtry = c(2, 3, 4),
      splitrule = "gini",
      min.node.size = c(5, 10, 20)
    )
  )
  
  # XGBoost
  xgb_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "xgbTree",
    trControl = ctrl,
    metric = "ROC",
    tuneLength = 5
  )
  
  # Support Vector Machine
  svm_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "svmRadial",
    trControl = ctrl,
    metric = "ROC",
    tuneLength = 5
  )
  
  # Neural Network
  nn_model <- train(
    as.formula(paste(target, "~ .")),
    data = train_data,
    method = "nnet",
    trControl = ctrl,
    metric = "ROC",
    tuneLength = 5,
    trace = FALSE,
    maxit = 1000
  )
  
  # Make predictions on test data
  pred_logit <- predict(logit_model, test_data, type = "prob")[,"Yes"]
  pred_rf <- predict(rf_model, test_data, type = "prob")[,"Yes"]
  pred_xgb <- predict(xgb_model, test_data, type = "prob")[,"Yes"]
  pred_svm <- predict(svm_model, test_data, type = "prob")[,"Yes"]
  pred_nn <- predict(nn_model, test_data, type = "prob")[,"Yes"]
  
  # Ensemble prediction (average of all models)
  pred_ensemble <- (pred_logit + pred_rf + pred_xgb + pred_svm + pred_nn) / 5
  
  # Return all predictions and models
  list(
    logit = pred_logit,
    rf = pred_rf,
    xgb = pred_xgb,
    svm = pred_svm,
    nn = pred_nn,
    ensemble = pred_ensemble,
    models = list(
      logit = logit_model,
      rf = rf_model,
      xgb = xgb_model,
      svm = svm_model,
      nn = nn_model
    )
  )
}


# Modify the prediction data preparation in ml_predictions()
ml_predictions <- function() {
  # Set up parallel processing
  registerDoParallel(cores = detectCores() - 1)
  
  # Setup data
  df_place <- df_race %>%
    group_by(Season) %>%
    mutate(Place = rank(-Pct_of_Max_Points, ties.method = "min")) %>%
    ungroup()
  
  # Create binary outcomes
  df_place <- df_place %>%
    mutate(
      Top1 = factor(ifelse(Place <= 1, "Yes", "No"), levels = c("No", "Yes")),
      TopThree = factor(ifelse(Place <= 3, "Yes", "No"), levels = c("No", "Yes")),
      Top5 = factor(ifelse(Place <= 5, "Yes", "No"), levels = c("No", "Yes")),
      Top10 = factor(ifelse(Place <= 10, "Yes", "No"), levels = c("No", "Yes")),
      Top30 = factor(ifelse(Place <= 30, "Yes", "No"), levels = c("No", "Yes"))
    )
  
  # Prepare prediction data with ALL possible columns needed
pred_data <- df_2025 %>%
  dplyr::select(Skier, Nation, 
                Pelo, Distance_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
                Sprint_Pelo, Classic_Pelo, Freestyle_Pelo,
                Pct_of_Max_Points) %>%
  rename(
    Prev_Pelo = Pelo,
    Prev_Distance = Distance_Pelo,
    Prev_Sprint_C = Sprint_C_Pelo,
    Prev_Sprint_F = Sprint_F_Pelo,
    Prev_Sprint = Sprint_Pelo,
    Prev_C = Classic_Pelo,
    Prev_F = Freestyle_Pelo,
    Prev_Pct_of_Max_Points = Pct_of_Max_Points
  )
  
  # Add Distance_C and Distance_F if they're not already there
  if(!"Prev_Distance_C" %in% names(pred_data)) {
    pred_data <- pred_data %>%
      mutate(Prev_Distance_C = Prev_Distance)
  }
  
  if(!"Prev_Distance_F" %in% names(pred_data)) {
    pred_data <- pred_data %>%
      mutate(Prev_Distance_F = Prev_Distance)
  }
  
  # Print the features being used for each target
  cat("\nFeatures being used for each target:\n")
  for(target in targets) {
    cat("\n", target, ":", paste(optimal_features_sets[[target]], collapse = ", "))
  }
  
  # Print available columns in prediction data
  cat("\n\nAvailable columns in prediction data:\n")
  print(names(pred_data))
  
  # Train models for each target using optimal features
  model_results <- list()
  
  for(target in targets) {
    cat("\nTraining models for", target, "using features:", 
        paste(optimal_features_sets[[target]], collapse = ", "), "\n")
    
    # Check if all required features are available
    missing_features <- setdiff(optimal_features_sets[[target]], names(pred_data))
    if(length(missing_features) > 0) {
      stop("Missing features for ", target, ": ", paste(missing_features, collapse = ", "))
    }
    
    model_results[[target]] <- train_models(df_place, target, pred_data, 
                                          optimal_features_sets[[target]])
  }
  
  # Rest of the function remains the same...
  # Create results dataframe
  results <- data.frame(
    Skier = pred_data$Skier,
    Nation = pred_data$Nation,
    Top1_Prob = model_results$Top1$ensemble,
    Top3_Prob = model_results$TopThree$ensemble,
    Top5_Prob = model_results$Top5$ensemble,
    Top10_Prob = model_results$Top10$ensemble,
    Top30_Prob = model_results$Top30$ensemble
  )
  
  # Calculate odds and format results
  results <- results %>%
    mutate(
      Top1_Decimal_Odds = 1 / Top1_Prob,
      Top1_American_Odds = ifelse(Top1_Prob >= 0.5,
                                 -Top1_Prob/(1-Top1_Prob) * 100,
                                 (1-Top1_Prob)/Top1_Prob * 100),
      Top3_Decimal_Odds = 1 / Top3_Prob,
      Top3_American_Odds = ifelse(Top3_Prob >= 0.5,
                                 -Top3_Prob/(1-Top3_Prob) * 100,
                                 (1-Top3_Prob)/Top3_Prob * 100),
      Top5_Decimal_Odds = 1 / Top5_Prob,
      Top5_American_Odds = ifelse(Top5_Prob >= 0.5,
                                 -Top5_Prob/(1-Top5_Prob) * 100,
                                 (1-Top5_Prob)/Top5_Prob * 100),
      Top10_Decimal_Odds = 1 / Top10_Prob,
      Top10_American_Odds = ifelse(Top10_Prob >= 0.5,
                                  -Top10_Prob/(1-Top10_Prob) * 100,
                                  (1-Top10_Prob)/Top10_Prob * 100),
      Top30_Decimal_Odds = 1 / Top30_Prob,
      Top30_American_Odds = ifelse(Top30_Prob >= 0.5,
                                  -Top30_Prob/(1-Top30_Prob) * 100,
                                  (1-Top30_Prob)/Top30_Prob * 100)
    ) %>%
    mutate(
      Top1_Prob = sprintf("%.1f%%", Top1_Prob * 100),
      Top3_Prob = sprintf("%.1f%%", Top3_Prob * 100),
      Top5_Prob = sprintf("%.1f%%", Top5_Prob * 100),
      Top10_Prob = sprintf("%.1f%%", Top10_Prob * 100),
      Top30_Prob = sprintf("%.1f%%", Top30_Prob * 100),
      Top1_Decimal_Odds = round(Top1_Decimal_Odds, 2),
      Top3_Decimal_Odds = round(Top3_Decimal_Odds, 2),
      Top5_Decimal_Odds = round(Top5_Decimal_Odds, 2),
      Top10_Decimal_Odds = round(Top10_Decimal_Odds, 2),
      Top30_Decimal_Odds = round(Top30_Decimal_Odds, 2),
      Top1_American_Odds = ifelse(Top1_American_Odds > 0, 
                                 sprintf("+%.0f", round(Top1_American_Odds, 0)),
                                 sprintf("%.0f", round(Top1_American_Odds, 0))),
      Top3_American_Odds = ifelse(Top3_American_Odds > 0, 
                                 sprintf("+%.0f", round(Top3_American_Odds, 0)),
                                 sprintf("%.0f", round(Top3_American_Odds, 0))),
      Top5_American_Odds = ifelse(Top5_American_Odds > 0, 
                                 sprintf("+%.0f", round(Top5_American_Odds, 0)),
                                 sprintf("%.0f", round(Top5_American_Odds, 0))),
      Top10_American_Odds = ifelse(Top10_American_Odds > 0, 
                                  sprintf("+%.0f", round(Top10_American_Odds, 0)),
                                  sprintf("%.0f", round(Top10_American_Odds, 0))),
      Top30_American_Odds = ifelse(Top30_American_Odds > 0, 
                                  sprintf("+%.0f", round(Top30_American_Odds, 0)),
                                  sprintf("%.0f", round(Top30_American_Odds, 0)))
    ) %>%
    arrange(desc(as.numeric(sub("%", "", Top1_Prob))))
  
  # Add feature information to results
  features_used <- data.frame(
    Target = names(optimal_features_sets),
    Features = sapply(optimal_features_sets, paste, collapse = ", ")
  )
  
  # Create extended results with feature information
  results_with_features <- list(
    Predictions = results,
    Features_Used = features_used
  )
  
  return(results_with_features)
}

# Run predictions and save results
ml_results <- ml_predictions()

# Save results with feature information
write.xlsx(ml_results, 
           file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_predictions-with-selected-features.xlsx",
           sheetName = c("Predictions", "Features_Used"))

# Print summary of features used
cat("\nSummary of features used for each target:\n")
print(ml_results$Features_Used)
```


### Breakout Potential
```{r restart}
M_chrono <- read_feather('/Users/syverjohansen/ski/elo/python/ski/polars/excel365/men_chrono.feather')

# Step Two: Create a column called WC Points that maps place to world cup points from a list
wc_points <- c(100,95,90,85,80,75,72,69,66,63,60,58,56,54,52,50,48,46,44,42,40,38,36,34,32,30,28,26,24,22,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1)
stage_points <- c(50, 47, 44, 41, 38, 35, 32, 30, 28, 26, 24, 22, 20, 18, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1)
tds_points <- c(300, 285, 270, 255, 240, 216, 207, 198, 189, 180, 174, 168, 162, 156, 150, 144, 138, 132, 126, 120, 114, 108, 102, 96, 90, 84, 78, 72, 66, 60, 57, 54, 51, 48, 45, 42, 39, 36, 33, 30, 27, 24, 21, 18, 15, 12, 9, 6, 3)

# Function to safely fetch points based on Place
get_points <- function(place, points_list) {
  if (place >= 1 && place <= length(points_list)) {
    return(points_list[place])
  }
  return(0)
}

# Apply points logic based on Event and Distance
df <- M_chrono %>%
  mutate(Points = case_when(
    Event == "Tour de Ski" & Distance == "Stage" ~ map_int(Place, ~ get_points(.x, tds_points)),
    Event == "Tour de Ski" ~ map_int(Place, ~ get_points(.x, stage_points)),
    TRUE ~ map_int(Place, ~ get_points(.x, wc_points))
  ))

# Sort the df by Date, Race, and Place
df <- df %>%
  arrange(Date, Race, Place)

# Step Three: Filter for the last five years
df <- df %>%
  filter(Event %in% c("Offseason", "World Cup", "Nordic Opening", "Tour de Ski",  "World Cup Final", "Ski Tour Canada")) %>%
  group_by(ID, Season) %>%
  mutate(Cumulative_Points = cumsum(Points)) %>%
  ungroup()

# Function to replace NAs with the first quartile within each group
replace_na_with_quartile <- function(x) {
  quartile_1 <- quantile(x, 0.25, na.rm = TRUE)  # Calculate the first quartile, ignoring NAs
  ifelse(is.na(x), quartile_1, x)  # Replace NAs with the first quartile
}

# Apply this logic to each of the specified columns and then perform the percentage calculations
df <- df %>%
  group_by(Season, Race) %>%
  mutate(
    Distance_Pelo = replace_na_with_quartile(Distance_Pelo),
    Distance_C_Pelo = replace_na_with_quartile(Distance_C_Pelo),
    Distance_F_Pelo = replace_na_with_quartile(Distance_F_Pelo),
    Pelo = replace_na_with_quartile(Pelo),
    Sprint_Pelo = replace_na_with_quartile(Sprint_Pelo),
    Sprint_C_Pelo = replace_na_with_quartile(Sprint_C_Pelo),
    Sprint_F_Pelo = replace_na_with_quartile(Sprint_F_Pelo),
    Freestyle_Pelo = replace_na_with_quartile(Freestyle_Pelo),
    Classic_Pelo = replace_na_with_quartile(Classic_Pelo)
  ) %>%
  ungroup()
df <- df %>%
  filter(! Distance %in% c("Ts", "Rel"))


# Step One: Filter for first place
first_place <- df %>%
  filter(Place == 1)

# Step Two: Group by Season and calculate total points for first place
max_points_per_season <- first_place %>%
  group_by(Season) %>%
  summarise(Max_Points = sum(Points), .groups = 'drop')

# Step Three: Join the max points back to the original data frame
df <- df %>%
  left_join(max_points_per_season, by = "Season")

# Step Four: Calculate percentage of maximum points
df <- df %>%
  mutate(Pct_of_Max_Points = Cumulative_Points / Max_Points)


# Step Four: Set up the explanatory variables
# Filter for the offseason
elo_df <- df %>%
  filter(Event == "Offseason") %>%
  arrange(ID, Season) %>%
  group_by(ID) %>%
  mutate(Prev_Pelo = lag(Pelo)) %>%
  ungroup()



elo_df <- df %>%
  filter(Event == "Offseason") %>%
  arrange(ID, Season) %>%
  group_by(ID) %>%
  mutate(Prev_Pelo = lag(Pelo), Prev_Sprint=lag(Sprint_Pelo), Prev_Sprint_C=lag(Sprint_C_Pelo), Prev_Sprint_F=lag(Sprint_F_Pelo), Prev_Distance=lag(Distance_Pelo), Prev_Distance_F=lag(Distance_F_Pelo), Prev_Distance_C=lag(Distance_C_Pelo), Prev_F=lag(Freestyle_Pelo), Prev_C=lag(Classic_Pelo), Prev_Pct_of_Max_Points=lag(Pct_of_Max_Points)) %>%
  ungroup()





elo_df <- elo_df %>%
  filter(Season>2001)

df <- elo_df

df <- df %>%
  group_by(Season) %>%
  mutate(
    Prev_Distance= replace_na_with_quartile(Prev_Distance),
    Prev_Distance_C = replace_na_with_quartile(Prev_Distance_C),
    Prev_Distance_F = replace_na_with_quartile(Prev_Distance_F),
    Prev_Pelo = replace_na_with_quartile(Prev_Pelo),
    Prev_Sprint = replace_na_with_quartile(Prev_Sprint),
    Prev_Sprint_C = replace_na_with_quartile(Prev_Sprint_C),
    Prev_Sprint_F = replace_na_with_quartile(Prev_Sprint_F),
    Prev_F = replace_na_with_quartile(Prev_F),
    Prev_C = replace_na_with_quartile(Prev_C),
    Prev_Pct_of_Max_Points = replace(Prev_Pct_of_Max_Points, is.na(Prev_Pct_of_Max_Points), 0)
  ) %>%
  ungroup()
```

### Identify what I mean by breakout
```{r breakout-identifier}
 

top_performers = df[df$Pct_of_Max_Points>.5, c("Skier", "Nation", "Season", "Pct_of_Max_Points", "Age", "Exp")]
unique(top_performers$Skier)
```

### What Claude suggests
```{r claude-breakout}
library(dplyr)
library(ggplot2)


# 1. Year-over-Year Improvement Analysis
yoy_improvement <- function(df) {
  # Calculate year-over-year changes in key metrics
  improvement_df <- df %>%
    arrange(ID, Season) %>%
    group_by(ID) %>%
    mutate(
      Elo_Change = Prev_Pelo - lag(Prev_Pelo),
      Distance_Change = Prev_Distance - lag(Prev_Distance),
      Sprint_C_Change = Prev_Sprint_C - lag(Prev_Sprint_C),
      Sprint_F_Change = Prev_Sprint_F - lag(Prev_Sprint_F),
      Distance_C_Change = Prev_Distance_C - lag(Prev_Distance_C),
      Distance_F_Change = Prev_Distance_F - lag(Prev_Distance_F),
      Classic_Change = Prev_C - lag(Prev_C),
      Freestyle_Change = Prev_F - lag(Prev_F),
      Points_Change = Pct_of_Max_Points - lag(Pct_of_Max_Points),
      Prior_Best_Place = lag(cummin(Place))
    ) %>%
    ungroup() %>%
    filter(Season == 2024) %>%  # Focus on most recent season
    dplyr::select(Skier, Nation, Age, Elo_Change, Distance_Change, 
           Sprint_C_Change, Sprint_F_Change, Distance_C_Change,
           Distance_F_Change, Classic_Change, Freestyle_Change,
           Points_Change, Prior_Best_Place, Place)

  return(improvement_df)
}

# 1 Age-Based Analysis with character Age handling and NA management
age_potential <- function(df) {
  # First, let's examine the Age column
  print("Age column summary:")
  print(table(df$Age, useNA = "ifany"))
  
  # Create age performance benchmarks with cleaned age data
  age_df <- df %>%
    mutate(
      Age_Group = case_when(
        is.numeric(Age) ~ floor(as.numeric(Age)),
        !is.na(Age) ~ floor(as.numeric(gsub("[^0-9.]", "", Age))),  # Remove any non-numeric characters
        TRUE ~ NA_real_
      )
    ) %>%
    filter(!is.na(Age_Group)) %>%  # Remove NAs before grouping
    group_by(Age_Group) %>%
    summarise(
      Avg_Points = mean(Pct_of_Max_Points, na.rm = TRUE),
      Max_Points = max(Pct_of_Max_Points, na.rm = TRUE),
      Min_Points = min(Pct_of_Max_Points, na.rm = TRUE),
      n = n()
    ) %>%
    filter(n >= 10)  # Only include ages with sufficient data
  
  print("Age group summary after cleaning:")
  print(age_df)
  
  # Compare current skiers to age benchmarks
  potential_df <- df %>%
    filter(Season == 2024) %>%
    mutate(
      Age_Group = case_when(
        is.numeric(Age) ~ floor(as.numeric(Age)),
        !is.na(Age) ~ floor(as.numeric(gsub("[^0-9.]", "", Age))),
        TRUE ~ NA_real_
      )
    ) %>%
    # Join with age benchmarks
    left_join(age_df, by = "Age_Group") %>%
    # Handle potential NAs from the join
    mutate(
      Performance_vs_Age = if_else(!is.na(Avg_Points), 
                                  Pct_of_Max_Points - Avg_Points, 
                                  NA_real_),
      Pct_of_Age_Max = if_else(!is.na(Max_Points), 
                              Pct_of_Max_Points / Max_Points, 
                              NA_real_),
      Age_Potential = case_when(
        is.na(Performance_vs_Age) ~ "Unknown (Missing Age Data)",
        Performance_vs_Age > 0 & Age_Group < 23 ~ "High Young Potential",
        Performance_vs_Age > 0 & Age_Group >= 23 ~ "Strong Performance",
        Performance_vs_Age <= 0 & Pct_of_Age_Max > 0.7 ~ "Room to Improve",
        TRUE ~ "Standard Development"
      )
    ) %>%
    dplyr::select(Skier, Nation, Age, Age_Group, Performance_vs_Age, 
           Pct_of_Age_Max, Age_Potential, Place, Avg_Points, Max_Points)
  
  # Print diagnostic information
  print("\nDiagnostic Information:")
  print(paste("Total skiers:", nrow(potential_df)))
  print(paste("Skiers with valid age data:", sum(!is.na(potential_df$Age_Group))))
  print(paste("Age range:", 
              min(potential_df$Age_Group, na.rm = TRUE), "to",
              max(potential_df$Age_Group, na.rm = TRUE)))
  
  # Print summary by age potential category
  print("\nBreakdown by Age Potential Category:")
  print(table(potential_df$Age_Potential, useNA = "ifany"))
  
  return(potential_df)
}

#2 Age Potential
age_potential <- function(df) {
  # First, examine Age column
  print("Age column structure:")
  print(str(df$Age))
  print("\nFirst few Age values:")
  print(head(df$Age, 20))
  
  # Create age performance benchmarks with more careful age handling
  age_df <- df %>%
    dplyr::mutate(
      # Print some age values before conversion
      Age_Debug = Age,
      Age_Num = suppressWarnings(as.numeric(Age)),
      Age_Group = floor(Age_Num)
    ) %>%
    dplyr::filter(!is.na(Age_Group)) %>%
    dplyr::group_by(Age_Group) %>%
    dplyr::summarise(
      Avg_Points = mean(Pct_of_Max_Points, na.rm = TRUE),
      Max_Points = max(Pct_of_Max_Points, na.rm = TRUE),
      Min_Points = min(Pct_of_Max_Points, na.rm = TRUE),
      n = n(),
      .groups = 'drop'
    ) %>%
    dplyr::filter(n >= 10)
  
  print("\nAge performance benchmarks:")
  print(age_df)
  
  # Compare current skiers to age benchmarks
  potential_df <- df %>%
    dplyr::filter(Season == 2024) %>%
    dplyr::mutate(
      Age_Num = suppressWarnings(as.numeric(Age)),
      Age_Group = floor(Age_Num)
    ) %>%
    # Join with age benchmarks and ensure all columns are preserved
    dplyr::left_join(
      age_df %>% 
        dplyr::rename(
          Age_Avg_Points = Avg_Points,
          Age_Max_Points = Max_Points,
          Age_Min_Points = Min_Points,
          Age_Group_N = n
        ),
      by = "Age_Group"
    ) %>%
    dplyr::mutate(
      Performance_vs_Age = Pct_of_Max_Points - Age_Avg_Points,
      Pct_of_Age_Max = Pct_of_Max_Points / Age_Max_Points,
      Age_Potential = dplyr::case_when(
        is.na(Age_Group) ~ "Unknown (Missing Age Data)",
        Performance_vs_Age > 0 & Age_Group < 23 ~ "High Young Potential",
        Performance_vs_Age > 0 & Age_Group >= 23 ~ "Strong Performance",
        !is.na(Pct_of_Age_Max) & Pct_of_Age_Max > 0.7 ~ "Room to Improve",
        TRUE ~ "Standard Development"
      )
    ) %>%
    dplyr::select(
      Skier, Nation, Age, Age_Num, Age_Group, 
      Pct_of_Max_Points, Age_Avg_Points, Age_Max_Points,
      Performance_vs_Age, Pct_of_Age_Max, 
      Age_Potential, Place
    )
  
  # Print diagnostic information
  print("\nAge conversion summary:")
  print(paste("Original Age NAs:", sum(is.na(df$Age))))
  print(paste("Converted Age NAs:", sum(is.na(potential_df$Age_Num))))
  print(paste("Age range after conversion:", 
              min(potential_df$Age_Num, na.rm = TRUE), "to",
              max(potential_df$Age_Num, na.rm = TRUE)))
  
  print("\nAge potential categories:")
  print(table(potential_df$Age_Potential, useNA = "ifany"))
  
  return(potential_df)
}

# Let's try to run it with some error catching
tryCatch({
  age_results <- age_potential(df_race)
  print("\nAnalysis completed successfully")
}, error = function(e) {
  print("\nError occurred:")
  print(e)
}, warning = function(w) {
  print("\nWarning occurred:")
  print(w)
})
# 3. Recent Momentum Analysis 
momentum_analysis <- function(df) {
  momentum_df <- df %>%
    arrange(ID, Date) %>%
    group_by(ID) %>%
    mutate(
      Recent_Pelo_Trend = (Pelo - lag(Pelo, 5)) / 5,  # Rate of change over last 5 races
      Recent_Place_Trend = (Place - lag(Place, 5)) / 5,
      Last_3_Avg_Place = rollmean(Place, k = 3, fill = NA, align = "right"),
      Season_Avg_Place = mean(Place),
      Improvement_Rate = (Last_3_Avg_Place - Season_Avg_Place) / Season_Avg_Place
    ) %>%
    filter(Season == 2024) %>%
    dplyr::select(Skier, Nation, Recent_Pelo_Trend, Recent_Place_Trend, 
           Improvement_Rate, Last_3_Avg_Place, Season_Avg_Place)
  
  return(momentum_df)
}

# 4. Consistency vs Peaks Analysis
library(ggrepel)

peak_analysis <- function(chrono_df, current_df) {
  # Analyze individual race performances from 2024 season
  peak_df <- chrono_df %>%
    dplyr::filter(
      Season == 2024,
      # Filter out team events if they exist
      !Distance %in% c("Rel", "Ts"),
      # Filter for individual races
      Event %in% c("World Cup", "Tour de Ski", "World Cup Final")
    ) %>%
    dplyr::group_by(ID, Skier) %>%
    dplyr::summarise(
      Races_Completed = n(),
      Best_Place = min(Place, na.rm = TRUE),
      Worst_Place = max(Place, na.rm = TRUE),
      Avg_Place = mean(Place, na.rm = TRUE),
      Median_Place = median(Place, na.rm = TRUE),
      Place_SD = sd(Place, na.rm = TRUE),
      
      # Handle potential Inf values in specialized metrics
      Distance_Best = ifelse(all(is.na(Place[Distance == "Distance"])), 
                           NA, 
                           min(Place[Distance == "Distance"], na.rm = TRUE)),
      Sprint_Best = ifelse(all(is.na(Place[Distance == "Sprint"])), 
                          NA, 
                          min(Place[Distance == "Sprint"], na.rm = TRUE)),
      Distance_Avg = ifelse(all(is.na(Place[Distance == "Distance"])), 
                           NA, 
                           mean(Place[Distance == "Distance"], na.rm = TRUE)),
      Sprint_Avg = ifelse(all(is.na(Place[Distance == "Sprint"])), 
                         NA, 
                         mean(Place[Distance == "Sprint"], na.rm = TRUE)),
      
      Classic_Best = ifelse(all(is.na(Place[Technique == "C"])), 
                           NA, 
                           min(Place[Technique == "C"], na.rm = TRUE)),
      Freestyle_Best = ifelse(all(is.na(Place[Technique == "F"])), 
                             NA, 
                             min(Place[Technique == "F"], na.rm = TRUE)),
      Classic_Avg = ifelse(all(is.na(Place[Technique == "C"])), 
                          NA, 
                          mean(Place[Technique == "C"], na.rm = TRUE)),
      Freestyle_Avg = ifelse(all(is.na(Place[Technique == "F"])), 
                            NA, 
                            mean(Place[Technique == "F"], na.rm = TRUE)),
      
      # Consistency metrics (only calculate if we have valid places)
      Peak_Potential = ifelse(is.finite(Best_Place) && is.finite(Avg_Place),
                            Best_Place / Avg_Place, 
                            NA),
      Consistency = ifelse(is.finite(Place_SD) && is.finite(Avg_Place),
                          1 - (Place_SD / Avg_Place), 
                          NA),
      
      # Top results frequency
      Top_10_Rate = mean(Place <= 10, na.rm = TRUE),
      Top_20_Rate = mean(Place <= 20, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    # Add categorization of performance patterns
    dplyr::mutate(
      Performance_Pattern = dplyr::case_when(
        is.na(Consistency) | is.na(Top_10_Rate) ~ "Insufficient Data",
        Consistency > 0.7 & Top_10_Rate > 0.5 ~ "Consistent High Performer",
        Consistency > 0.7 & Top_20_Rate > 0.5 ~ "Consistent Solid Performer",
        !is.na(Peak_Potential) & Peak_Potential < 0.5 & Consistency < 0.5 ~ "High Peaks but Inconsistent",
        !is.na(Peak_Potential) & Peak_Potential < 0.7 & Consistency > 0.6 ~ "Solid and Stable",
        TRUE ~ "Developing"
      ),
      Specialization = dplyr::case_when(
        is.na(Distance_Avg) | is.na(Sprint_Avg) ~ "Insufficient Data",
        abs(Distance_Avg - Sprint_Avg) < 5 ~ "All-Round",
        Distance_Avg < Sprint_Avg ~ "Distance Specialist",
        Sprint_Avg < Distance_Avg ~ "Sprint Specialist",
        TRUE ~ "Mixed Results"
      ),
      Technique_Preference = dplyr::case_when(
        is.na(Classic_Avg) | is.na(Freestyle_Avg) ~ "Insufficient Data",
        abs(Classic_Avg - Freestyle_Avg) < 5 ~ "Balanced",
        Classic_Avg < Freestyle_Avg ~ "Classic Preference",
        Freestyle_Avg < Classic_Avg ~ "Freestyle Preference",
        TRUE ~ "Mixed Results"
      )
    )
  
  # Join with current season standings
  final_df <- current_df %>%
    dplyr::select(ID, Skier, Nation, Pct_of_Max_Points) %>%
    dplyr::left_join(peak_df, by = "ID") %>%
    dplyr::arrange(desc(Pct_of_Max_Points))
  
  # Create summary statistics
  pattern_summary <- final_df %>%
    dplyr::group_by(Performance_Pattern) %>%
    dplyr::summarise(
      Count = n(),
      Avg_Points = mean(Pct_of_Max_Points, na.rm = TRUE),
      Avg_Consistency = mean(Consistency, na.rm = TRUE),
      .groups = 'drop'
    )
  
  # Print summary information
  print("Performance Pattern Summary:")
  print(pattern_summary)
  
  # Create visualization of consistency vs peak performance
  p1 <- ggplot(final_df %>% dplyr::filter(!is.na(Consistency) & !is.na(Peak_Potential)), 
               aes(x = Consistency, y = Peak_Potential)) +
    geom_point(aes(color = Performance_Pattern, size = Pct_of_Max_Points)) +
    geom_text_repel(
      data = final_df %>% 
        dplyr::filter(!is.na(Consistency) & !is.na(Peak_Potential)) %>%
        dplyr::filter(Pct_of_Max_Points > quantile(Pct_of_Max_Points, 0.8, na.rm = TRUE)),
      aes(label = Skier),
      size = 3
    ) +
    labs(title = "Consistency vs Peak Performance",
         subtitle = "Size indicates season points, labels for top 20% performers") +
    theme_minimal()
  
  # Create technique preference visualization
  p2 <- ggplot(final_df %>% 
                 dplyr::filter(!is.na(Classic_Avg) & !is.na(Freestyle_Avg)), 
               aes(x = Classic_Avg, y = Freestyle_Avg)) +
    geom_point(aes(color = Technique_Preference, size = Pct_of_Max_Points)) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    labs(title = "Classic vs Freestyle Performance",
         x = "Average Classic Place",
         y = "Average Freestyle Place") +
    theme_minimal()
  
  # Return both the data and the plots
  return(list(
    data = final_df,
    pattern_summary = pattern_summary,
    consistency_plot = p1,
    technique_plot = p2
  ))
}


df_race <- df_race %>%
  group_by(Season) %>%
  mutate(Place = rank(-Pct_of_Max_Points, ties.method = "min")) %>%
  ungroup()

# Run all analyses
yoy_results <- yoy_improvement(df_race)
print(yoy_results)
age_results <- age_potential(df_race)
print(age_results)
momentum_results <- momentum_analysis(df_race)
print(momentum_results)
peak_results <- peak_analysis(M_chrono, df_race)
#print(peak_results)
head(peak_results$data)

# Clean peak_results data
peak_data <- peak_results$data %>%
  dplyr::select(-Skier.y) %>%  # Remove duplicate Skier column
  rename(Skier = Skier.x) %>%  # Rename remaining Skier column
  distinct(Skier, .keep_all = TRUE)  # Remove duplicates

# Add this helper function
scale_minmax <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

yoy_results %>%
  left_join(age_results, by = c("Skier", "Nation")) %>%
  left_join(momentum_results, by = c("Skier", "Nation")) %>%
  left_join(peak_data, by = c("Skier", "Nation")) %>%
  names() %>%
  print()
# Now modify the join
breakout_potential <- yoy_results %>%
 left_join(age_results, by = c("Skier", "Nation")) %>%
 left_join(momentum_results, by = c("Skier", "Nation")) %>%
 left_join(peak_data, by = c("Skier", "Nation")) %>%
 mutate(
   Pelo_Score = scale_minmax(Elo_Change),
   Age_Score = scale_minmax(1 / as.numeric(Age.x)),
   Momentum_Score = scale_minmax(-Recent_Place_Trend),
   Potential_Score = scale_minmax(Performance_vs_Age),
   Peak_Score = scale_minmax(Peak_Potential),
   Breakout_Score = (Pelo_Score * 0.3) + 
                    (Age_Score * 0.15) + 
                    (Momentum_Score * 0.2) + 
                    (Potential_Score * 0.2) + 
                    (Peak_Score * 0.15)
 ) %>%
 arrange(desc(Breakout_Score))

# Create different categories of breakout candidates
breakout_categories <- breakout_potential %>%
 mutate(
   Young_Rising_Star = Age.x <= 23 & Elo_Change > 0 & Performance_vs_Age > 0,
   Consistent_Improver = Recent_Place_Trend < 0 & Improvement_Rate < 0 & Peak_Potential < 2,
   Late_Bloomer = Age.x >= 25 & Elo_Change > median(Elo_Change, na.rm = TRUE),
   Raw_Talent = Peak_Potential < 1.5 & Performance_vs_Age > 0,
   Category = case_when(
     Young_Rising_Star ~ "Young Rising Star",
     Consistent_Improver ~ "Consistent Improver",
     Late_Bloomer ~ "Late Bloomer", 
     Raw_Talent ~ "Raw Talent",
     TRUE ~ "Other"
   )
 )

# Create visualizations
## Breakout Potential Plot
p1 <- ggplot(head(breakout_potential, 20), 
             aes(x = reorder(Skier, Breakout_Score), y = Breakout_Score)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 20 Breakout Candidates",
       x = "Skier",
       y = "Breakout Score")

## Age vs Performance Plot
p2 <- ggplot(breakout_potential, 
             aes(x = Age, y = Performance_vs_Age, size = Breakout_Score)) +
  geom_point(alpha = 0.6) +
  geom_text(aes(label = ifelse(Breakout_Score > quantile(Breakout_Score, 0.9), 
                              Skier, "")), 
            hjust = -0.1, vjust = 0) +
  theme_minimal() +
  labs(title = "Age vs Performance Analysis",
       x = "Age",
       y = "Performance vs Age Average")

# Save results
# Fix column name
results_list <- list(
 Breakout_Potential = breakout_potential %>%
   dplyr::select(Skier, Nation, Age.x, Breakout_Score, Elo_Change, 
          Performance_vs_Age, Recent_Place_Trend, Peak_Potential),
 
 Breakout_Categories = breakout_categories %>%
   dplyr::select(Skier, Nation, Category, Age.x, Breakout_Score, 
          Performance_vs_Age, Recent_Place_Trend),
 
 Young_Talents = breakout_categories %>%
   filter(Young_Rising_Star) %>%
   arrange(desc(Breakout_Score)),
 
 Consistent_Improvers = breakout_categories %>%
   filter(Consistent_Improver) %>%
   arrange(desc(Breakout_Score)),
 
 Late_Bloomers = breakout_categories %>%
   filter(Late_Bloomer) %>%
   arrange(desc(Breakout_Score))
)

# Save to Excel
write.xlsx(results_list, 
           file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_breakout-candidates.xlsx",
           rowNames = FALSE)

# Print summary of top candidates in each category
print("Top Breakout Candidates by Category:")
for(category in unique(breakout_categories$Category)) {
  cat("\n", category, ":\n")
  print(breakout_categories %>%
          filter(Category == category) %>%
          arrange(desc(Breakout_Score)) %>%
          dplyr::select(Skier, Nation, Age.x, Breakout_Score) %>%
          head(5))
}

# Helper function for scaling
scale_minmax <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
```
The analysis has potential but is very incomplete.  Will be pulling stuff from it for a final analysis.


### Feature Selection
```{r feat-sel-break}
# Function to evaluate predictor importance
evaluate_predictors <- function(df, predictors) {
  # Prepare data and handle NAs
  model_data <- df %>%
    dplyr::group_by(ID) %>%
    dplyr::mutate(
      Will_Breakthrough = lead(Pct_of_Max_Points >= 0.5, 1) & Pct_of_Max_Points < 0.5,
      # Convert to factor with meaningful levels
      Will_Breakthrough = factor(Will_Breakthrough, levels = c(FALSE, TRUE), 
                               labels = c("No", "Yes")),
      Age = as.numeric(Age)
    ) %>%
    dplyr::ungroup() %>%
    dplyr::filter(!is.na(Will_Breakthrough)) %>%
    # Select only the columns we need
    dplyr::select(Will_Breakthrough, all_of(predictors)) %>%
    # Remove rows with any NAs
    na.omit()
  
  print("Data dimensions after NA removal:")
  print(dim(model_data))
  print("\nBreakthrough distribution:")
  print(table(model_data$Will_Breakthrough))
  
  # Set up cross-validation
  ctrl <- trainControl(
    method = "cv",
    number = 5,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
  
  # Train model with all predictors
  full_model <- train(
    as.formula(paste("Will_Breakthrough ~", paste(predictors, collapse = " + "))),
    data = model_data,
    method = "glm",
    family = "binomial",
    trControl = ctrl,
    metric = "ROC"
  )
  
  # Random Forest for variable importance
  rf_model <- train(
    as.formula(paste("Will_Breakthrough ~", paste(predictors, collapse = " + "))),
    data = model_data,
    method = "ranger",
    trControl = ctrl,
    importance = 'impurity',
    metric = "ROC"
  )
  
  # Get variable importance from both models
  log_importance <- varImp(full_model)$importance
  rf_importance <- varImp(rf_model)$importance
  
  # Combine importance scores
  importance_df <- data.frame(
    Predictor = rownames(log_importance),
    Logistic_Importance = log_importance$Overall,
    RF_Importance = rf_importance$Overall
  ) %>%
    dplyr::mutate(
      Avg_Importance = (scale(Logistic_Importance) + scale(RF_Importance)) / 2
    ) %>%
    dplyr::arrange(desc(Avg_Importance))
  
  # Identify top predictors (those above mean importance)
  top_predictors <- importance_df %>%
    dplyr::filter(Avg_Importance > mean(Avg_Importance)) %>%
    dplyr::pull(Predictor)
  
  # Test reduced model with only top predictors
  reduced_model <- train(
    as.formula(paste("Will_Breakthrough ~", paste(top_predictors, collapse = " + "))),
    data = model_data,
    method = "glm",
    family = "binomial",
    trControl = ctrl,
    metric = "ROC"
  )
  
  # Compare model performance
  model_comparison <- data.frame(
    Model = c("Full", "Reduced"),
    ROC = c(max(full_model$results$ROC),
            max(reduced_model$results$ROC)),
    Predictors = c(length(predictors),
                   length(top_predictors))
  )
  
  # Create importance plot
  importance_plot <- ggplot(importance_df, 
                          aes(x = reorder(Predictor, Avg_Importance),
                              y = Avg_Importance)) +
    geom_bar(stat = "identity", 
             aes(fill = Avg_Importance > mean(Avg_Importance))) +
    coord_flip() +
    labs(title = "Predictor Importance",
         subtitle = "Based on combined Logistic and Random Forest importance",
         x = "Predictor",
         y = "Average Standardized Importance",
         fill = "Above Average\nImportance") +
    theme_minimal()
  
  return(list(
    importance = importance_df,
    top_predictors = top_predictors,
    model_comparison = model_comparison,
    importance_plot = importance_plot,
    full_model = full_model,
    reduced_model = reduced_model,
    model_data = model_data  # Include the processed data for inspection
  ))
}

# Get all potential predictors
predictors <- c(names(df)[grep("^Prev_", names(df))], "Age", "Exp")
print("Available predictors:")
print(predictors)

# Run the analysis
predictor_analysis <- evaluate_predictors(df, predictors)

# Print results
print("Predictor Importance Summary:")
print(predictor_analysis$importance)

print("\nTop Predictors:")
print(predictor_analysis$top_predictors)

print("\nModel Comparison:")
print(predictor_analysis$model_comparison)

# Save results
write.xlsx(list(
  Predictor_Importance = predictor_analysis$importance,
  Model_Comparison = predictor_analysis$model_comparison,
  Full_Model_Summary = capture.output(summary(predictor_analysis$full_model$finalModel)),
  Reduced_Model_Summary = capture.output(summary(predictor_analysis$reduced_model$finalModel))
), 
file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_breakthrough_predictor-selection-results.xlsx",
sheetName = c("Importance", "Comparison", "Full_Model", "Reduced_Model"))

# Display importance plot
print(predictor_analysis$importance_plot)
```

### Potential to Have a Big Season
```{r big-break}
predict_breakthroughs <- function(historical_df, current_df, predictor_analysis, start_year = 2006) {
  # Define mapping from prev variables to pelo variables
  predictor_mapping <- c(
    "Prev_Pelo" = "Pelo",
    "Prev_Sprint" = "Sprint_Pelo",
    "Prev_Sprint_C" = "Sprint_C_Pelo",
    "Prev_Sprint_F" = "Sprint_F_Pelo",
    "Prev_Distance" = "Distance_Pelo",
    "Prev_Distance_F" = "Distance_F_Pelo",
    "Prev_Distance_C" = "Distance_C_Pelo",
    "Prev_F" = "Freestyle_Pelo",
    "Prev_C" = "Classic_Pelo"
  )
  
  # Get the top predictors from our analysis
  best_predictors <- predictor_analysis$top_predictors
  #best_predictors <- c("Prev_Pelo", "Prev_Distance", "Prev_Sprint", "Age", "Age", "Exp")
  
  # Print the predictors we're using
  print("Using the following predictors:")
  print(best_predictors)
  
  # Identify current non-breakthrough skiers with proper age handling
  current_candidates <- current_df %>%
    dplyr::filter(Season == 2024) %>%  # Get latest season
    dplyr::mutate(
      # Handle Age conversion
      Age = as.numeric(Age),
      # Replace NA ages with median
      Age = ifelse(is.na(Age),
                   median(as.numeric(Age[!is.na(Age)]), na.rm = TRUE),
                  Age)
    ) %>%
    dplyr::left_join(
      historical_df %>%
        dplyr::group_by(ID) %>%
        dplyr::summarise(
          Historical_Max = max(Pct_of_Max_Points, na.rm = TRUE),
          First_Season = min(Season),
          .groups = 'drop'
        ),
      by = "ID"
    ) %>%
    # Filter for skiers who haven't broken through yet
    dplyr::filter(Historical_Max < 0.5 | is.na(Historical_Max)) %>%
    dplyr::distinct(ID, .keep_all = TRUE)
  
  # Create prediction data by mapping current Pelo values to prev_ names
  prediction_data <- current_candidates
  for(prev_var in names(predictor_mapping)) {
    if(prev_var %in% best_predictors) {
      pelo_var <- predictor_mapping[prev_var]
      prediction_data[[prev_var]] <- prediction_data[[pelo_var]]
    }
  }
  
  # Print NA diagnostics for the mapped predictors
  print("\nChecking for NAs in predictors:")
  for(pred in best_predictors) {
    print(paste(pred, "NAs:", sum(is.na(prediction_data[[pred]]))))
  }
  
  # Handle any remaining NAs in predictors
  for(pred in best_predictors) {
    if(sum(is.na(prediction_data[[pred]])) > 0) {
      pred_median <- median(prediction_data[[pred]], na.rm = TRUE)
      prediction_data[[pred]] <- ifelse(is.na(prediction_data[[pred]]),
                                      pred_median,
                                      prediction_data[[pred]])
    }
  }
  
  # Use the reduced model from our analysis to make predictions
  predictions <- predict(predictor_analysis$reduced_model,
                       newdata = prediction_data,
                       type = "prob")
  
  # Combine predictions with skier information
  results <- prediction_data %>%
    dplyr::select(Skier, Nation, dplyr::all_of(best_predictors),
                 Pct_of_Max_Points, Historical_Max) %>%
    dplyr::mutate(
      Breakthrough_Prob = predictions[,"Yes"],
      # Calculate improvement needed
      Points_To_Threshold = 0.5 - Pct_of_Max_Points,
      # Categorize likelihood
      Likelihood = dplyr::case_when(
        Breakthrough_Prob >= 0.75 ~ "Very High",
        Breakthrough_Prob >= 0.5 ~ "High",
        Breakthrough_Prob >= 0.25 ~ "Medium",
        Breakthrough_Prob >= 0.1 ~ "Low",
        TRUE ~ "Very Low"
      ),
      # Add model confidence
      Model_Confidence = dplyr::case_when(
        abs(Breakthrough_Prob - 0.5) >= 0.4 ~ "High",
        abs(Breakthrough_Prob - 0.5) >= 0.2 ~ "Medium",
        TRUE ~ "Low"
      )
    ) %>%
    dplyr::arrange(desc(Breakthrough_Prob))
  
  # Create visualization
  p1 <- ggplot(results,
               aes(x = Pct_of_Max_Points, y = Breakthrough_Prob)) +
    geom_point(aes(color = Likelihood, size = Model_Confidence)) +
    geom_text_repel(
      data = results %>% dplyr::filter(Breakthrough_Prob >= 0.25),
      aes(label = Skier),
      size = 3
    ) +
    geom_vline(xintercept = 0.5, linetype = "dashed", color = "red") +
    labs(title = "2025 Breakthrough Predictions",
         subtitle = "Red line indicates breakthrough threshold (0.5)",
         x = "Current Points (% of Max)",
         y = "Predicted Breakthrough Probability") +
    theme_minimal()
  
  # Create predictor comparison plot based on top two predictors
  if(length(best_predictors) >= 2) {
    top2_predictors <- best_predictors[1:2]
    p2 <- ggplot(results,
                 aes_string(x = top2_predictors[1], y = top2_predictors[2])) +
      geom_point(aes(color = Likelihood, size = Breakthrough_Prob)) +
      geom_text_repel(
        data = results %>% dplyr::filter(Breakthrough_Prob >= 0.25),
        aes(label = Skier),
        size = 3
      ) +
      labs(title = "Top Predictors Comparison",
           subtitle = "Size indicates breakthrough probability") +
      theme_minimal()
  } else {
    p2 <- NULL
  }
  
  # Print summary statistics
  print("\nBreakthrough Candidates Summary:")
  print(table(results$Likelihood))
  
  # Calculate average predictor values for each likelihood category
  predictor_means <- results %>%
    dplyr::group_by(Likelihood) %>%
    dplyr::summarise(across(dplyr::all_of(best_predictors),
                           list(mean = mean),
                           na.rm = TRUE))
  
  return(list(
    predictions = results,
    predictor_means = predictor_means,
    probability_plot = p1,
    predictor_plot = p2
  ))
}

# Make predictions
predictions <- predict_breakthroughs(df, df_race, predictor_analysis)

# Save detailed results
write.xlsx(list(
  Predictions = predictions$predictions %>%
    dplyr::select(Skier, Nation, Pct_of_Max_Points, Historical_Max,
                  Breakthrough_Prob, Points_To_Threshold, 
                  Likelihood, Model_Confidence,
                  #c("Prev_Pelo", "Prev_Distance", "Prev_Sprint", "Age", "Exp")),
                  all_of(predictor_analysis$top_predictors)),
  Predictor_Means = predictions$predictor_means,
  Model_Info = data.frame(
    Predictors_Used = paste(predictor_analysis$top_predictors, collapse = ", "),
    Model_ROC = predictor_analysis$model_comparison$ROC[2]
  )
), 
file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_breakthrough-predictions-2025-final.xlsx",
sheetName = c("Predictions", "Predictor_Means", "Model_Info"))


# Display plots
print(predictions$probability_plot)
if(!is.null(predictions$predictor_plot)) print(predictions$predictor_plot)
```

### Best Newcomers/Young Skiers


### Over the Hill?

## How will Johaug do?  How would Bolshunov do?


### Similarity Scores



```{r df82}
# Step Two: Create a column called WC Points that maps place to world cup points from a list
wc_points <- c(100,95,90,85,80,75,72,69,66,63,60,58,56,54,52,50,48,46,44,42,40,38,36,34,32,30,28,26,24,22,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1)
stage_points <- c(50, 47, 44, 41, 38, 35, 32, 30, 28, 26, 24, 22, 20, 18, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1)
tds_points <- c(300, 285, 270, 255, 240, 216, 207, 198, 189, 180, 174, 168, 162, 156, 150, 144, 138, 132, 126, 120, 114, 108, 102, 96, 90, 84, 78, 72, 66, 60, 57, 54, 51, 48, 45, 42, 39, 36, 33, 30, 27, 24, 21, 18, 15, 12, 9, 6, 3)

# Function to safely fetch points based on Place
get_points <- function(place, points_list) {
  if (place >= 1 && place <= length(points_list)) {
    return(points_list[place])
  }
  return(0)
}

# Apply points logic based on Event and Distance
df82 <- M_chrono %>%
  mutate(Points = case_when(
    Event == "Tour de Ski" & Distance == "Stage" ~ map_int(Place, ~ get_points(.x, tds_points)),
    Event == "Tour de Ski" ~ map_int(Place, ~ get_points(.x, stage_points)),
    TRUE ~ map_int(Place, ~ get_points(.x, wc_points))
  ))

# Sort the df by Date, Race, and Place
df82 <- df82 %>%
  arrange(Date, Race, Place)

# Step Three: Filter for the last five years
df82 <- df82 %>%
  filter(Event %in% c("Offseason", "World Cup", "Nordic Opening", "Tour de Ski",  "World Cup Final", "Ski Tour Canada")) %>%
  group_by(ID, Season) %>%
  mutate(Cumulative_Points = cumsum(Points)) %>%
  ungroup()

# df82 <- df82 %>%
#   group_by(ID, Season) %>%
#   mutate(Races_in_Season = n()) %>%
#   ungroup() %>%
#   filter(Races_in_Season >= 10)

# Function to replace NAs with the first quartile within each group
replace_na_with_quartile <- function(x) {
  quartile_1 <- quantile(x, 0.25, na.rm = TRUE)  # Calculate the first quartile, ignoring NAs
  ifelse(is.na(x), quartile_1, x)  # Replace NAs with the first quartile
}

# Apply this logic to each of the specified columns and then perform the percentage calculations
df82 <- df82 %>%
  group_by(Season, Race) %>%
  mutate(
    Distance_Pelo = replace_na_with_quartile(Distance_Pelo),
    Distance_C_Pelo = replace_na_with_quartile(Distance_C_Pelo),
    Distance_F_Pelo = replace_na_with_quartile(Distance_F_Pelo),
    Pelo = replace_na_with_quartile(Pelo),
    Sprint_Pelo = replace_na_with_quartile(Sprint_Pelo),
    Sprint_C_Pelo = replace_na_with_quartile(Sprint_C_Pelo),
    Sprint_F_Pelo = replace_na_with_quartile(Sprint_F_Pelo),
    Freestyle_Pelo = replace_na_with_quartile(Freestyle_Pelo),
    Classic_Pelo = replace_na_with_quartile(Classic_Pelo)
  ) %>%
  ungroup()
df82 <- df82 %>%
  filter(! Distance %in% c("Ts", "Rel"))


# Step One: Filter for first place
first_place <- df82 %>%
  filter(Place == 1)

# Step Two: Group by Season and calculate total points for first place
max_points_per_season <- first_place %>%
  group_by(Season) %>%
  summarise(Max_Points = sum(Points), .groups = 'drop')

# Step Three: Join the max points back to the original data frame
df82 <- df82 %>%
  left_join(max_points_per_season, by = "Season")

# Step Four: Calculate percentage of maximum points
df82 <- df82 %>%
  mutate(Pct_of_Max_Points = Cumulative_Points / Max_Points)


# Step Four: Set up the explanatory variables
# Filter for the offseason
elo_df <- df82 %>%
  filter(Event == "Offseason") %>%
  arrange(ID, Season) %>%
  group_by(ID) %>%
  mutate(Prev_Pelo = lag(Pelo)) %>%
  ungroup()



elo_df82 <- df82 %>%
  filter(Event == "Offseason") %>%
  arrange(ID, Season) %>%
  group_by(ID) %>%
  mutate(Prev_Pelo = lag(Pelo), Prev_Sprint=lag(Sprint_Pelo), Prev_Sprint_C=lag(Sprint_C_Pelo), Prev_Sprint_F=lag(Sprint_F_Pelo), Prev_Distance=lag(Distance_Pelo), Prev_Distance_F=lag(Distance_F_Pelo), Prev_Distance_C=lag(Distance_C_Pelo), Prev_F=lag(Freestyle_Pelo), Prev_C=lag(Classic_Pelo), Prev_Pct_of_Max_Points=lag(Pct_of_Max_Points)) %>%
  ungroup()





elo_df82 <- elo_df82 %>%
  filter(Season>1981)

df82 <- elo_df82

df82 <- df82 %>%
  group_by(Season) %>%
  mutate(
    Prev_Distance= replace_na_with_quartile(Prev_Distance),
    Prev_Distance_C = replace_na_with_quartile(Prev_Distance_C),
    Prev_Distance_F = replace_na_with_quartile(Prev_Distance_F),
    Prev_Pelo = replace_na_with_quartile(Prev_Pelo),
    Prev_Sprint = replace_na_with_quartile(Prev_Sprint),
    Prev_Sprint_C = replace_na_with_quartile(Prev_Sprint_C),
    Prev_Sprint_F = replace_na_with_quartile(Prev_Sprint_F),
    Prev_F = replace_na_with_quartile(Prev_F),
    Prev_C = replace_na_with_quartile(Prev_C),
    Prev_Pct_of_Max_Points = replace(Prev_Pct_of_Max_Points, is.na(Prev_Pct_of_Max_Points), 0)
  ) %>%
  ungroup()

```      



```{r simple-comp}
# # Helper function for trajectory similarity
# calculate_trajectory_similarity <- function(traj1, traj2) {
#   # Remove NAs from both trajectories
#   valid_indices1 <- !is.na(traj1)
#   valid_indices2 <- !is.na(traj2)
#   traj1 <- traj1[valid_indices1]
#   traj2 <- traj2[valid_indices2]
#   
#   # Check if we have enough valid data points
#   if(length(traj1) < 2 || length(traj2) < 2) {
#     return(0)  # Return 0 similarity if insufficient data
#   }
#   
#   # Interpolate to same length for correlation
#   max_length <- max(length(traj1), length(traj2))
#   
#   # Safe interpolation with error handling
#   tryCatch({
#     traj1_interp <- approx(seq_along(traj1), traj1, n = max_length)$y
#     traj2_interp <- approx(seq_along(traj2), traj2, n = max_length)$y
#     
#     # Check for constant trajectories
#     sd1 <- sd(traj1_interp)
#     sd2 <- sd(traj2_interp)
#     
#     # Calculate correlation on interpolated data
#     cor_score <- if(sd1 == 0 || sd2 == 0) {
#       if(mean(traj1_interp) == mean(traj2_interp)) {
#         1  # Perfect correlation for identical constant trajectories
#       } else {
#         0  # No correlation for different constant trajectories
#       }
#     } else {
#       suppressWarnings(cor(traj1_interp, traj2_interp, use = "complete.obs"))
#     }
#     
#     if(is.na(cor_score)) cor_score <- 0
#     
#     # Calculate level similarity
#     level_diff <- mean(abs(traj1_interp - traj2_interp), na.rm = TRUE)
#     max_val <- max(abs(c(traj1_interp, traj2_interp)), na.rm = TRUE)
#     level_score <- 1 - (level_diff / max_val)
#     if(is.na(level_score)) level_score <- 0
#     
#     # Final similarity score (simplified version without DTW)
#     similarity <- (cor_score + level_score) / 2
#     
#     # Scale to 0-100 for interpretability
#     similarity <- (similarity + 1) * 50
#     
#     # Handle any remaining NAs
#     if(is.na(similarity)) similarity <- 0
#     
#     return(similarity)
#   }, error = function(e) {
#     return(0)  # Return 0 similarity if interpolation fails
#   })
# }
# 
# 
# 
# 
# # Simplified comparison function
# compare_two_skiers <- function(df, skier1_name, skier2_name) {
#   # Extract careers for both skiers
#   careers <- df %>%
#     dplyr::arrange(ID, Age) %>%
#     dplyr::group_by(ID) %>%
#     dplyr::mutate(
#       Age_Clean = as.numeric(Age),
#       Age_Clean = ifelse(is.na(Age_Clean), 
#                         median(Age_Clean, na.rm = TRUE),
#                         Age_Clean)
#     ) %>%
#     dplyr::filter(Skier %in% c(skier1_name, skier2_name)) %>%
#     dplyr::summarise(
#       Name = first(Skier),
#       Career_Start = min(Season),
#       Career_End = max(Season),
#       Career_Length = n_distinct(Season),
#       Peak_Points = max(Pct_of_Max_Points, na.rm = TRUE),
#       Peak_Elo = max(Pelo, na.rm = TRUE),
#       Age_Trajectory = list(Age_Clean),
#       Elo_Trajectory = list(Pelo),
#       Points_Trajectory = list(Pct_of_Max_Points),
#       .groups = 'drop'
#     )
#   
#   # Extract trajectories
#   skier1 <- careers %>% filter(Name == skier1_name)
#   skier2 <- careers %>% filter(Name == skier2_name)
#   
#   # Calculate similarities
#   elo_sim <- calculate_trajectory_similarity(
#     unlist(skier1$Elo_Trajectory),
#     unlist(skier2$Elo_Trajectory)
#   )
#   
#   points_sim <- calculate_trajectory_similarity(
#     unlist(skier1$Points_Trajectory),
#     unlist(skier2$Points_Trajectory)
#   )
#   
#   overall_sim <- (elo_sim + points_sim) / 2
#   
#   # Create comparison plots
#   elo_plot <- ggplot() +
#     geom_line(data = data.frame(
#       Age = unlist(skier1$Age_Trajectory),
#       Elo = unlist(skier1$Elo_Trajectory)
#     ), aes(x = Age, y = Elo, color = skier1_name)) +
#     geom_line(data = data.frame(
#       Age = unlist(skier2$Age_Trajectory),
#       Elo = unlist(skier2$Elo_Trajectory)
#     ), aes(x = Age, y = Elo, color = skier2_name)) +
#     labs(title = "Elo Rating Trajectories Comparison",
#          x = "Age", y = "Elo Rating") +
#     theme_minimal()
#   
#   points_plot <- ggplot() +
#     geom_line(data = data.frame(
#       Age = unlist(skier1$Age_Trajectory),
#       Points = unlist(skier1$Points_Trajectory)
#     ), aes(x = Age, y = Points, color = skier1_name)) +
#     geom_line(data = data.frame(
#       Age = unlist(skier2$Age_Trajectory),
#       Points = unlist(skier2$Points_Trajectory)
#     ), aes(x = Age, y = Points, color = skier2_name)) +
#     labs(title = "Points Trajectories Comparison",
#          x = "Age", y = "Points (% of Max)") +
#     theme_minimal()
#   
#   # Create career summary
#   career_summary <- bind_rows(
#     skier1 %>% dplyr::select(Name, Career_Start, Career_End, Career_Length, Peak_Points, Peak_Elo),
#     skier2 %>% dplyr::select(Name, Career_Start, Career_End, Career_Length, Peak_Points, Peak_Elo)
#   )
#   
#   return(list(
#     similarity_scores = list(
#       elo_similarity = elo_sim,
#       points_similarity = points_sim,
#       overall_similarity = overall_sim
#     ),
#     career_summary = career_summary,
#     plots = list(
#       elo_plot = elo_plot,
#       points_plot = points_plot
#     )
#   ))
# }
# 
# # Run comparison
# comparison <- compare_two_skiers(df82, 
#                                "Erik Valnes", 
#                                "Therese Johaug")
# 
# # Print results
# print("Similarity Scores (0-100 scale):")
# print(paste("Elo Similarity:", round(comparison$similarity_scores$elo_similarity, 1)))
# print(paste("Points Similarity:", round(comparison$similarity_scores$points_similarity, 1)))
# print(paste("Overall Similarity:", round(comparison$similarity_scores$overall_similarity, 1)))
# 
# print("\nCareer Summary:")
# print(comparison$career_summary)
# 
# # Display plots
# print(comparison$plots$elo_plot)
# print(comparison$plots$points_plot)

```


```{r simple-dtw}
# Function to compare two skiers using DTW
compare_skiers_dtw <- function(df, skier1_name, skier2_name) {
  # Extract careers for both skiers
  careers <- df %>%
    dplyr::arrange(ID, Age) %>%
    dplyr::group_by(ID) %>%
    dplyr::mutate(
      Age_Clean = suppressWarnings(as.numeric(Age)),
      Age_Clean = ifelse(is.na(Age_Clean), 
                        median(Age_Clean, na.rm = TRUE),
                        Age_Clean)
    ) %>%
    dplyr::filter(Skier %in% c(skier1_name, skier2_name)) %>%
    dplyr::summarise(
      Name = first(Skier),
      Career_Start = min(Season),
      Career_End = max(Season),
      Career_Length = n_distinct(Season),
      Peak_Elo = max(Pelo, na.rm = TRUE),
      Age_Trajectory = list(Age_Clean),
      Elo_Trajectory = list(Pelo),
      .groups = 'drop'
    )
  
  # Extract trajectories
  skier1 <- careers %>% filter(Name == skier1_name)
  skier2 <- careers %>% filter(Name == skier2_name)
  
  # Clean trajectories (remove NAs and align by age)
  elo1 <- unlist(skier1$Elo_Trajectory)
  elo2 <- unlist(skier2$Elo_Trajectory)
  age1 <- unlist(skier1$Age_Trajectory)
  age2 <- unlist(skier2$Age_Trajectory)
  
  # Remove NAs
  valid1 <- !is.na(elo1) & !is.na(age1)
  valid2 <- !is.na(elo2) & !is.na(age2)
  
  elo1 <- elo1[valid1]
  elo2 <- elo2[valid2]
  age1 <- age1[valid1]
  age2 <- age2[valid2]
  
  # Calculate DTW
  dtw_result <- dtw(elo1, elo2, keep = TRUE)
  
  # Create visualization
  matches <- data.frame(
    x1 = age1[dtw_result$index1],
    x2 = age2[dtw_result$index2],
    y1 = elo1[dtw_result$index1],
    y2 = elo2[dtw_result$index2]
  )
  
  # Create comparison plot
  dtw_plot <- ggplot() +
    # Plot original trajectories
    geom_line(data = data.frame(Age = age1, Elo = elo1),
              aes(x = Age, y = Elo, color = skier1_name), size = 1) +
    geom_line(data = data.frame(Age = age2, Elo = elo2),
              aes(x = Age, y = Elo, color = skier2_name), size = 1) +
    # Add DTW matching lines
    geom_segment(data = matches,
                aes(x = x1, y = y1, xend = x2, yend = y2),
                color = "gray", alpha = 0.3) +
    labs(title = "Career Trajectory Comparison with DTW Matching",
         subtitle = paste(skier1_name, "vs", skier2_name),
         x = "Age", 
         y = "Elo Rating",
         color = "Skier") +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  # Calculate normalized DTW distance
  max_dist <- max(abs(diff(range(c(elo1, elo2))))) * max(length(elo1), length(elo2))
  normalized_dtw_dist <- dtw_result$distance / max_dist
  similarity_score <- (1 - normalized_dtw_dist) * 100
  
  # Create career phase analysis
  phase_matches <- data.frame(
    Age1 = age1[dtw_result$index1],
    Age2 = age2[dtw_result$index2],
    Elo1 = elo1[dtw_result$index1],
    Elo2 = elo2[dtw_result$index2]
  ) %>%
    mutate(
      Phase = case_when(
        Elo1 >= lag(Elo1, default = first(Elo1)) & 
          Elo2 >= lag(Elo2, default = first(Elo2)) ~ "Rising",
        Elo1 <= lag(Elo1, default = first(Elo1)) & 
          Elo2 <= lag(Elo2, default = first(Elo2)) ~ "Declining",
        TRUE ~ "Stable"
      )
    )
  
  return(list(
    similarity_score = similarity_score,
    dtw_plot = dtw_plot,  # Changed from plot to dtw_plot
    career_summary = careers,
    phase_analysis = phase_matches,
    dtw_matches = matches
  ))
}

# # Run the analysis
# comparison <- compare_skiers_dtw(df82, 
#                                "Johannes Høsflot Klæbo", 
#                                "Petter Northug")
# 
# # Print results
# cat("\nCareer Similarity Score (0-100):", round(comparison$similarity_score, 1), "\n")
# 
# # Print career summary
# print("\nCareer Summary:")
# print(comparison$career_summary)
# 
# # Display the plot
# print(comparison$dtw_plot)  # Changed from plots to dtw_plot
# 
# # Show some key matching points
# cat("\nKey Career Phase Matches:\n")
# print(head(comparison$phase_analysis %>% 
#      dplyr::select(Age1, Age2, Phase) %>% 
#      distinct(), 10))
```


```{r most-common5-dtw}
# Function to plot a single metric comparison
plot_metric <- function(ref_skier, comp_skier, metric_name, metric_col, ref_name) {
    ref_data <- data.frame(
      Age = unlist(ref_skier$Age_Trajectory),
      Value = unlist(ref_skier[[metric_col]])
    ) %>% filter(!is.na(Value))
    
    comp_data <- data.frame(
      Age = unlist(comp_skier$Age_Trajectory),
      Value = unlist(comp_skier[[metric_col]])
    ) %>% filter(!is.na(Value))
    
    ggplot() +
      geom_line(data = ref_data, aes(x = Age, y = Value, color = ref_name), size = 1) +
      geom_line(data = comp_data, aes(x = Age, y = Value, color = comp_skier$Name), size = 1) +
      labs(title = paste(metric_name, "Comparison"),
           subtitle = paste(ref_name, "vs", comp_skier$Name),
           x = "Age", y = metric_name,
           color = "Skier") +
      theme_minimal() +
      theme(legend.position = "bottom")
}

# Main function
find_similar_careers_multi <- function(df, reference_skier, n_similar = 5,
                                     weights = c(0.3, 0.3, 0.2, 0.2)) {
    # Validate weights
    if(abs(sum(weights) - 1) > 0.001) {
        stop("Weights must sum to 1")
    }
    
    # Extract careers
    all_careers <- df %>%
        dplyr::arrange(ID, Age) %>%
        dplyr::group_by(ID) %>%
        dplyr::mutate(
            Age_Clean = suppressWarnings(as.numeric(Age)),
            Age_Clean = ifelse(is.na(Age_Clean),
                             median(Age_Clean, na.rm = TRUE),
                             Age_Clean)
        ) %>%
        dplyr::summarise(
            Name = first(Skier),
            Career_Start = min(Season),
            Career_End = max(Season),
            Career_Length = n_distinct(Season),
            Peak_Overall_Elo = ifelse(all(is.na(Pelo)), NA, max(Pelo, na.rm = TRUE)),
            Peak_Distance_Elo = ifelse(all(is.na(Prev_Distance)), NA, max(Prev_Distance, na.rm = TRUE)),
            Peak_Sprint_Elo = ifelse(all(is.na(Prev_Sprint)), NA, max(Prev_Sprint, na.rm = TRUE)),
            Peak_Points = ifelse(all(is.na(Pct_of_Max_Points)), NA, max(Pct_of_Max_Points, na.rm = TRUE)),
            Age_Trajectory = list(Age_Clean),
            Overall_Elo_Trajectory = list(Pelo),
            Distance_Elo_Trajectory = list(Prev_Distance),
            Sprint_Elo_Trajectory = list(Prev_Sprint),
            Points_Trajectory = list(Pct_of_Max_Points),
            .groups = 'drop'
        ) %>%
        filter(!is.na(Peak_Overall_Elo) | !is.na(Peak_Distance_Elo) |
               !is.na(Peak_Sprint_Elo) | !is.na(Peak_Points))
    
    # Get reference skier
    ref_skier <- all_careers %>% filter(Name == reference_skier)
    
    if(nrow(ref_skier) == 0) {
        stop(paste("Reference skier", reference_skier, "not found in dataset"))
    }
    
    # DTW similarity calculation function
    calculate_dtw_similarity <- function(ref_traj, comp_traj) {
        ref_clean <- unlist(ref_traj)
        comp_clean <- unlist(comp_traj)
        
        valid_ref <- !is.na(ref_clean)
        valid_comp <- !is.na(comp_clean)
        
        ref_clean <- ref_clean[valid_ref]
        comp_clean <- comp_clean[valid_comp]
        
        if(length(ref_clean) < 2 || length(comp_clean) < 2) return(0)
        
        tryCatch({
            dtw_result <- dtw(ref_clean, comp_clean, keep = TRUE)
            max_dist <- max(abs(diff(range(c(ref_clean, comp_clean))))) *
                max(length(ref_clean), length(comp_clean))
            normalized_dist <- dtw_result$distance / max_dist
            return((1 - normalized_dist) * 100)
        }, error = function(e) {
            return(0)
        })
    }
    
    # Calculate similarities
    similarities <- all_careers %>%
        filter(Name != reference_skier) %>%
        mutate(
            Overall_Elo_Similarity = map2_dbl(Overall_Elo_Trajectory,
                                           ref_skier$Overall_Elo_Trajectory,
                                           calculate_dtw_similarity),
            Distance_Elo_Similarity = map2_dbl(Distance_Elo_Trajectory,
                                            ref_skier$Distance_Elo_Trajectory,
                                            calculate_dtw_similarity),
            Sprint_Elo_Similarity = map2_dbl(Sprint_Elo_Trajectory,
                                          ref_skier$Sprint_Elo_Trajectory,
                                          calculate_dtw_similarity),
            Points_Similarity = map2_dbl(Points_Trajectory,
                                      ref_skier$Points_Trajectory,
                                      calculate_dtw_similarity),
            Weighted_Similarity = Overall_Elo_Similarity * weights[1] +
                Points_Similarity * weights[2] +
                Distance_Elo_Similarity * weights[3] +
                Sprint_Elo_Similarity * weights[4]
        ) %>%
        arrange(desc(Weighted_Similarity))
    
    # Get top similar skiers
    top_similar <- similarities %>%
        slice_head(n = n_similar)
    
    # Generate plots
    plots <- list()
    for(i in 1:nrow(top_similar)) {
        similar_skier <- top_similar$Name[i]
        comp_skier <- all_careers %>% filter(Name == similar_skier)
        
        plots[[similar_skier]] <- list(
            overall_elo = plot_metric(ref_skier, comp_skier, "Overall Elo", "Overall_Elo_Trajectory", reference_skier),
            distance_elo = plot_metric(ref_skier, comp_skier, "Distance Elo", "Distance_Elo_Trajectory", reference_skier),
            sprint_elo = plot_metric(ref_skier, comp_skier, "Sprint Elo", "Sprint_Elo_Trajectory", reference_skier),
            points = plot_metric(ref_skier, comp_skier, "Points", "Points_Trajectory", reference_skier)
        )
    }
    
    # Return results
    return(list(
        reference_skier = reference_skier,  # Return full reference skier data
        similar_skiers = top_similar %>%
            dplyr::select(Name, Career_Start, Career_End, Career_Length,
                   Peak_Overall_Elo, Peak_Distance_Elo, Peak_Sprint_Elo, Peak_Points,
                   Overall_Elo_Similarity, Distance_Elo_Similarity,
                   Sprint_Elo_Similarity, Points_Similarity,
                   Weighted_Similarity),
        comparison_plots = plots
    ))
}

# # Run the analysis
# similar_careers <- find_similar_careers_multi(df82, "Petter Northug", 5)
# 
# # Print results
# #cat("\nMost Similar Careers to", similar_careers$reference_skier$Name, ":\n")
# print(similar_careers$similar_skiers %>%
#       dplyr::select(Name, Weighted_Similarity, Overall_Elo_Similarity,
#              Distance_Elo_Similarity, Sprint_Elo_Similarity, Points_Similarity))
# 
# # Display plots for each similar skier
# for(skier in names(similar_careers$comparison_plots)) {
#     cat("\nComparison plots for", skier, ":\n")
#     print(similar_careers$comparison_plots[[skier]]$overall_elo)
#     print(similar_careers$comparison_plots[[skier]]$distance_elo)
#     print(similar_careers$comparison_plots[[skier]]$sprint_elo)
#     print(similar_careers$comparison_plots[[skier]]$points)
# }
```



### Age and Elo
```{r age-exploration}
# Function to create both plots with individual skier overlay
create_elo_progression_plots <- function(df82, skier_name = NULL) {
  # First identify skiers who reach 100+ experience
  experienced_skiers <- df82 %>%
    group_by(Skier) %>%
    summarize(Max_Exp = max(Exp)) %>%
    filter(Max_Exp >= 100) %>%
    dplyr::select(Skier)
  
  # Calculate population averages
  elo_changes <- df82 %>%
    inner_join(experienced_skiers, by = "Skier") %>%
    mutate(
      Age_Clean = floor(as.numeric(Age))
    ) %>%
    mutate(
      Distance_Change = Distance_Pelo - Prev_Distance,
      Sprint_Change = Sprint_Pelo - Prev_Sprint,
      Classic_Change = Classic_Pelo - Prev_C,
      Freestyle_Change = Freestyle_Pelo - Prev_F,
      Distance_Pct_Change = (Distance_Pelo - Prev_Distance) / Prev_Distance * 100,
      Sprint_Pct_Change = (Sprint_Pelo - Prev_Sprint) / Prev_Sprint * 100,
      Classic_Pct_Change = (Classic_Pelo - Prev_C) / Prev_C * 100,
      Freestyle_Pct_Change = (Freestyle_Pelo - Prev_F) / Prev_F * 100
    ) %>%
    group_by(Age_Clean) %>%
    summarize(
      Avg_Distance_Change = mean(Distance_Change, na.rm = TRUE),
      Avg_Sprint_Change = mean(Sprint_Change, na.rm = TRUE),
      Avg_Classic_Change = mean(Classic_Change, na.rm = TRUE),
      Avg_Freestyle_Change = mean(Freestyle_Change, na.rm = TRUE),
      Avg_Distance_Pct = mean(Distance_Pct_Change, na.rm = TRUE),
      Avg_Sprint_Pct = mean(Sprint_Pct_Change, na.rm = TRUE),
      Avg_Classic_Pct = mean(Classic_Pct_Change, na.rm = TRUE),
      Avg_Freestyle_Pct = mean(Freestyle_Pct_Change, na.rm = TRUE),
      n = n()
    ) %>%
    filter(!is.na(Age_Clean), 
           n >= 30,
           Age_Clean >= 15,
           Age_Clean <= 40)
  
  # If skier_name is provided, get their progression
  if (!is.null(skier_name)) {
    skier_progression <- df82 %>%
      filter(Skier == skier_name) %>%
      mutate(
        Age_Clean = floor(as.numeric(Age)),
        Distance_Change = Distance_Pelo - Prev_Distance,
        Sprint_Change = Sprint_Pelo - Prev_Sprint,
        Classic_Change = Classic_Pelo - Prev_C,
        Freestyle_Change = Freestyle_Pelo - Prev_F,
        Distance_Pct_Change = (Distance_Pelo - Prev_Distance) / Prev_Distance * 100,
        Sprint_Pct_Change = (Sprint_Pelo - Prev_Sprint) / Prev_Sprint * 100,
        Classic_Pct_Change = (Classic_Pelo - Prev_C) / Prev_C * 100,
        Freestyle_Pct_Change = (Freestyle_Pelo - Prev_F) / Prev_F * 100
      )
  }
  
  # Create absolute change visualization
  p1 <- ggplot(elo_changes, aes(x = Age_Clean)) +
    geom_line(aes(y = Avg_Distance_Change, color = "Distance (Population)"), size = 1) +
    geom_line(aes(y = Avg_Sprint_Change, color = "Sprint (Population)"), size = 1) +
    geom_line(aes(y = Avg_Classic_Change, color = "Classic (Population)"), size = 1) +
    geom_line(aes(y = Avg_Freestyle_Change, color = "Freestyle (Population)"), size = 1)
  
  # Add individual skier's lines if provided
  if (!is.null(skier_name)) {
    p1 <- p1 +
      geom_line(data = skier_progression, aes(y = Distance_Change, color = "Distance (Skier)"), 
                linetype = "dashed", size = 1) +
      geom_line(data = skier_progression, aes(y = Sprint_Change, color = "Sprint (Skier)"), 
                linetype = "dashed", size = 1) +
      geom_line(data = skier_progression, aes(y = Classic_Change, color = "Classic (Skier)"), 
                linetype = "dashed", size = 1) +
      geom_line(data = skier_progression, aes(y = Freestyle_Change, color = "Freestyle (Skier)"), 
                linetype = "dashed", size = 1)
  }
  
  p1 <- p1 +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    scale_x_continuous(breaks = seq(15, 40, by = 1)) +
    labs(
      title = paste("Average ELO Rating Changes by Age\n(Population vs", 
                   if(!is.null(skier_name)) skier_name else "Population", ")"),
      x = "Age",
      y = "Average Year-over-Year ELO Change",
      color = "Metric"
    ) +
    theme_minimal() +
    scale_color_manual(values = c(
      "Distance (Population)" = "#4477AA",
      "Sprint (Population)" = "#66CCEE",
      "Classic (Population)" = "#228833",
      "Freestyle (Population)" = "#CCBB44",
      "Distance (Skier)" = "#AA3377",
      "Sprint (Skier)" = "#EE6677",
      "Classic (Skier)" = "#44AA99",
      "Freestyle (Skier)" = "#999933"
    )) +
    theme(
      legend.position = "bottom",
      plot.title = element_text(hjust = 0.5, size = 14),
      axis.title = element_text(size = 12),
      legend.title = element_text(size = 12)
    )
  
  # Create percentage change visualization
  p2 <- ggplot(elo_changes, aes(x = Age_Clean)) +
    geom_line(aes(y = Avg_Distance_Pct, color = "Distance (Population)"), size = 1) +
    geom_line(aes(y = Avg_Sprint_Pct, color = "Sprint (Population)"), size = 1) +
    geom_line(aes(y = Avg_Classic_Pct, color = "Classic (Population)"), size = 1) +
    geom_line(aes(y = Avg_Freestyle_Pct, color = "Freestyle (Population)"), size = 1)
  
  # Add individual skier's percentage lines if provided
  if (!is.null(skier_name)) {
    p2 <- p2 +
      geom_line(data = skier_progression, aes(y = Distance_Pct_Change, color = "Distance (Skier)"), 
                linetype = "dashed", size = 1) +
      geom_line(data = skier_progression, aes(y = Sprint_Pct_Change, color = "Sprint (Skier)"), 
                linetype = "dashed", size = 1) +
      geom_line(data = skier_progression, aes(y = Classic_Pct_Change, color = "Classic (Skier)"), 
                linetype = "dashed", size = 1) +
      geom_line(data = skier_progression, aes(y = Freestyle_Pct_Change, color = "Freestyle (Skier)"), 
                linetype = "dashed", size = 1)
  }
  
  p2 <- p2 +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    scale_x_continuous(breaks = seq(15, 40, by = 1)) +
    labs(
      title = paste("Percentage ELO Rating Changes by Age\n(Population vs", 
                   if(!is.null(skier_name)) skier_name else "Population", ")"),
      x = "Age",
      y = "Average Year-over-Year ELO Change (%)",
      color = "Metric"
    ) +
    theme_minimal() +
    scale_color_manual(values = c(
      "Distance (Population)" = "#4477AA",
      "Sprint (Population)" = "#66CCEE",
      "Classic (Population)" = "#228833",
      "Freestyle (Population)" = "#CCBB44",
      "Distance (Skier)" = "#AA3377",
      "Sprint (Skier)" = "#EE6677",
      "Classic (Skier)" = "#44AA99",
      "Freestyle (Skier)" = "#999933"
    )) +
    theme(
      legend.position = "bottom",
      plot.title = element_text(hjust = 0.5, size = 14),
      axis.title = element_text(size = 12),
      legend.title = element_text(size = 12)
    )
  
  return(list(absolute = p1, percentage = p2))
}

# Example usage:
# For population only:
plots <- create_elo_progression_plots(df82)
print(plots$absolute)
print(plots$percentage)

# For specific skier (replace "Skier Name" with actual name):
 plots <- create_elo_progression_plots(df82, "Therese Johaug")
 print(plots$absolute)
 print(plots$percentage)
```
#### Age Adjusted Predictions
```{r age-adjusted-2025}
# First, get the average ELO changes by age
elo_age_changes <- df82 %>%
  # Calculate Max_Exp for each skier first
  group_by(Skier) %>%
  mutate(Max_Exp = max(Exp)) %>%
  filter(Max_Exp >= 100) %>%  # Keep experienced skiers
  mutate(
    Age_Clean = floor(as.numeric(Age)),
    Distance_Change = Distance_Pelo - Prev_Distance,
    Sprint_Change = Sprint_Pelo - Prev_Sprint
  ) %>%
  group_by(Age_Clean) %>%
  summarize(
    Avg_Distance_Change = mean(Distance_Change, na.rm = TRUE),
    Avg_Sprint_Change = mean(Sprint_Change, na.rm = TRUE),
    n = n()
  ) %>%
  filter(!is.na(Age_Clean), 
         n >= 20,
         Age_Clean >= 15,
         Age_Clean <= 40)

# Function to predict 2025 performance
predict_2025 <- function(df_race, model_2025, elo_age_changes) {
  # Create a dataframe for 2025 predictions
  df_2025 <- df_race %>%
    # Get the most recent values for each skier
    group_by(Skier) %>%
    filter(Date == max(Date)) %>%
    ungroup() %>%
    mutate(
      Age_2025 = floor(as.numeric(Age)) + 1,  # Age in 2025
      # Look up expected ELO changes based on age
      Distance_Change = elo_age_changes$Avg_Distance_Change[match(Age_2025, elo_age_changes$Age_Clean)],
      Sprint_Change = elo_age_changes$Avg_Sprint_Change[match(Age_2025, elo_age_changes$Age_Clean)],
      # Adjust ELO values for 2025
      Distance_Pelo = Distance_Pelo + coalesce(Distance_Change, 0),
      Sprint_Pelo = Sprint_Pelo + coalesce(Sprint_Change, 0),
      # Use previous season's points as Prev_Pct_of_Max_Points
      Prev_Pct_of_Max_Points = Pct_of_Max_Points
    )
  
  # Make predictions using the adjusted values
  predictions_2025 <- predict(model_2025, newdata = df_2025, type = "response")
  
  # Add predictions to the dataframe
  df_2025$Predicted_2025_Points = predictions_2025
  
  # Sort by predicted points
  df_2025 <- df_2025 %>%
    dplyr::select(Skier, Age_2025, 
                  Distance_Pelo, Sprint_Pelo,
                  Distance_Change, Sprint_Change,
                  Prev_Pct_of_Max_Points, Predicted_2025_Points) %>%
    arrange(desc(Predicted_2025_Points))
  
  return(df_2025)
}

# Make predictions
predictions_2025 <- predict_2025(df_2025, model_2025, elo_age_changes)

# Print top 20 predictions
print("Top 20 Predicted Performers for 2025:")
print(head(predictions_2025, 20))

# Create visualization of predictions vs previous performance
ggplot(predictions_2025, aes(x = Prev_Pct_of_Max_Points, y = Predicted_2025_Points)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Predicted 2025 Performance vs Previous Season",
    x = "Previous Season Points (%)",
    y = "Predicted 2025 Points (%)"
  ) +
  theme_minimal()

# Show largest predicted improvements and declines
print("\nTop 10 Predicted Improvements:")
predictions_2025 %>%
  mutate(Change = Predicted_2025_Points - Prev_Pct_of_Max_Points) %>%
  arrange(desc(Change)) %>%
  head(10) %>%
  dplyr::select(Skier, Age_2025, Prev_Pct_of_Max_Points, Predicted_2025_Points, Change)

print("\nTop 10 Predicted Declines:")
predictions_2025 %>%
  mutate(Change = Predicted_2025_Points - Prev_Pct_of_Max_Points) %>%
  arrange(Change) %>%
  head(10) %>%
  dplyr::select(Skier, Age_2025, Prev_Pct_of_Max_Points, Predicted_2025_Points, Change)

predictions_2025_elo = predictions_2025
```

```{r age-adjusted-percent}
# Calculate average percentage changes by age
elo_age_changes <- df82 %>%
  # Calculate Max_Exp for each skier first
  group_by(Skier) %>%
  mutate(Max_Exp = max(Exp)) %>%
  filter(Max_Exp >= 100) %>%  # Keep experienced skiers
  mutate(
    Age_Clean = floor(as.numeric(Age)),
    # Calculate percentage changes instead of absolute changes
    Distance_Pct_Change = ((Distance_Pelo - Prev_Distance) / Prev_Distance) * 100,
    Sprint_Pct_Change = ((Sprint_Pelo - Prev_Sprint) / Prev_Sprint) * 100
  ) %>%
  group_by(Age_Clean) %>%
  summarize(
    Avg_Distance_Pct_Change = mean(Distance_Pct_Change, na.rm = TRUE),
    Avg_Sprint_Pct_Change = mean(Sprint_Pct_Change, na.rm = TRUE),
    n = n()
  ) %>%
  filter(!is.na(Age_Clean), 
         n >= 20,
         Age_Clean >= 15,
         Age_Clean <= 40)

# Function to predict 2025 performance
predict_2025 <- function(df_race, model_2025, elo_age_changes) {
  # Create a dataframe for 2025 predictions
  df_2025 <- df_race %>%
    # Get the most recent values for each skier
    group_by(Skier) %>%
    filter(Date == max(Date)) %>%
    ungroup() %>%
    mutate(
      Age_2025 = floor(as.numeric(Age)) + 1,  # Age in 2025
      # Look up expected percentage changes based on age
      Distance_Pct_Change = elo_age_changes$Avg_Distance_Pct_Change[match(Age_2025, elo_age_changes$Age_Clean)],
      Sprint_Pct_Change = elo_age_changes$Avg_Sprint_Pct_Change[match(Age_2025, elo_age_changes$Age_Clean)],
      # Adjust ELO values using percentage changes
      Distance_Pelo = Distance_Pelo * (1 + coalesce(Distance_Pct_Change, 0)/100),
      Sprint_Pelo = Sprint_Pelo * (1 + coalesce(Sprint_Pct_Change, 0)/100),
      # Use previous season's points as Prev_Pct_of_Max_Points
      Prev_Pct_of_Max_Points = Pct_of_Max_Points
    )
  
  # Make predictions using the adjusted values
  predictions_2025 <- predict(model_2025, newdata = df_2025, type = "response")
  
  # Add predictions to the dataframe
  df_2025$Predicted_2025_Points = predictions_2025
  
  # Sort by predicted points
  df_2025 <- df_2025 %>%
    dplyr::select(Skier, Age_2025, 
                  Distance_Pelo, Sprint_Pelo,
                  Distance_Pct_Change, Sprint_Pct_Change,
                  Prev_Pct_of_Max_Points, Predicted_2025_Points) %>%
    arrange(desc(Predicted_2025_Points))
  
  return(df_2025)
}

# Make predictions
predictions_2025 <- predict_2025(df_2025, model_2025, elo_age_changes)

# Print top 20 predictions
print("Top 20 Predicted Performers for 2025:")
print(head(predictions_2025, 20))

# Create visualization of predictions vs previous performance
ggplot(predictions_2025, aes(x = Prev_Pct_of_Max_Points, y = Predicted_2025_Points)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Predicted 2025 Performance vs Previous Season\n(Using % Changes)",
    x = "Previous Season Points (%)",
    y = "Predicted 2025 Points (%)"
  ) +
  theme_minimal()

# Show largest predicted improvements and declines
print("\nTop 10 Predicted Improvements:")
predictions_2025 %>%
  mutate(Change = Predicted_2025_Points - Prev_Pct_of_Max_Points) %>%
  arrange(desc(Change)) %>%
  head(10) %>%
  dplyr::select(Skier, Age_2025, Prev_Pct_of_Max_Points, Predicted_2025_Points, Change)

print("\nTop 10 Predicted Declines:")
predictions_2025 %>%
  mutate(Change = Predicted_2025_Points - Prev_Pct_of_Max_Points) %>%
  arrange(Change) %>%
  head(10) %>%
  dplyr::select(Skier, Age_2025, Prev_Pct_of_Max_Points, Predicted_2025_Points, Change)

# Print average percentage changes by age for reference
print("\nAverage ELO Percentage Changes by Age:")
print(elo_age_changes)

predictions_2025_pct = predictions_2025
```


```{r elo-difference}
elo_age_changes <- df82 %>%
  # Calculate Max_Exp for each skier first
  group_by(Skier) %>%
  mutate(Max_Exp = max(Exp)) %>%
  filter(Max_Exp >= 100) %>%  # Keep experienced skiers
  # Calculate percentages of max for each season
  group_by(Season) %>%
  mutate(
    # Convert current and previous Elo to percentage of max for that season
    Distance_Pelo_Pct = (Distance_Pelo / max(Distance_Pelo, na.rm = TRUE)) * 100,
    Sprint_Pelo_Pct = (Sprint_Pelo / max(Sprint_Pelo, na.rm = TRUE)) * 100,
    Prev_Distance_Pct = (Prev_Distance / max(Prev_Distance, na.rm = TRUE)) * 100,
    Prev_Sprint_Pct = (Prev_Sprint / max(Prev_Sprint, na.rm = TRUE)) * 100
  ) %>%
  # Calculate changes in percentages
  mutate(
    Age_Clean = floor(as.numeric(Age)),
    # Calculate how much closer they got to 100%
    Distance_Pct_Change = Distance_Pelo_Pct - Prev_Distance_Pct,
    Sprint_Pct_Change = Sprint_Pelo_Pct - Prev_Sprint_Pct
  ) %>%
  group_by(Age_Clean) %>%
  summarize(
    Avg_Distance_Pct_Change = mean(Distance_Pct_Change, na.rm = TRUE),
    Avg_Sprint_Pct_Change = mean(Sprint_Pct_Change, na.rm = TRUE),
    n = n(),
    n_distance = sum(!is.na(Distance_Pct_Change)),
    n_sprint = sum(!is.na(Sprint_Pct_Change))
  ) %>%
  filter(!is.na(Age_Clean), 
         n >= 20,
         Age_Clean >= 15,
         Age_Clean <= 40,
         n_distance >= 10,
         n_sprint >= 10)
print(elo_age_changes)

predict_2025 <- function(df_race, model_2025, elo_age_changes) {
  # Get current and previous season's max Elo scores
  latest_top_distance <- max(df_race$Distance_Pelo, na.rm = TRUE)
  latest_top_sprint <- max(df_race$Sprint_Pelo, na.rm = TRUE)
  prev_top_distance <- max(df_race$Prev_Distance, na.rm = TRUE)
  prev_top_sprint <- max(df_race$Prev_Sprint, na.rm = TRUE)
  
  # Create a dataframe for 2025 predictions
  df_2025 <- df_race %>%
    # Get the most recent values for each skier
    group_by(Skier) %>%
    filter(Date == max(Date)) %>%
    ungroup() %>%
    mutate(
      Age_2025 = floor(as.numeric(Age)) + 1,  # Age in 2025
      # Calculate current and previous percentages of max
      Distance_Pelo_Pct = (Distance_Pelo / latest_top_distance) * 100,
      Sprint_Pelo_Pct = (Sprint_Pelo / latest_top_sprint) * 100,
      Prev_Distance_Pct = (Prev_Distance / prev_top_distance) * 100,
      Prev_Sprint_Pct = (Prev_Sprint / prev_top_sprint) * 100,
      # Look up expected percentage improvements based on age
      Distance_Pct_Change = coalesce(elo_age_changes$Avg_Distance_Pct_Change[match(Age_2025, elo_age_changes$Age_Clean)], 0),
      Sprint_Pct_Change = coalesce(elo_age_changes$Avg_Sprint_Pct_Change[match(Age_2025, elo_age_changes$Age_Clean)], 0),
      # Calculate new percentages (capped at 100%)
      New_Distance_Pct = pmin(100, Distance_Pelo_Pct + Distance_Pct_Change),
      New_Sprint_Pct = pmin(100, Sprint_Pelo_Pct + Sprint_Pct_Change),
      # Convert back to Elo scores
      Distance_Pelo = (New_Distance_Pct / 100) * latest_top_distance,
      Sprint_Pelo = (New_Sprint_Pct / 100) * latest_top_sprint,
      # Use previous season's points as Prev_Pct_of_Max_Points
      Prev_Pct_of_Max_Points = coalesce(Pct_of_Max_Points, 0)
    )
  
  # Make predictions using the adjusted values
  predictions_2025 <- predict(model_2025, newdata = df_2025, type = "response")
  
  # Add predictions to the dataframe
  df_2025$Predicted_2025_Points = predictions_2025
  
  # Sort by predicted points
  df_2025 <- df_2025 %>%
    dplyr::select(Skier, Age_2025, 
                  Distance_Pelo, Sprint_Pelo,
                  Distance_Pelo_Pct, Prev_Distance_Pct, New_Distance_Pct,
                  Sprint_Pelo_Pct, Prev_Sprint_Pct, New_Sprint_Pct,
                  Distance_Pct_Change, Sprint_Pct_Change,
                  Prev_Pct_of_Max_Points, Predicted_2025_Points) %>%
    arrange(desc(Predicted_2025_Points))
  
  return(df_2025)
}

# Make predictions
predictions_2025 <- predict_2025(df_2025, model_2025, elo_age_changes)

# Print top 20 predictions
print("Top 20 Predicted Performers for 2025:")
print(head(predictions_2025, 20))

# Create visualization of percentage changes
ggplot(predictions_2025, aes(x = Distance_Pelo_Pct, y = New_Distance_Pct)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Predicted Changes in Distance Performance (% of Top)",
    x = "Current Performance (%)",
    y = "Predicted 2025 Performance (%)"
  ) +
  scale_x_continuous(limits = c(0, 100)) +
  scale_y_continuous(limits = c(0, 100)) +
  theme_minimal()

# Show largest predicted improvements
print("\nTop 10 Predicted Improvements (Distance):")
predictions_2025 %>%
  mutate(Pct_Improvement = New_Distance_Pct - Distance_Pelo_Pct) %>%
  arrange(desc(Pct_Improvement)) %>%
  head(10) %>%
  dplyr::select(Skier, Age_2025, Distance_Pelo_Pct, New_Distance_Pct, Pct_Improvement)

print("\nTop 10 Predicted Improvements (Sprint):")
predictions_2025 %>%
  mutate(Pct_Improvement = New_Sprint_Pct - Sprint_Pelo_Pct) %>%
  arrange(desc(Pct_Improvement)) %>%
  head(10) %>%
  dplyr::select(Skier, Age_2025, Sprint_Pelo_Pct, New_Sprint_Pct, Pct_Improvement)

# Print average percentage changes by age for reference
print("\nAverage Performance Changes by Age (as % of top performer):")
print(elo_age_changes)

# Additional visualization: Age vs Average Improvement
ggplot(elo_age_changes, aes(x = Age_Clean)) +
  geom_line(aes(y = Avg_Distance_Pct_Change, color = "Distance"), size = 1) +
  geom_line(aes(y = Avg_Sprint_Pct_Change, color = "Sprint"), size = 1) +
  labs(
    title = "Average Performance Improvement by Age",
    x = "Age",
    y = "Average Improvement (% points)",
    color = "Discipline"
  ) +
  scale_color_manual(values = c("Distance" = "blue", "Sprint" = "red")) +
  theme_minimal()

predictions_2025_max = predictions_2025
```


### Putting it all together
```{r age-df-merge}
age_elo = predictions_2025_elo[,c(1, 2, 8)]
colnames(age_elo) <- c("Skier", "Age 2025", "Predicted Points Elo Total")
age_pct = predictions_2025_pct[,c(1, 8)]
colnames(age_pct) <- c("Skier", "Predicted Points Elo Pct")
age_max = predictions_2025_max[,c(1, 14)]
colnames(age_max) <- c("Skier",  "Predicted Points WC Pct")
age_normal = output_25[,c(1, 2, 4)]
colnames(age_normal) = c("Skier", "Nation", "Predicted Points Baseline")
age_elo
age_pct
age_max
age_normal

merged_df <- Reduce(function(x, y) merge(x, y, by="Skier", all=TRUE), 
                  list(age_elo, age_pct, age_max, age_normal))
merged_df
colnames(merged_df)
merged_df <- merged_df[,c(1, 6, 2, 7, 3, 4, 5)]
merged_df <- merged_df[,c(1, 2, 3, 4, 6)]
merged_df <- merged_df[order(-merged_df$`Predicted Points Elo Pct`), ]
colnames(merged_df)[5] <- "Age-Adjusted Predicted Points Pct"

write.xlsx(merged_df, 
           file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/men_age_table.xlsx",
           rowNames = FALSE)

print(merged_df)
```


### Johaug
```{r johaug}
# Get Johaug's last data point
johaug_last <- df82 %>%
  filter(Skier == "Therese Johaug") %>%
  filter(Season == max(Season))

# Function to predict for a specific number of years ahead
predict_johaug_season <- function(base_data, years_ahead) {
  # Get skier name from base_data
  skier_name <- base_data$Skier[1]
  
  # Calculate aging effects for specific timespan
  johaug_age <- floor(as.numeric(base_data$Age)) + years_ahead
  aging_effects_year <- elo_trends %>%
    filter(Age_Clean >= floor(as.numeric(base_data$Age)) & 
           Age_Clean <= johaug_age) %>%
    summarize(
      Cumulative_Distance = prod((100 + Avg_Distance_Change)/100),
      Cumulative_Distance_Classic = prod((100 + Avg_Distance_Classic_Change)/100),
      Cumulative_Distance_Freestyle = prod((100 + Avg_Distance_Freestyle_Change)/100),
      Cumulative_Sprint = prod((100 + Avg_Sprint_Change)/100),
      Cumulative_Classic = prod((100 + Avg_Classic_Change)/100),
      Cumulative_Freestyle = prod((100 + Avg_Freestyle_Change)/100)
    )
  
  # Create prediction
  year_pred <- base_data %>%
    dplyr::select(
      Skier, Nation, Age,
      Pelo, Distance_Pelo, Distance_C_Pelo, Distance_F_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
      Classic_Pelo, Freestyle_Pelo, Sprint_Pelo
    ) %>%
    mutate(
      Season = 2022 + years_ahead,
      Age = as.character(as.numeric(Age) + years_ahead),
      Pelo = Pelo * aging_effects_year$Cumulative_Distance,
      Distance_Pelo = Distance_Pelo * aging_effects_year$Cumulative_Distance,
      Distance_C_Pelo = Distance_C_Pelo * aging_effects_year$Cumulative_Distance_Classic,
      Distance_F_Pelo = Distance_F_Pelo * aging_effects_year$Cumulative_Distance_Freestyle,
      Sprint_C_Pelo = Sprint_C_Pelo * aging_effects_year$Cumulative_Sprint,
      Sprint_F_Pelo = Sprint_F_Pelo * aging_effects_year$Cumulative_Sprint,
      Classic_Pelo = Classic_Pelo * aging_effects_year$Cumulative_Classic,
      Freestyle_Pelo = Freestyle_Pelo * aging_effects_year$Cumulative_Freestyle,
      Sprint_Pelo = Sprint_Pelo * aging_effects_year$Cumulative_Sprint,
      Prev_Pct_of_Max_Points = if("Predicted Points" %in% names(base_data)) {
        base_data$`Predicted Points`
      } else {
        base_data$Pct_of_Max_Points
      }
    )
  
  # Create temporary df without the skier we're predicting for
  df_temp <- df_2025_simple %>%
    filter(Skier != skier_name) %>%
    bind_rows(year_pred)
  
  # Make predictions
  predictions_temp <- predict(model_2025, newdata = df_temp, type = "response")
  year_pred$`Predicted Points` <- tail(predictions_temp, 1)  # Get last prediction (our skier)
  
  return(year_pred)
}

# Get actual 2022 values
johaug_2022 <- johaug_last %>%
  dplyr::select(
    Skier, Nation, Age,
    Pelo, Distance_Pelo, Distance_C_Pelo, Distance_F_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
    Classic_Pelo, Freestyle_Pelo, Sprint_Pelo, Pct_of_Max_Points
  ) %>%
  mutate(
    Season = 2022,
    `Predicted Points` = Pct_of_Max_Points
  ) %>%
  dplyr::select(-Pct_of_Max_Points)

# Get predictions for 2023 and 2024
johaug_2023 <- predict_johaug_season(johaug_last, 1)
johaug_2024 <- predict_johaug_season(johaug_last, 2)
johaug_2025 <- predict_johaug_season(johaug_last, 3)

# Combine all seasons
johaug_all_seasons <- bind_rows(johaug_2022, johaug_2023, johaug_2024, johaug_2025)

# Format final dataframe
johaug_all_seasons <- johaug_all_seasons %>%
 dplyr::select(Skier, Nation, Season, Age, `Predicted Points`, 
               Pelo, Distance_Pelo, Distance_C_Pelo, Distance_F_Pelo) %>%
 rename(
   "Elo" = "Pelo",
   "Distance Elo" = "Distance_Pelo",
   "Distance Classic Elo" = "Distance_C_Pelo",
   "Distance Freestyle Elo" = "Distance_F_Pelo"
 )

# Function to get historical data for a skier
# Function to get historical data for a skier
get_skier_history <- function(skier_name) {
  df82 %>%
    dplyr::filter(Skier == skier_name,
           Season >= 2022,
           Season <= 2024) %>%
    dplyr::group_by(Season) %>%
    dplyr::arrange(desc(Date)) %>%
    dplyr::slice_head(n = 1) %>%
    dplyr::ungroup() %>%
    dplyr::select(
      Skier, Nation, Season, Age,
      Pelo, Distance_Pelo, Distance_C_Pelo, Distance_F_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
      Classic_Pelo, Freestyle_Pelo, Sprint_Pelo,
      Pct_of_Max_Points
    ) %>%
    dplyr::rename(`Predicted Points` = Pct_of_Max_Points)
}

# Get historical data for each skier
diggins_history <- get_skier_history("Jessie Diggins")
karlsson_history <- get_skier_history("Frida Karlsson")
andersson_history <- get_skier_history("Ebba Andersson")

# Get 2025 predictions for each skier using their 2024 data as base
diggins_2024 <- diggins_history %>% filter(Season == 2024)
karlsson_2024 <- karlsson_history %>% filter(Season == 2024)
andersson_2024 <- andersson_history %>% filter(Season == 2024)

diggins_2025 <- predict_johaug_season(diggins_2024, 1)
karlsson_2025 <- predict_johaug_season(karlsson_2024, 1)
andersson_2025 <- predict_johaug_season(andersson_2024, 1)
diggins_2025$Season = 2025
karlsson_2025$Season = 2025
andersson_2025$Season = 2025

# Function to standardize column names
standardize_columns <- function(df) {
  df %>%
    dplyr::rename(
      "Elo" = "Pelo",
      "Distance Elo" = "Distance_Pelo",
      "Distance Classic Elo" = "Distance_C_Pelo",
      "Distance Freestyle Elo" = "Distance_F_Pelo"
    ) %>%
    dplyr::select(
      Skier, Nation, Season, Age, `Predicted Points`,
      `Elo`, `Distance Elo`, `Distance Classic Elo`, `Distance Freestyle Elo`
    )
}

# Combine all data
all_seasons <- bind_rows(
  johaug_all_seasons,
  diggins_history %>% standardize_columns(),
  karlsson_history %>% standardize_columns(),
  andersson_history %>% standardize_columns(),
  diggins_2025 %>% standardize_columns(),
  karlsson_2025 %>% standardize_columns(),
  andersson_2025 %>% standardize_columns()
) %>%
  arrange(Skier, Season)

# Save to Excel
write.xlsx(johaug_all_seasons, 
           file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/johaug.xlsx",
           rowNames = FALSE)

print(johaug_all_seasons)
print(all_seasons)

# Create the plot
johaug_age_plot <- ggplot(all_seasons, aes(x = Season, y = `Distance Elo`, color = as.factor(Skier), group = Skier)) +
  geom_line() +
  labs(
    title = "Distance Elo Across Seasons by Skier",
    x = "Season",
    y = "Distance Elo",
    color = "Skier"
  ) +
  theme_minimal()

ggsave("/Users/syverjohansen/blog/daehl-e/static/img/johaug-age.png", johaug_age_plot, width = 6, height = 4, dpi = 300)

```

### So how good is Klaebo?






```{r klaebo-catchup}
replace_na_with_quartile <- function(x) {
    quartile_1 <- quantile(x, 0.25, na.rm = TRUE)
    ifelse(is.na(x), quartile_1, x)
}
# Load data and create two dataframes for different point systems

df_full <- read_feather("~/ski/elo/python/ski/polars/excel365/men_merged.feather") %>%
 dplyr::filter(!Distance %in% c("Stage", "Ts", "Rel"),) 

# Create World Cup points dataframe
df_wc <- df_full %>%
 dplyr::mutate(Points = purrr::map_dbl(Place, ~get_points(.x, wc_points)))

# Create Stage points dataframe
df_stage <- df_full %>%
 dplyr::mutate(Points = purrr::map_dbl(Place, ~get_points(.x, stage_points)))

df_tds <-  read_feather("~/ski/elo/python/ski/polars/excel365/men_merged.feather") %>%
 dplyr::filter(City == "Tour de Ski") %>%
  dplyr::mutate(Points = purrr::map_dbl(Place, ~get_points(.x, tds_points)))



print("World Cup races dataframe:")
print(str(df_wc))
print("\nStage races dataframe:")
print(str(df_stage))


# Get Klæbo's current Elo ratings and percentages
get_skier_metrics <- function(skier_name) {
 # Get skier's current Elo ratings and percentages
 skier_metrics <- df82 %>%
   # Get the date of skier's most recent entry
   dplyr::filter(Skier == skier_name) %>%
   dplyr::arrange(desc(Date)) %>%
   dplyr::slice(1) %>%
   # Get this date
   dplyr::select(Date) %>%
   {
     date <- pull(., Date)
     # Now get skier's ratings and the max ratings from that date
     df82 %>%
       dplyr::filter(Date == date) %>%
       {
         max_ratings <- summarise(.,
           max_Pelo = max(Pelo, na.rm = TRUE),
           max_Distance_Pelo = max(Distance_Pelo, na.rm = TRUE),
           max_Sprint_Pelo = max(Sprint_Pelo, na.rm = TRUE),
           max_Distance_C_Pelo = max(Distance_C_Pelo, na.rm = TRUE),
           max_Distance_F_Pelo = max(Distance_F_Pelo, na.rm = TRUE),
           max_Sprint_C_Pelo = max(Sprint_C_Pelo, na.rm = TRUE),
           max_Sprint_F_Pelo = max(Sprint_F_Pelo, na.rm = TRUE),
           max_Classic_Pelo = max(Classic_Pelo, na.rm = TRUE),
           max_Freestyle_Pelo = max(Freestyle_Pelo, na.rm = TRUE)
         )
         
         # Get skier's ratings and calculate percentages
         filter(., Skier == skier_name) %>%
           dplyr::select(
             Pelo, Distance_Pelo, Sprint_Pelo,
             Distance_C_Pelo, Distance_F_Pelo,
             Sprint_C_Pelo, Sprint_F_Pelo,
             Classic_Pelo, Freestyle_Pelo
           ) %>%
           mutate(
             Pelo_Pct = Pelo / max_ratings$max_Pelo,
             Distance_Pelo_Pct = Distance_Pelo / max_ratings$max_Distance_Pelo,
             Sprint_Pelo_Pct = Sprint_Pelo / max_ratings$max_Sprint_Pelo,
             Distance_C_Pelo_Pct = Distance_C_Pelo / max_ratings$max_Distance_C_Pelo,
             Distance_F_Pelo_Pct = Distance_F_Pelo / max_ratings$max_Distance_F_Pelo,
             Sprint_C_Pelo_Pct = Sprint_C_Pelo / max_ratings$max_Sprint_C_Pelo,
             Sprint_F_Pelo_Pct = Sprint_F_Pelo / max_ratings$max_Sprint_F_Pelo,
             Classic_Pelo_Pct = Classic_Pelo / max_ratings$max_Classic_Pelo,
             Freestyle_Pelo_Pct = Freestyle_Pelo / max_ratings$max_Freestyle_Pelo
           )
       }
   }

# Function to get weighted last 5 average for specific race type
get_last_5_weighted <- function(skier, race_distance, race_technique = NULL) {
 races <- df_wc %>%
   dplyr::filter(
     Skier == skier,
     if(race_distance == "Distance") {
       if(!is.null(race_technique)) {
         Distance != "Sprint" & Technique == race_technique
       } else {
         Distance != "Sprint"
       }
     } else {
       if(!is.null(race_technique)) {
         Distance == race_distance & Technique == race_technique
       } else {
         Distance == race_distance
       }
     }
   ) %>%
   dplyr::arrange(desc(Date)) %>%
   dplyr::slice(1:5) %>%
   dplyr::pull(Points)
 print(races)
 if(length(races) > 0) {
   weights <- seq_along(races)
   return(weighted.mean(races, w = weights, na.rm = TRUE))
 } else {
   return(NA)
 }
}

 # Calculate averages for skier
 skier_averages <- data.frame(
   Distance_Classic_Avg = get_last_5_weighted(skier_name, "Distance", "C"),
   Distance_Freestyle_Avg = get_last_5_weighted(skier_name, "Distance", "F"),
   Sprint_Classic_Avg = get_last_5_weighted(skier_name, "Sprint", "C"),
   Sprint_Freestyle_Avg = get_last_5_weighted(skier_name, "Sprint", "F"),
   Distance_Avg = get_last_5_weighted(skier_name, "Distance")
 )

 # Combine metrics and averages
 skier_metrics <- skier_metrics %>%
   dplyr::bind_cols(skier_averages)

 return(skier_metrics)
}

# Example usage:
klaebo_metrics <- get_skier_metrics("Harald Østberg Amundsen")
print("Klæbo's complete metrics:")
print(klaebo_metrics)

create_race_model <- function(race_type, df) {
  # Define mapping between race types and filtering conditions
  race_filters <- list(
    "Distance Classic" = list(
      distance_filter = "Distance != 'Sprint'",
      technique_filter = "Technique == 'C'",
      pelo_pct = "Distance_C_Pelo_Pct",
      avg_col_from = "Distance_Classic_Avg"
    ),
    "Distance Freestyle" = list(
      distance_filter = "Distance != 'Sprint'",
      technique_filter = "Technique == 'F'",
      pelo_pct = "Distance_F_Pelo_Pct",
      avg_col_from = "Distance_Freestyle_Avg"
    ),
    "Sprint Classic" = list(
      distance_filter = "Distance == 'Sprint'",
      technique_filter = "Technique == 'C'",
      pelo_pct = "Sprint_C_Pelo_Pct",
      avg_col_from = "Sprint_Classic_Avg"
    ),
    "Sprint Freestyle" = list(
      distance_filter = "Distance == 'Sprint'",
      technique_filter = "Technique == 'F'",
      pelo_pct = "Sprint_F_Pelo_Pct",
      avg_col_from = "Sprint_Freestyle_Avg"
    ),
    "Distance" = list(
      distance_filter = "Distance != 'Sprint'",
      technique_filter = "TRUE",
      pelo_pct = "Distance_Pelo_Pct",
      avg_col_from = "Distance_Avg"
    )
  )
  
  filters <- race_filters[[race_type]]
  
  # Create data with Pelo percentages and previous weighted averages
  df_model <- df %>%
    dplyr::filter(
      eval(parse(text = filters$distance_filter)),
      eval(parse(text = filters$technique_filter))
    ) %>%
    # Calculate Pelo percentages by date
    dplyr::group_by(Date) %>%
      dplyr::mutate(
    across(
      c(Distance_C_Pelo, Classic_Pelo, Distance_Pelo, Pelo,
        Distance_F_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
        Sprint_Pelo, Freestyle_Pelo),
      replace_na_with_quartile
    )
  ) %>%
    dplyr::mutate(
      Distance_C_Pelo_Pct = Distance_C_Pelo / max(Distance_C_Pelo, na.rm = TRUE),
      Classic_Pelo_Pct = Classic_Pelo / max(Classic_Pelo, na.rm = TRUE),
      Distance_Pelo_Pct = Distance_Pelo / max(Distance_Pelo, na.rm = TRUE),
      Pelo_Pct = Pelo / max(Pelo, na.rm = TRUE),
      Distance_F_Pelo_Pct = Distance_F_Pelo / max(Distance_F_Pelo, na.rm = TRUE),
      Sprint_C_Pelo_Pct = Sprint_C_Pelo / max(Sprint_C_Pelo, na.rm = TRUE),
      Sprint_F_Pelo_Pct = Sprint_F_Pelo / max(Sprint_F_Pelo, na.rm = TRUE),
      Sprint_Pelo_Pct = Sprint_Pelo / max(Sprint_Pelo, na.rm = TRUE),
      Freestyle_Pelo_Pct = Freestyle_Pelo / max(Freestyle_Pelo, na.rm = TRUE)
    ) %>%
    dplyr::ungroup() %>%
    # Calculate weighted averages
    dplyr::group_by(Skier) %>%
    dplyr::arrange(Date) %>%
    dplyr::mutate(
      Prev_Points_Weighted = sapply(1:n(), function(j) {
        if (j == 1) return(0)
        past_points <- Points[max(1, j-5):(j-1)]
        weights <- seq_along(past_points)
        weighted.mean(past_points, w = weights, na.rm = TRUE)
      })
    ) %>%
    dplyr::ungroup()
  
  # Define potential predictors based on race type
  predictors <- c(filters$pelo_pct, "Classic_Pelo_Pct", "Distance_Pelo_Pct", 
                 "Pelo_Pct", "Sprint_Pelo_Pct", "Freestyle_Pelo_Pct",
                 "Prev_Points_Weighted")
  
  # Filter for top performers and do exhaustive search
  model_data <- df_model %>%
    dplyr::filter(get(filters$pelo_pct) > 0.75) %>%
    dplyr::filter(Season>2014)
  
  # Exhaustive search
  formula <- as.formula(paste("Points ~", paste(predictors, collapse = " + ")))
  exhaustive_selection <- regsubsets(formula, data = model_data, nbest = 1, 
                                   method = "exhaustive", nvmax = length(predictors))
  summary_exhaustive <- summary(exhaustive_selection)
  best_bic_vars <- names(coef(exhaustive_selection, which.min(summary_exhaustive$bic)))[-1]
  
  # Create GAM formula with selected features
  gam_formula <- as.formula(paste("Points ~", paste(paste0("s(", best_bic_vars, ")"), collapse = " + ")))
  
  # Fit GAM model
  race_model <- gam(gam_formula, data = model_data)
  
  # Create prediction function for this model
  predict_race <- function(skier_metrics) {
    pred_data <- skier_metrics %>%
      dplyr::rename("Prev_Points_Weighted" = filters$avg_col_from) %>%
      dplyr::select(all_of(best_bic_vars))
    
    predict(race_model, newdata = pred_data)
  }
  
  # Return both model and its prediction function
  return(list(
    model = race_model,
    predict = predict_race,
    variables = best_bic_vars
  ))
}

create_tds_model <- function() {
 # Create data with Pelo percentages and previous weighted averages for Tour de Ski only
 df_tds <- df_tds %>%
   # Calculate Pelo percentages by date
   dplyr::group_by(Date) %>%
   dplyr::mutate(
    across(
      c(Distance_C_Pelo, Classic_Pelo, Distance_Pelo, Pelo,
        Distance_F_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
        Sprint_Pelo, Freestyle_Pelo),
      replace_na_with_quartile
    )
  ) %>%
   dplyr::mutate(
     Distance_C_Pelo_Pct = Distance_C_Pelo / max(Distance_C_Pelo, na.rm = TRUE),
     Classic_Pelo_Pct = Classic_Pelo / max(Classic_Pelo, na.rm = TRUE),
     Distance_Pelo_Pct = Distance_Pelo / max(Distance_Pelo, na.rm = TRUE),
     Pelo_Pct = Pelo / max(Pelo, na.rm = TRUE),
     Distance_F_Pelo_Pct = Distance_F_Pelo / max(Distance_F_Pelo, na.rm = TRUE),
     Sprint_C_Pelo_Pct = Sprint_C_Pelo / max(Sprint_C_Pelo, na.rm = TRUE),
     Sprint_F_Pelo_Pct = Sprint_F_Pelo / max(Sprint_F_Pelo, na.rm = TRUE),
     Sprint_Pelo_Pct = Sprint_Pelo / max(Sprint_Pelo, na.rm = TRUE),
     Freestyle_Pelo_Pct = Freestyle_Pelo / max(Freestyle_Pelo, na.rm = TRUE)
   ) %>%
   dplyr::ungroup() %>%
   # Calculate weighted averages
   dplyr::group_by(Skier) %>%
   dplyr::arrange(Date) %>%
   dplyr::mutate(
     Prev_Points_Weighted = sapply(1:n(), function(j) {
       if (j == 1) return(0)
       past_points <- Points[max(1, j-5):(j-1)]
       weights <- seq_along(past_points)
       weighted.mean(past_points, w = weights, na.rm = TRUE)
     })
   ) %>%
   dplyr::ungroup()
 
 # Define potential predictors for Tour de Ski
 predictors <- c("Distance_C_Pelo_Pct", "Classic_Pelo_Pct", "Distance_Pelo_Pct", 
                "Pelo_Pct", "Sprint_Pelo_Pct", "Freestyle_Pelo_Pct", 
                "Distance_F_Pelo_Pct", "Sprint_C_Pelo_Pct", "Sprint_F_Pelo_Pct",
                "Prev_Points_Weighted")
 
 # Filter for top performers and do exhaustive search
 model_data <- df_tds %>%
   dplyr::filter(Pelo_Pct > 0.75) %>%  # Using overall Pelo for TdS
   dplyr::filter(Season>2014)
 # Exhaustive search
 formula <- as.formula(paste("Points ~", paste(predictors, collapse = " + ")))
 exhaustive_selection <- regsubsets(formula, data = model_data, nbest = 1, 
                                  method = "exhaustive", nvmax = length(predictors))
 summary_exhaustive <- summary(exhaustive_selection)
 best_bic_vars <- names(coef(exhaustive_selection, which.min(summary_exhaustive$bic)))[-1]
 
 # Create GAM formula with selected features
 gam_formula <- as.formula(paste("Points ~", paste(paste0("s(", best_bic_vars, ")"), collapse = " + ")))
 
 # Fit GAM model
 tds_model <- gam(gam_formula, data = model_data)
 
 # Create prediction function
 predict_tds <- function(skier_metrics) {
   pred_data <- skier_metrics %>%
     dplyr::rename("Prev_Points_Weighted" = "Distance_Avg") %>%  # Using Distance_Avg for TdS
     dplyr::select(all_of(best_bic_vars))
   
   predict(tds_model, newdata = pred_data)
 }
 
 return(list(
   model = tds_model,
   predict = predict_tds,
   variables = best_bic_vars
 ))
}

# Create TdS model
tds_model <- create_tds_model()

# Example usage:
print("Tour de Ski prediction for Klaebo:")
print(tds_model$predict(klaebo_metrics))
print("Variables used:")
print(tds_model$variables)

# Create models for each race type
wc_models <- list(
  distance_classic = create_race_model("Distance Classic", df_wc),
  distance_freestyle = create_race_model("Distance Freestyle", df_wc),
  sprint_classic = create_race_model("Sprint Classic", df_wc),
  sprint_freestyle = create_race_model("Sprint Freestyle", df_wc),
  distance = create_race_model("Distance", df_wc)
)

stage_models <- list(
  distance_classic = create_race_model("Distance Classic", df_stage),
  distance_freestyle = create_race_model("Distance Freestyle", df_stage),
  sprint_classic = create_race_model("Sprint Classic", df_stage),
  sprint_freestyle = create_race_model("Sprint Freestyle", df_stage),
  distance = create_race_model("Distance", df_stage)
)

# Example usage:
print("Distance Classic prediction for Klaebo:")
print(wc_models$distance_classic$predict(klaebo_metrics))
print("Variables used:")
print(wc_models$distance_classic$variables)

# Example usage:
print("Distance Classic prediction for Klaebo:")
print(stage_models$distance_freestyle$predict(klaebo_metrics))
print("Variables used:")
print(stage_models$distance_classic$variables)


predictions_df <- data.frame(
  WC_Distance_Classic = pmin(100, unname(wc_models$distance_classic$predict(klaebo_metrics))),
  WC_Distance_Freestyle = pmin(100, unname(wc_models$distance_freestyle$predict(klaebo_metrics))),
  WC_Sprint_Classic = pmin(100, unname(wc_models$sprint_classic$predict(klaebo_metrics))),
  WC_Sprint_Freestyle = pmin(100, unname(wc_models$sprint_freestyle$predict(klaebo_metrics))),
  WC_Distance = pmin(100, unname(wc_models$distance$predict(klaebo_metrics))),
  
  Stage_Distance_Classic = unname(stage_models$distance_classic$predict(klaebo_metrics)),
  Stage_Distance_Freestyle = unname(stage_models$distance_freestyle$predict(klaebo_metrics)),
  Stage_Sprint_Classic = unname(stage_models$sprint_classic$predict(klaebo_metrics)),
  Stage_Sprint_Freestyle = unname(stage_models$sprint_freestyle$predict(klaebo_metrics)),
  Stage_Distance = unname(stage_models$distance$predict(klaebo_metrics)),
  
  TDS = unname(tds_model$predict(klaebo_metrics))
)

print("Predictions dataframe with capped values:")
print(predictions_df)
# Can do the same for other race types

```




```{r multiple-skiers}
replace_na_with_quartile <- function(x) {
    quartile_1 <- quantile(x, 0.25, na.rm = TRUE)
    ifelse(is.na(x), quartile_1, x)
}
# Load data and create two dataframes for different point systems

df_full <- read_feather("~/ski/elo/python/ski/polars/excel365/men_merged.feather") %>%
 dplyr::filter(!Distance %in% c("Stage", "Ts", "Rel"),) 

# Create World Cup points dataframe
df_wc <- df_full %>%
 dplyr::mutate(Points = purrr::map_dbl(Place, ~get_points(.x, wc_points)))

# Create Stage points dataframe
df_stage <- df_full %>%
 dplyr::mutate(Points = purrr::map_dbl(Place, ~get_points(.x, stage_points)))

df_tds <-  read_feather("~/ski/elo/python/ski/polars/excel365/men_merged.feather") %>%
 dplyr::filter(City == "Tour de Ski") %>%
  dplyr::mutate(Points = purrr::map_dbl(Place, ~get_points(.x, tds_points)))



print("World Cup races dataframe:")
print(str(df_wc))
print("\nStage races dataframe:")
print(str(df_stage))

skiers_2024 <- df82 %>%
  dplyr::filter(Season == 2024) %>%
  dplyr::filter(Pct_of_Max_Points>.25) %>%
  dplyr::select(Skier) %>%
  distinct() %>%
  pull(Skier)



get_skier_metrics <- function(skier_name) {
 # Get skier's current Elo ratings and percentages
 skier_metrics <- df82 %>%
   # Get the date of skier's most recent entry
   dplyr::filter(Skier == skier_name) %>%
   dplyr::arrange(desc(Date)) %>%
   dplyr::slice(1) %>%
   # Get this date
   dplyr::select(Date) %>%
   {
     date <- pull(., Date)
     # Now get skier's ratings and the max ratings from that date
     df82 %>%
       dplyr::filter(Date == date) %>%
       {
         max_ratings <- summarise(.,
           max_Pelo = max(Pelo, na.rm = TRUE),
           max_Distance_Pelo = max(Distance_Pelo, na.rm = TRUE),
           max_Sprint_Pelo = max(Sprint_Pelo, na.rm = TRUE),
           max_Distance_C_Pelo = max(Distance_C_Pelo, na.rm = TRUE),
           max_Distance_F_Pelo = max(Distance_F_Pelo, na.rm = TRUE),
           max_Sprint_C_Pelo = max(Sprint_C_Pelo, na.rm = TRUE),
           max_Sprint_F_Pelo = max(Sprint_F_Pelo, na.rm = TRUE),
           max_Classic_Pelo = max(Classic_Pelo, na.rm = TRUE),
           max_Freestyle_Pelo = max(Freestyle_Pelo, na.rm = TRUE)
         )
         
         # Get skier's ratings and calculate percentages
         filter(., Skier == skier_name) %>%
           dplyr::select(
             Pelo, Distance_Pelo, Sprint_Pelo,
             Distance_C_Pelo, Distance_F_Pelo,
             Sprint_C_Pelo, Sprint_F_Pelo,
             Classic_Pelo, Freestyle_Pelo
           ) %>%
           mutate(
             Pelo_Pct = Pelo / max_ratings$max_Pelo,
             Distance_Pelo_Pct = Distance_Pelo / max_ratings$max_Distance_Pelo,
             Sprint_Pelo_Pct = Sprint_Pelo / max_ratings$max_Sprint_Pelo,
             Distance_C_Pelo_Pct = Distance_C_Pelo / max_ratings$max_Distance_C_Pelo,
             Distance_F_Pelo_Pct = Distance_F_Pelo / max_ratings$max_Distance_F_Pelo,
             Sprint_C_Pelo_Pct = Sprint_C_Pelo / max_ratings$max_Sprint_C_Pelo,
             Sprint_F_Pelo_Pct = Sprint_F_Pelo / max_ratings$max_Sprint_F_Pelo,
             Classic_Pelo_Pct = Classic_Pelo / max_ratings$max_Classic_Pelo,
             Freestyle_Pelo_Pct = Freestyle_Pelo / max_ratings$max_Freestyle_Pelo
           )
       }
   }

# Function to get weighted last 5 average for specific race type
get_last_5_weighted <- function(skier, race_distance, race_technique = NULL) {
 races <- df_wc %>%
   dplyr::filter(
     Skier == skier,
     if(race_distance == "Distance") {
       if(!is.null(race_technique)) {
         Distance != "Sprint" & Technique == race_technique
       } else {
         Distance != "Sprint"
       }
     } else {
       if(!is.null(race_technique)) {
         Distance == race_distance & Technique == race_technique
       } else {
         Distance == race_distance
       }
     }
   ) %>%
   dplyr::arrange(desc(Date)) %>%
   dplyr::slice(1:5) %>%
   dplyr::pull(Points)
 print(races)
 if(length(races) > 0) {
   weights <- seq_along(races)
   return(weighted.mean(races, w = weights, na.rm = TRUE))
 } else {
   return(NA)
 }
}

 # Calculate averages for skier
 skier_averages <- data.frame(
   Distance_Classic_Avg = get_last_5_weighted(skier_name, "Distance", "C"),
   Distance_Freestyle_Avg = get_last_5_weighted(skier_name, "Distance", "F"),
   Sprint_Classic_Avg = get_last_5_weighted(skier_name, "Sprint", "C"),
   Sprint_Freestyle_Avg = get_last_5_weighted(skier_name, "Sprint", "F"),
   Distance_Avg = get_last_5_weighted(skier_name, "Distance")
 )

 # Combine metrics and averages
 skier_metrics <- skier_metrics %>%
   dplyr::bind_cols(skier_averages)

 return(skier_metrics)
}



create_race_model <- function(race_type, df) {
  # Define mapping between race types and filtering conditions
  race_filters <- list(
    "Distance Classic" = list(
      distance_filter = "Distance != 'Sprint'",
      technique_filter = "Technique == 'C'",
      pelo_pct = "Distance_C_Pelo_Pct",
      avg_col_from = "Distance_Classic_Avg"
    ),
    "Distance Freestyle" = list(
      distance_filter = "Distance != 'Sprint'",
      technique_filter = "Technique == 'F'",
      pelo_pct = "Distance_F_Pelo_Pct",
      avg_col_from = "Distance_Freestyle_Avg"
    ),
    "Sprint Classic" = list(
      distance_filter = "Distance == 'Sprint'",
      technique_filter = "Technique == 'C'",
      pelo_pct = "Sprint_C_Pelo_Pct",
      avg_col_from = "Sprint_Classic_Avg"
    ),
    "Sprint Freestyle" = list(
      distance_filter = "Distance == 'Sprint'",
      technique_filter = "Technique == 'F'",
      pelo_pct = "Sprint_F_Pelo_Pct",
      avg_col_from = "Sprint_Freestyle_Avg"
    ),
    "Distance" = list(
      distance_filter = "Distance != 'Sprint'",
      technique_filter = "TRUE",
      pelo_pct = "Distance_Pelo_Pct",
      avg_col_from = "Distance_Avg"
    )
  )
  
  filters <- race_filters[[race_type]]
  
  # Create data with Pelo percentages and previous weighted averages
  df_model <- df %>%
    dplyr::filter(
      eval(parse(text = filters$distance_filter)),
      eval(parse(text = filters$technique_filter))
    ) %>%
    # Calculate Pelo percentages by date
    dplyr::group_by(Date) %>%
      dplyr::mutate(
    across(
      c(Distance_C_Pelo, Classic_Pelo, Distance_Pelo, Pelo,
        Distance_F_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
        Sprint_Pelo, Freestyle_Pelo),
      replace_na_with_quartile
    )
  ) %>%
    dplyr::mutate(
      Distance_C_Pelo_Pct = Distance_C_Pelo / max(Distance_C_Pelo, na.rm = TRUE),
      Classic_Pelo_Pct = Classic_Pelo / max(Classic_Pelo, na.rm = TRUE),
      Distance_Pelo_Pct = Distance_Pelo / max(Distance_Pelo, na.rm = TRUE),
      Pelo_Pct = Pelo / max(Pelo, na.rm = TRUE),
      Distance_F_Pelo_Pct = Distance_F_Pelo / max(Distance_F_Pelo, na.rm = TRUE),
      Sprint_C_Pelo_Pct = Sprint_C_Pelo / max(Sprint_C_Pelo, na.rm = TRUE),
      Sprint_F_Pelo_Pct = Sprint_F_Pelo / max(Sprint_F_Pelo, na.rm = TRUE),
      Sprint_Pelo_Pct = Sprint_Pelo / max(Sprint_Pelo, na.rm = TRUE),
      Freestyle_Pelo_Pct = Freestyle_Pelo / max(Freestyle_Pelo, na.rm = TRUE)
    ) %>%
    dplyr::ungroup() %>%
    # Calculate weighted averages
    dplyr::group_by(Skier) %>%
    dplyr::arrange(Date) %>%
    dplyr::mutate(
      Prev_Points_Weighted = sapply(1:n(), function(j) {
        if (j == 1) return(0)
        past_points <- Points[max(1, j-5):(j-1)]
        weights <- seq_along(past_points)
        weighted.mean(past_points, w = weights, na.rm = TRUE)
      })
    ) %>%
    dplyr::ungroup()
  
  # Define potential predictors based on race type
  predictors <- c(filters$pelo_pct, "Classic_Pelo_Pct", "Distance_Pelo_Pct", 
                 "Pelo_Pct", "Sprint_Pelo_Pct", "Freestyle_Pelo_Pct",
                 "Prev_Points_Weighted")
  
  # Filter for top performers and do exhaustive search
  model_data <- df_model %>%
    dplyr::filter(get(filters$pelo_pct) > 0.75) %>%
    dplyr::filter(Season>2014)
  
  # Exhaustive search
  formula <- as.formula(paste("Points ~", paste(predictors, collapse = " + ")))
  exhaustive_selection <- regsubsets(formula, data = model_data, nbest = 1, 
                                   method = "exhaustive", nvmax = length(predictors))
  summary_exhaustive <- summary(exhaustive_selection)
  best_bic_vars <- names(coef(exhaustive_selection, which.min(summary_exhaustive$bic)))[-1]
  
  # Create GAM formula with selected features
  gam_formula <- as.formula(paste("Points ~", paste(paste0("s(", best_bic_vars, ")"), collapse = " + ")))
  
  # Fit GAM model
  race_model <- gam(gam_formula, data = model_data)
  
  # Create prediction function for this model
  predict_race <- function(skier_metrics) {
    pred_data <- skier_metrics %>%
      dplyr::rename("Prev_Points_Weighted" = filters$avg_col_from) %>%
      dplyr::select(all_of(best_bic_vars))
    
    predict(race_model, newdata = pred_data)
  }
  
  # Return both model and its prediction function
  return(list(
    model = race_model,
    predict = predict_race,
    variables = best_bic_vars
  ))
}

create_tds_model <- function() {
 # Create data with Pelo percentages and previous weighted averages for Tour de Ski only
 df_tds <- df_tds %>%
   # Calculate Pelo percentages by date
   dplyr::group_by(Date) %>%
   dplyr::mutate(
    across(
      c(Distance_C_Pelo, Classic_Pelo, Distance_Pelo, Pelo,
        Distance_F_Pelo, Sprint_C_Pelo, Sprint_F_Pelo,
        Sprint_Pelo, Freestyle_Pelo),
      replace_na_with_quartile
    )
  ) %>%
   dplyr::mutate(
     Distance_C_Pelo_Pct = Distance_C_Pelo / max(Distance_C_Pelo, na.rm = TRUE),
     Classic_Pelo_Pct = Classic_Pelo / max(Classic_Pelo, na.rm = TRUE),
     Distance_Pelo_Pct = Distance_Pelo / max(Distance_Pelo, na.rm = TRUE),
     Pelo_Pct = Pelo / max(Pelo, na.rm = TRUE),
     Distance_F_Pelo_Pct = Distance_F_Pelo / max(Distance_F_Pelo, na.rm = TRUE),
     Sprint_C_Pelo_Pct = Sprint_C_Pelo / max(Sprint_C_Pelo, na.rm = TRUE),
     Sprint_F_Pelo_Pct = Sprint_F_Pelo / max(Sprint_F_Pelo, na.rm = TRUE),
     Sprint_Pelo_Pct = Sprint_Pelo / max(Sprint_Pelo, na.rm = TRUE),
     Freestyle_Pelo_Pct = Freestyle_Pelo / max(Freestyle_Pelo, na.rm = TRUE)
   ) %>%
   dplyr::ungroup() %>%
   # Calculate weighted averages
   dplyr::group_by(Skier) %>%
   dplyr::arrange(Date) %>%
   dplyr::mutate(
     Prev_Points_Weighted = sapply(1:n(), function(j) {
       if (j == 1) return(0)
       past_points <- Points[max(1, j-5):(j-1)]
       weights <- seq_along(past_points)
       weighted.mean(past_points, w = weights, na.rm = TRUE)
     })
   ) %>%
   dplyr::ungroup()
 
 # Define potential predictors for Tour de Ski
 predictors <- c("Distance_C_Pelo_Pct", "Classic_Pelo_Pct", "Distance_Pelo_Pct", 
                "Pelo_Pct", "Sprint_Pelo_Pct", "Freestyle_Pelo_Pct", 
                "Distance_F_Pelo_Pct", "Sprint_C_Pelo_Pct", "Sprint_F_Pelo_Pct",
                "Prev_Points_Weighted")
 
 # Filter for top performers and do exhaustive search
 model_data <- df_tds %>%
   dplyr::filter(Pelo_Pct > 0.75) %>%  # Using overall Pelo for TdS
   dplyr::filter(Season>2014)
 # Exhaustive search
 formula <- as.formula(paste("Points ~", paste(predictors, collapse = " + ")))
 exhaustive_selection <- regsubsets(formula, data = model_data, nbest = 1, 
                                  method = "exhaustive", nvmax = length(predictors))
 summary_exhaustive <- summary(exhaustive_selection)
 best_bic_vars <- names(coef(exhaustive_selection, which.min(summary_exhaustive$bic)))[-1]
 
 # Create GAM formula with selected features
 gam_formula <- as.formula(paste("Points ~", paste(paste0("s(", best_bic_vars, ")"), collapse = " + ")))
 
 # Fit GAM model
 tds_model <- gam(gam_formula, data = model_data)
 
 # Create prediction function
 predict_tds <- function(skier_metrics) {
   pred_data <- skier_metrics %>%
     dplyr::rename("Prev_Points_Weighted" = "Distance_Avg") %>%  # Using Distance_Avg for TdS
     dplyr::select(all_of(best_bic_vars))
   
   predict(tds_model, newdata = pred_data)
 }
 
 return(list(
   model = tds_model,
   predict = predict_tds,
   variables = best_bic_vars
 ))
}



# Create models for each race type
wc_models <- list(
  distance_classic = create_race_model("Distance Classic", df_wc),
  distance_freestyle = create_race_model("Distance Freestyle", df_wc),
  sprint_classic = create_race_model("Sprint Classic", df_wc),
  sprint_freestyle = create_race_model("Sprint Freestyle", df_wc),
  distance = create_race_model("Distance", df_wc)
)

stage_models <- list(
  distance_classic = create_race_model("Distance Classic", df_stage),
  distance_freestyle = create_race_model("Distance Freestyle", df_stage),
  sprint_classic = create_race_model("Sprint Classic", df_stage),
  sprint_freestyle = create_race_model("Sprint Freestyle", df_stage),
  distance = create_race_model("Distance", df_stage)
)



get_skier_predictions <- function(skier_name){

  skier_metrics <- get_skier_metrics(skier_name)
  
    # Get raw stage predictions
  stage_dc <- unname(stage_models$distance_classic$predict(skier_metrics))
  stage_df <- unname(stage_models$distance_freestyle$predict(skier_metrics))
  stage_sc <- unname(stage_models$sprint_classic$predict(skier_metrics))
  stage_sf <- unname(stage_models$sprint_freestyle$predict(skier_metrics))
  stage_d <- unname(stage_models$distance$predict(skier_metrics))
  
    # Scale function that maintains relative differences but caps at 50
  scale_to_50 <- function(x) {
    if(max(x) > 50) {
      (x / max(x)) * 50
    } else {
      x
    }
  }
  
predictions <- data.frame(
  Skier = skier_name,
  WC_Distance_Classic = pmin(100, unname(wc_models$distance_classic$predict(skier_metrics))),
  WC_Distance_Freestyle = pmin(100, unname(wc_models$distance_freestyle$predict(skier_metrics))),
  WC_Sprint_Classic = pmin(100, unname(wc_models$sprint_classic$predict(skier_metrics))),
  WC_Sprint_Freestyle = pmin(100, unname(wc_models$sprint_freestyle$predict(skier_metrics))),
  WC_Distance = pmin(100, unname(wc_models$distance$predict(skier_metrics))),
  
  Stage_Distance_Classic = unname(stage_models$distance_classic$predict(skier_metrics)),
  Stage_Distance_Freestyle = unname(stage_models$distance_freestyle$predict(skier_metrics)),
  Stage_Sprint_Classic = unname(stage_models$sprint_classic$predict(skier_metrics)),
  Stage_Sprint_Freestyle = unname(stage_models$sprint_freestyle$predict(skier_metrics)),
  Stage_Distance = unname(stage_models$distance$predict(skier_metrics)),
    # Stage_Distance_Classic = scale_to_50(stage_dc),
    # Stage_Distance_Freestyle = scale_to_50(stage_df),
    # Stage_Sprint_Classic = scale_to_50(stage_sc),
    # Stage_Sprint_Freestyle = scale_to_50(stage_sf),
    # Stage_Distance = scale_to_50(stage_d),  
 
  # Stage_Distance = unname(stage_models$distance$predict(skier_metrics)),
  
  TDS = unname(tds_model$predict(skier_metrics))

)
  return(predictions)
}

# Get predictions for all skiers
all_predictions <- bind_rows(lapply(skiers_2024, function(skier) {
  tryCatch({
    get_skier_predictions(skier)
  }, error = function(e) {
    print(paste("Error with skier:", skier))
    print(e)
    return(NULL)
  })
}))
all_predictions <- all_predictions %>%
  mutate(
    Stage_Distance_Classic = Stage_Distance_Classic * (50 / max(Stage_Distance_Classic, na.rm = TRUE)),
    Stage_Distance_Freestyle = Stage_Distance_Freestyle * (50 / max(Stage_Distance_Freestyle, na.rm = TRUE)),
    Stage_Sprint_Classic = Stage_Sprint_Classic * (50 / max(Stage_Sprint_Classic, na.rm = TRUE)),
    Stage_Sprint_Freestyle = Stage_Sprint_Freestyle * (50 / max(Stage_Sprint_Freestyle, na.rm = TRUE)),
    Stage_Distance = Stage_Distance * (50 / max(Stage_Distance, na.rm = TRUE))
  )

# Sort by predicted WC points to see top contenders
all_predictions <- all_predictions %>%
  dplyr::select(Skier, everything()) %>%
  arrange(desc(WC_Distance_Classic + WC_Distance_Freestyle + 
              WC_Sprint_Classic + WC_Sprint_Freestyle + WC_Distance))

print("Top 10 skiers by predicted World Cup points:")
print(head(all_predictions, 10))


```






```{r klaebo-missing-races}
# Define race structure constants
RACE_STRUCTURE <- list(
  WC_Distance_Classic = 6,
  WC_Distance_Freestyle = 6,
  WC_Sprint_Classic = 6,
  WC_Sprint_Freestyle = 6,
  WC_Distance = 1
)

simulate_all_seasons <- function(all_predictions_df, races_missed = 0, skip_tds = FALSE, n_sims = 100) {
  # Apply simulation to each skier
  results <- all_predictions_df %>%
    group_by(Skier) %>%
    group_modify(~{
      wc_races <- c(
        rep(.x$WC_Distance_Classic, RACE_STRUCTURE$WC_Distance_Classic),
        rep(.x$WC_Distance_Freestyle, RACE_STRUCTURE$WC_Distance_Freestyle),
        rep(.x$WC_Sprint_Classic, RACE_STRUCTURE$WC_Sprint_Classic),
        rep(.x$WC_Sprint_Freestyle, RACE_STRUCTURE$WC_Sprint_Freestyle),
        rep(.x$WC_Distance, RACE_STRUCTURE$WC_Distance)
      )
      
      stage_points <- sum(
        .x$Stage_Distance_Classic * 2,
        .x$Stage_Distance_Freestyle * 2,
        .x$Stage_Sprint_Classic * 1,
        .x$Stage_Sprint_Freestyle * 1,
        .x$Stage_Distance * 1
      )
      
      if(races_missed > 0) {
        sim_results <- replicate(n_sims, {
          remaining_points <- sum(wc_races) - sum(sample(wc_races, races_missed))
          if(skip_tds) {
            remaining_points
          } else {
            remaining_points + stage_points + .x$TDS
          }
        })
        points <- mean(sim_results)
      } else {
        points <- if(skip_tds) {
          sum(wc_races)
        } else {
          sum(wc_races) + stage_points + .x$TDS
        }
      }
      
      data.frame(
        Points = points
      )
    })
  
  return(results %>% 
           mutate(
             Races_Missed = races_missed,
             TDS_Included = !skip_tds
           ))
}

# Create the two dataframes
results_with_tds <- lapply(0:10, function(n_missed) {
 simulate_all_seasons(all_predictions, n_missed, FALSE) %>%
   dplyr::select(Skier, Points)
}) %>%
 reduce(left_join, by = "Skier") %>%
 setNames(c("Skier", paste0("Missed_", 0:10)))

results_without_tds <- lapply(0:10, function(n_missed) {
 simulate_all_seasons(all_predictions, n_missed, TRUE) %>%
   dplyr::select(Skier, Points)
}) %>%
 reduce(left_join, by = "Skier") %>%
 setNames(c("Skier", paste0("Missed_", 0:10)))

# Sort both dataframes by total points (0 missed races)
results_with_tds <- results_with_tds %>%
 arrange(desc(Missed_0))

results_without_tds <- results_without_tds %>%
 arrange(desc(Missed_0))

# Print top 10 from each
print("Top 10 skiers with TDS and stage races:")
print(head(results_with_tds, 10))

print("\nTop 10 skiers without TDS and stage races:")
print(head(results_without_tds, 10))

# Optionally save to Excel
write.xlsx(list(
 "With TDS" = results_with_tds,
 "Without TDS" = results_without_tds
), "men_points-predictions-missing-races.xlsx")
```




```{r volatility-predictions}
calculate_volatility <- function(skier_name, race_type, technique = NULL) {
  races <- df_wc %>%
    dplyr::filter(
      Skier == skier_name,
      Season >= 2022,  # Use last 2 seasons for volatility
      if(race_type == "Distance") {
        if(!is.null(technique)) {
          Distance != "Sprint" & Technique == technique
        } else {
          Distance != "Sprint"
        }
      } else {
        if(!is.null(technique)) {
          Distance == race_type & Technique == technique
        } else {
          Distance == race_type
        }
      }
    ) %>%
    dplyr::arrange(desc(Date)) %>%
    dplyr::pull(Points)
    
  if(length(races) >= 3) {  # Only calculate if we have at least 3 races
    return(sd(races, na.rm = TRUE))
  } else {
    return(0)  # Return 0 volatility if not enough races
  }
}

# Add volatility to predictions
all_predictions_with_volatility <- all_predictions %>%
  group_by(Skier) %>%
  mutate(
    WC_Distance_Classic_SD = map_dbl(Skier, ~calculate_volatility(., "Distance", "C")),
    WC_Distance_Freestyle_SD = map_dbl(Skier, ~calculate_volatility(., "Distance", "F")),
    WC_Sprint_Classic_SD = map_dbl(Skier, ~calculate_volatility(., "Sprint", "C")),
    WC_Sprint_Freestyle_SD = map_dbl(Skier, ~calculate_volatility(., "Sprint", "F")),
    WC_Distance_SD = map_dbl(Skier, ~calculate_volatility(., "Distance"))
  ) %>%
  ungroup()

all_predictions_with_volatility

# Modify simulate_season_with_volatility to handle NAs
simulate_season_with_volatility <- function(predictions_df, races_missed = 0, skip_tds = FALSE, n_sims = 1000) {
  # Replace any NA SDs with reasonable defaults
  default_sd <- 20  # You might want to adjust this value
  
  sim_results <- replicate(n_sims, {
    # Generate individual race results
    wc_races <- c(
      pmax(0, pmin(100, rnorm(6, predictions_df$WC_Distance_Classic, 
                             ifelse(is.na(predictions_df$WC_Distance_Classic_SD), default_sd, predictions_df$WC_Distance_Classic_SD)))),
      pmax(0, pmin(100, rnorm(6, predictions_df$WC_Distance_Freestyle,
                             ifelse(is.na(predictions_df$WC_Distance_Freestyle_SD), default_sd, predictions_df$WC_Distance_Freestyle_SD)))),
      pmax(0, pmin(100, rnorm(6, predictions_df$WC_Sprint_Classic,
                             ifelse(is.na(predictions_df$WC_Sprint_Classic_SD), default_sd, predictions_df$WC_Sprint_Classic_SD)))),
      pmax(0, pmin(100, rnorm(6, predictions_df$WC_Sprint_Freestyle,
                             ifelse(is.na(predictions_df$WC_Sprint_Freestyle_SD), default_sd, predictions_df$WC_Sprint_Freestyle_SD)))),
      pmax(0, pmin(100, rnorm(1, predictions_df$WC_Distance,
                             ifelse(is.na(predictions_df$WC_Distance_SD), default_sd, predictions_df$WC_Distance_SD))))
    )
    
    if(races_missed > 0) {
      wc_races <- wc_races[-sample(1:length(wc_races), races_missed)]
    }
    
    total_points <- sum(wc_races)
    
    if(!skip_tds) {
      total_points <- total_points + 
        sum(predictions_df$Stage_Distance_Classic * 2,
            predictions_df$Stage_Distance_Freestyle * 2,
            predictions_df$Stage_Sprint_Classic,
            predictions_df$Stage_Sprint_Freestyle,
            predictions_df$Stage_Distance) +
        predictions_df$TDS
    }
    
    total_points
  })
  
  return(data.frame(
    Mean_Points = mean(sim_results, na.rm = TRUE),
    Lower_CI = quantile(sim_results, 0.025, na.rm = TRUE),
    Upper_CI = quantile(sim_results, 0.975, na.rm = TRUE)
  ))
}

# Test with one skier
#test_skier <- all_predictions_with_volatility %>% slice(2)
#test_result <- simulate_season_with_volatility(test_skier, races_missed=1)
#print("Test simulation results:")
#print(test_result)

calculate_head_to_head <- function(skier1, skier2, all_predictions_vol, races_missed = 0, skip_tds = FALSE, n_sims = 1000) {
  skier1_data <- filter(all_predictions_vol, Skier == skier1)
  skier2_data <- filter(all_predictions_vol, Skier == skier2)
  
  sim_results <- replicate(n_sims, {
    skier1_points <- simulate_season_with_volatility(skier1_data, races_missed, skip_tds, 1)$Mean_Points
    skier2_points <- simulate_season_with_volatility(skier2_data, races_missed, skip_tds, 1)$Mean_Points
    skier1_points > skier2_points
  })
  
  mean(sim_results)
}

# Create results with ranges differently
results_with_ranges <- data.frame(
  Skier = character(),
  Mean_Points = numeric(),
  Lower_CI = numeric(),
  Upper_CI = numeric(),
  Races_Missed = numeric()
)

# Fill it for each number of missed races
for(n_missed in 0:10) {
  temp_results <- lapply(unique(all_predictions_with_volatility$Skier), function(skier) {
    skier_data <- all_predictions_with_volatility %>% 
      filter(Skier == skier)
    
    sim_results <- simulate_season_with_volatility(skier_data, n_missed)
    
    data.frame(
      Skier = skier,
      Mean_Points = sim_results$Mean_Points,
      Lower_CI = sim_results$Lower_CI,
      Upper_CI = sim_results$Upper_CI,
      Races_Missed = n_missed
    )
  })
  
  results_with_ranges <- rbind(results_with_ranges, do.call(rbind, temp_results))
}

print(results_with_ranges)
# Now get top 5 skiers
top_skiers <- results_with_ranges %>%
  filter(Races_Missed == 0) %>%  # Look at baseline scenario
  arrange(desc(Mean_Points)) %>%
  distinct(Skier) %>%
 # head(5) %>%
  pull(Skier)

print("Top 5 skiers by mean points:")
print(top_skiers)
```

```{r klaebo-miss}
# Function to calculate probability of beating Klæbo with n missed races
calc_prob_beating_klaebo <- function(challenger_name, n_missed, n_sims = 1000, klaebo_skip_tds) {
  klaebo_data <- all_predictions_with_volatility %>% 
    filter(Skier == "Johannes Høsflot Klæbo")
  
  challenger_data <- all_predictions_with_volatility %>% 
    filter(Skier == challenger_name)
  
  # Run simulations
  wins <- replicate(n_sims, {
    # Klæbo's points (with missed races and potentially skipping TDS)
    klaebo_points <- simulate_season_with_volatility(klaebo_data, n_missed, klaebo_skip_tds)$Mean_Points
    
    # Challenger's points (no missed races, always doing TDS)
    challenger_points <- simulate_season_with_volatility(challenger_data, 0, FALSE)$Mean_Points
    
    challenger_points > klaebo_points
  })
  
  mean(wins)  # Return probability of challenger winning
}

# Calculate for both scenarios (Klæbo doing/skipping TDS)
results_klaebo_with_tds <- lapply(top_skiers[top_skiers != "Johannes Høsflot Klæbo"], function(challenger) {
  for(n_missed in 0:25) {
    prob <- calc_prob_beating_klaebo(challenger, n_missed, n_sims=1, klaebo_skip_tds=FALSE)
    if(prob > 0.5) {
      return(n_missed)
    }
  }
  return(NA)
})

results_klaebo_without_tds <- lapply(top_skiers[top_skiers != "Johannes Høsflot Klæbo"], function(challenger) {
  for(n_missed in 0:25) {
    prob <- calc_prob_beating_klaebo(challenger, n_missed, n_sims=1, klaebo_skip_tds=TRUE)
    if(prob > 0.5) {
      return(n_missed)
    }
  }
  return(NA)
})

# Create combined dataframe
results_df <- data.frame(
  Skier = top_skiers[top_skiers != "Johannes Høsflot Klæbo"],
  `Klaebo With TDS` = unlist(results_klaebo_with_tds),
  `Klaebo Without TDS` = unlist(results_klaebo_without_tds)
)

colnames(results_df) <- c("Skier", "Klaebo with TDS", "Klaebo without TDS")
# Save to Excel
write.xlsx(results_df, 
           file = "/Users/syverjohansen/blog/daehl-e/content/post/drafts/season-prediction/klaebo.xlsx",
           rowNames = FALSE)

print("Combined results:")
print(results_df)
```
